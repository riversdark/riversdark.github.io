[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A Welcome is in Order…",
    "section": "",
    "text": "Kind of you to drop by! This is the personal website of Olivier Ma."
  },
  {
    "objectID": "stats.html",
    "href": "stats.html",
    "title": "Stats",
    "section": "",
    "text": "Bayesian statistics, machine learning, everything thereof and beyond.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding the attention mechanism\n\n\n\ntransformer\n\n\nLLM\n\n\n\nAll about matrix multiplications.\n\n\n\n\n\n\nJun 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEight Schools in NumPyro\n\n\n\nnumpyro\n\n\nBayesian\n\n\n\nRobust HMC inference using variable reparameterisation\n\n\n\n\n\n\nJun 22, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "culture.html",
    "href": "culture.html",
    "title": "Culture",
    "section": "",
    "text": "Every now and then I also write about this and that of little interest and no significance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEmma Zunz\n\n\n\n\n\n\nFR\n\n\nLit\n\n\n\nDes extraits d’une histoire, Emma Zunz, de Jorge Luis Borges.\n\n\n\n\n\nJun 9, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nWang Ziyou\n\n\n\n\n\n\nFR\n\n\nLit\n\n\nCN\n\n\n\n世说新语，任誕第二十三。\n\n\n\n\n\nFeb 28, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nHan Fei\n\n\n\n\n\n\nFR\n\n\nPhil\n\n\nCN\n\n\n\nUn extrait de 中国哲学史 de 冯友兰.\n\n\n\n\n\nJan 24, 2021\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "culture/2021-02-28-ziyou.html",
    "href": "culture/2021-02-28-ziyou.html",
    "title": "Wang Ziyou",
    "section": "",
    "text": "Huizhi habitait au nord de la montagne. (Huizhi était un des calligraphes chinois les plus renommés.)\nUn soir, il neigea. Huizhi se réveilla : il faisait encore nuit. Il ouvrit la porte, et ordonna à son domestique de préparer du vin.\nIl faisait très beau : tout était couvert de neige ; tout baignait dans une lumière claire et douce. Huizhi fut touché de la scène : il se leva, déambula dans la cour, récita un ancien poème qui raconte d’un ancienne histoire des recherches aux ermites. Il se souvint de son ami Andao. Il eut soudain l’envie irrésistible de le visiter, mais Andao habitait à des lieues de là.\nPeu importait. Il sortit, monta dans un petit bateau et partit. Il lui fallut toute la nuit pour arriver à destination. Il sortit du bateau, courut, arriva à la porte et alla frapper, mais il s’arrêta.\nIl revint sur ses pas et rentra chez lui.\nOn fut surpris et on lui demanda : mais pourquoi n’êtes-vous pas entré ? Vous avez parcouru tout ce chemin pour le voir ! Il répondit : j’avais l’envie de le voir et j’ai bien profité de cette envie. Maintenant, je n’en ai plus envie et je suis de retour. Le voir ou pas, ce n’est pas ce qui compte.\n王子猷居山阴。夜大雪，眠觉，开室命酌酒，四望皎然；因起彷徨，咏左思《招隐》诗，忽忆戴安道。时戴在剡，即便夜乘小船就之，经宿方至，造门不前而返。人问其故，王曰：吾本乘兴而行，兴尽而返，何必见戴。"
  },
  {
    "objectID": "stats/eight.html",
    "href": "stats/eight.html",
    "title": "Eight Schools in NumPyro",
    "section": "",
    "text": "This is an introduction to NumPyro, using the Eight Schools model as example.\nHere we demonstrate the effects of model reparameterisation. Reparameterisation is especially important in hierarchical models, where the joint density tend to have high curvatures.\nimport numpy as np\nimport jax.numpy as jnp\nimport numpyro\nimport numpyro.distributions as dist\nfrom jax import random\nfrom numpyro.infer import MCMC, NUTS, Predictive\n\nrng_key = random.PRNGKey(0)\n\n2024-06-10 19:11:54.689145: W external/xla/xla/service/gpu/nvptx_compiler.cc:760] The NVIDIA driver's CUDA version is 12.2 which is older than the ptxas CUDA version (12.5.40). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\nHere we are using the classic eight schools dataset from Gelman et al. We have collected the test score statistics for eight schools, including the mean and standard error. The goal, is to determine whether some schools have done better than others. Note that since we are working with the mean and standard error of eight different schools, we are actually modeling the statistical analysis resutls of some other people: this is essentially a meta analysis problem.\nJ = 8\ny = np.array([28.0, 8.0, -3.0, 7.0, -1.0, 1.0, 18.0, 12.0])\nsigma = np.array([15.0, 10.0, 16.0, 11.0, 9.0, 11.0, 10.0, 18.0])\nVisualize the data.\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import norm\n\nsns.set_theme()\nsns.set_palette(\"Set2\")\n\nfig, ax = plt.subplots(figsize=(8, 4))\nx = np.linspace(-50, 70, 1000)\nfor mean, sd in zip(y, sigma):\n    ax.plot(x, norm.pdf(x, mean, sd))\n\nplt.title('Test Score from Eight Schools')\nplt.savefig('fig/eight.png')"
  },
  {
    "objectID": "stats/eight.html#the-baseline-model",
    "href": "stats/eight.html#the-baseline-model",
    "title": "Eight Schools in NumPyro",
    "section": "The baseline model",
    "text": "The baseline model\nThe model we are building is a standard hierarchical model. We assume that the observed school means represent their true mean \\(\\theta_n\\), but corrupted with some Normal noise, and the true means are themselves drawn from another district level distribution, with mean \\(\\mu\\) and standard deviation \\(\\tau\\), which are also modeled with suitable distributions. Essentially it’s Gaussian all the way up, and we have three different levels to consider: student, school, and the whole district level population.\n\\[\n\\begin{align*}\ny_n &\\sim \\text{N} (\\theta_n, \\sigma_n) \\\\\n\\theta_n &\\sim \\text{N} (\\mu, \\tau) \\\\\n\\mu &\\sim \\text{N} (0, 5) \\\\\n\\tau &\\sim \\text{HalfCauchy} (5).\n\\end{align*}\n\\]\nIn NumPyro models are coded as functions.\n\ndef es_0(J, sigma):\n    mu = numpyro.sample('mu', dist.Normal(0, 5))\n    tau = numpyro.sample('tau', dist.HalfCauchy(5))\n\n    with numpyro.plate('J', J):\n        theta = numpyro.sample('theta', dist.Normal(mu, tau))\n        numpyro.sample('obs', dist.Normal(theta, sigma))\n\nNote that the code and the mathematical model are almost identical, except that they go in different directions. In the mathematical model, we start with the observed data, and reason backward to determine how they might be generated. In the code we start with the hyperparameters, and move forward to generate the observed data.\nJ and sigma are data we used to build our model, but they are not part of the model, in the sense that they are not assigned any probability distribution. y, on the other hand, is the central variable of the model, and is named obs in the model.\nnumpyro.plate is used to denote that the variables inside the plate are conditionally independent. Probability distributions are the building blocks of Bayesian models, and NumPyro has a lot of them. In NunPyro, probability distributions are wrappers of JAX random number generators, and they are translated into sampling statements using the numpyro.sample primitive.\nHowever numpyro.sample is used to define the model, not to draw samples from the distribution. To actually draw samples from a distribution, we use numpyro.infer.Predictive. In numpyro each model defines a joint distribution, and since it’s a probability distribution, we can draw samples from it. And since we haven’t conditioned on any data, the samples we draw are from the prior distribution.\nSampling directly from the prior distribution, and inspect the samples, is a good way to check if the model is correctly defined.\n\nes_prior_predictive_0 = Predictive(es_0, num_samples=1000)\nes_prior_samples_0 = es_prior_predictive_0(rng_key, J, sigma)\n\ndef print_stats(name, samples):\n    print(f\"Variable: {name}\")\n    print(f\"  Shape: {samples.shape}\")\n    print(f\"  Mean: {jnp.mean(samples, axis=0)}\")\n    print(f\"  Variance: {jnp.var(samples, axis=0)}\\n\")\n\n# Print statistics for each variable\nprint_stats('mu', es_prior_samples_0['mu'])\nprint_stats('tau', es_prior_samples_0['tau'])\nprint_stats('theta', es_prior_samples_0['theta'])\nprint_stats('obs', es_prior_samples_0['obs'])\n\nVariable: mu\n  Shape: (1000,)\n  Mean: -0.16857339441776276\n  Variance: 26.169570922851562\n\nVariable: tau\n  Shape: (1000,)\n  Mean: 19.337650299072266\n  Variance: 10626.4833984375\n\nVariable: theta\n  Shape: (1000, 8)\n  Mean: [-1.2455076   2.9482472  -5.281853   -0.06498987  2.3573008   0.31068215\n  3.729895   -1.5531777 ]\n  Variance: [11408.184   8016.4224 36956.      8181.7744  7636.8813  9586.139\n  6544.6753 26078.693 ]\n\nVariable: obs\n  Shape: (1000, 8)\n  Mean: [-1.7466605   2.9530644  -4.942445    0.10836948  2.5812457   0.34208587\n  3.1065757  -2.218382  ]\n  Variance: [11653.146   8119.2095 37439.16    8326.833   7699.3613  9800.354\n  6728.329  26469.115 ]\n\n\n\nWhen the samples of some variables are observed, we can condition on these observations, and infer the conditional distributions of the other variables. This process is called inference, and is commonly done using MCMC methods. The conditioning is done using the numpyro.handlers.condition primitive, by feeding it a data dict and the model.\nSince we have a GPU available, we will also configure the MCMC sampler to use vectorized chains.\n\nfrom numpyro.handlers import condition\n\nmcmc_args = {\n    'num_warmup': 1000,\n    'num_samples': 5000,\n    'num_chains': 8,\n    'progress_bar': True,\n    'chain_method': 'vectorized'\n}\n\nes_conditioned_0 = condition(es_0, data={'obs': y})\nes_nuts_0 = NUTS(es_conditioned_0)\nes_mcmc_0 = MCMC(es_nuts_0, **mcmc_args)\n\nes_mcmc_0.run(rng_key, J, sigma)\n\n  0%|          | 0/6000 [00:00&lt;?, ?it/s]warmup:   0%|          | 1/6000 [00:01&lt;2:54:40,  1.75s/it]warmup:   0%|          | 5/6000 [00:01&lt;30:21,  3.29it/s]  warmup:   0%|          | 7/6000 [00:02&lt;24:32,  4.07it/s]warmup:   0%|          | 10/6000 [00:02&lt;15:18,  6.52it/s]warmup:   0%|          | 12/6000 [00:02&lt;12:25,  8.03it/s]warmup:   0%|          | 14/6000 [00:02&lt;10:45,  9.27it/s]warmup:   0%|          | 17/6000 [00:02&lt;08:39, 11.52it/s]warmup:   0%|          | 19/6000 [00:02&lt;07:40, 12.99it/s]warmup:   0%|          | 21/6000 [00:02&lt;07:05, 14.05it/s]warmup:   0%|          | 23/6000 [00:03&lt;07:05, 14.03it/s]warmup:   0%|          | 27/6000 [00:03&lt;05:46, 17.25it/s]warmup:   0%|          | 29/6000 [00:03&lt;05:40, 17.53it/s]warmup:   1%|          | 32/6000 [00:03&lt;04:54, 20.25it/s]warmup:   1%|          | 39/6000 [00:03&lt;03:11, 31.20it/s]warmup:   1%|          | 43/6000 [00:03&lt;03:03, 32.52it/s]warmup:   1%|          | 47/6000 [00:03&lt;02:54, 34.14it/s]warmup:   1%|          | 51/6000 [00:03&lt;02:53, 34.30it/s]warmup:   1%|          | 55/6000 [00:04&lt;03:08, 31.60it/s]warmup:   1%|          | 59/6000 [00:04&lt;03:40, 26.88it/s]warmup:   1%|          | 64/6000 [00:04&lt;03:14, 30.47it/s]warmup:   1%|          | 68/6000 [00:04&lt;03:43, 26.56it/s]warmup:   1%|          | 71/6000 [00:04&lt;04:39, 21.18it/s]warmup:   1%|▏         | 76/6000 [00:05&lt;03:52, 25.44it/s]warmup:   1%|▏         | 80/6000 [00:05&lt;03:29, 28.23it/s]warmup:   1%|▏         | 84/6000 [00:05&lt;03:22, 29.15it/s]warmup:   1%|▏         | 88/6000 [00:05&lt;03:23, 29.12it/s]warmup:   2%|▏         | 92/6000 [00:05&lt;03:24, 28.88it/s]warmup:   2%|▏         | 96/6000 [00:05&lt;03:12, 30.70it/s]warmup:   2%|▏         | 104/6000 [00:05&lt;02:17, 42.75it/s]warmup:   2%|▏         | 116/6000 [00:05&lt;01:36, 61.04it/s]warmup:   2%|▏         | 127/6000 [00:05&lt;01:21, 72.27it/s]warmup:   2%|▏         | 138/6000 [00:06&lt;01:12, 81.33it/s]warmup:   2%|▏         | 149/6000 [00:06&lt;01:05, 89.11it/s]warmup:   3%|▎         | 160/6000 [00:06&lt;01:03, 92.05it/s]warmup:   3%|▎         | 170/6000 [00:06&lt;01:17, 75.07it/s]warmup:   3%|▎         | 181/6000 [00:06&lt;01:09, 83.45it/s]warmup:   3%|▎         | 194/6000 [00:06&lt;01:00, 95.31it/s]warmup:   3%|▎         | 208/6000 [00:06&lt;00:57, 101.12it/s]warmup:   4%|▎         | 219/6000 [00:06&lt;01:03, 91.58it/s] warmup:   4%|▍         | 233/6000 [00:07&lt;00:57, 101.03it/s]warmup:   4%|▍         | 246/6000 [00:07&lt;00:54, 106.32it/s]warmup:   4%|▍         | 262/6000 [00:07&lt;00:49, 115.19it/s]warmup:   5%|▍         | 274/6000 [00:07&lt;00:57, 99.66it/s] warmup:   5%|▍         | 285/6000 [00:07&lt;01:39, 57.19it/s]warmup:   5%|▍         | 294/6000 [00:08&lt;02:25, 39.16it/s]warmup:   5%|▌         | 301/6000 [00:08&lt;03:23, 27.94it/s]warmup:   5%|▌         | 307/6000 [00:08&lt;03:01, 31.33it/s]warmup:   5%|▌         | 314/6000 [00:09&lt;02:38, 35.95it/s]warmup:   5%|▌         | 320/6000 [00:09&lt;02:31, 37.42it/s]warmup:   6%|▌         | 333/6000 [00:09&lt;01:47, 52.87it/s]warmup:   6%|▌         | 346/6000 [00:09&lt;01:23, 67.42it/s]warmup:   6%|▌         | 361/6000 [00:09&lt;01:06, 84.89it/s]warmup:   6%|▋         | 380/6000 [00:09&lt;00:52, 108.08it/s]warmup:   7%|▋         | 397/6000 [00:09&lt;00:45, 122.21it/s]warmup:   7%|▋         | 417/6000 [00:09&lt;00:39, 142.02it/s]warmup:   7%|▋         | 436/6000 [00:09&lt;00:36, 153.87it/s]warmup:   8%|▊         | 453/6000 [00:10&lt;00:36, 152.34it/s]warmup:   8%|▊         | 469/6000 [00:10&lt;00:55, 100.18it/s]warmup:   8%|▊         | 482/6000 [00:10&lt;01:02, 88.54it/s] warmup:   8%|▊         | 493/6000 [00:10&lt;01:01, 89.00it/s]warmup:   8%|▊         | 504/6000 [00:10&lt;01:01, 89.85it/s]warmup:   9%|▊         | 517/6000 [00:10&lt;00:56, 97.19it/s]warmup:   9%|▉         | 537/6000 [00:10&lt;00:45, 121.28it/s]warmup:   9%|▉         | 554/6000 [00:11&lt;00:41, 131.67it/s]warmup:   9%|▉         | 569/6000 [00:11&lt;00:39, 136.00it/s]warmup:  10%|▉         | 590/6000 [00:11&lt;00:35, 154.57it/s]warmup:  10%|█         | 610/6000 [00:11&lt;00:32, 167.09it/s]warmup:  10%|█         | 628/6000 [00:11&lt;00:32, 165.55it/s]warmup:  11%|█         | 649/6000 [00:11&lt;00:30, 175.96it/s]warmup:  11%|█         | 667/6000 [00:11&lt;00:32, 165.02it/s]warmup:  11%|█▏        | 685/6000 [00:11&lt;00:31, 166.90it/s]warmup:  12%|█▏        | 706/6000 [00:11&lt;00:29, 177.92it/s]warmup:  12%|█▏        | 725/6000 [00:12&lt;00:29, 180.43it/s]warmup:  12%|█▏        | 744/6000 [00:12&lt;00:31, 168.90it/s]warmup:  13%|█▎        | 763/6000 [00:12&lt;00:30, 173.71it/s]warmup:  13%|█▎        | 781/6000 [00:12&lt;00:32, 158.21it/s]warmup:  13%|█▎        | 798/6000 [00:12&lt;00:32, 158.62it/s]warmup:  14%|█▎        | 817/6000 [00:12&lt;00:31, 163.75it/s]warmup:  14%|█▍        | 836/6000 [00:12&lt;00:30, 170.93it/s]warmup:  14%|█▍        | 854/6000 [00:12&lt;00:30, 170.85it/s]warmup:  15%|█▍        | 872/6000 [00:12&lt;00:30, 169.38it/s]warmup:  15%|█▍        | 894/6000 [00:13&lt;00:28, 180.56it/s]warmup:  15%|█▌        | 913/6000 [00:13&lt;00:28, 181.16it/s]warmup:  16%|█▌        | 932/6000 [00:13&lt;00:29, 173.97it/s]warmup:  16%|█▌        | 954/6000 [00:13&lt;00:28, 179.69it/s]warmup:  16%|█▌        | 973/6000 [00:13&lt;00:37, 133.76it/s]warmup:  16%|█▋        | 989/6000 [00:13&lt;00:40, 124.54it/s]sample:  17%|█▋        | 1005/6000 [00:13&lt;00:37, 131.90it/s]sample:  17%|█▋        | 1028/6000 [00:13&lt;00:32, 155.11it/s]sample:  18%|█▊        | 1050/6000 [00:14&lt;00:29, 170.36it/s]sample:  18%|█▊        | 1075/6000 [00:14&lt;00:26, 189.42it/s]sample:  18%|█▊        | 1096/6000 [00:14&lt;00:25, 194.61it/s]sample:  19%|█▊        | 1117/6000 [00:14&lt;00:24, 198.18it/s]sample:  19%|█▉        | 1138/6000 [00:14&lt;00:24, 199.07it/s]sample:  19%|█▉        | 1160/6000 [00:14&lt;00:23, 204.57it/s]sample:  20%|█▉        | 1181/6000 [00:14&lt;00:24, 195.46it/s]sample:  20%|██        | 1201/6000 [00:14&lt;00:24, 193.14it/s]sample:  20%|██        | 1221/6000 [00:14&lt;00:24, 191.26it/s]sample:  21%|██        | 1242/6000 [00:15&lt;00:24, 194.66it/s]sample:  21%|██        | 1262/6000 [00:15&lt;00:24, 192.10it/s]sample:  21%|██▏       | 1288/6000 [00:15&lt;00:22, 210.41it/s]sample:  22%|██▏       | 1313/6000 [00:15&lt;00:21, 221.46it/s]sample:  22%|██▏       | 1336/6000 [00:15&lt;00:23, 202.20it/s]sample:  23%|██▎       | 1357/6000 [00:15&lt;00:23, 200.55it/s]sample:  23%|██▎       | 1378/6000 [00:15&lt;00:23, 199.43it/s]sample:  23%|██▎       | 1399/6000 [00:15&lt;00:23, 199.27it/s]sample:  24%|██▎       | 1422/6000 [00:15&lt;00:22, 202.01it/s]sample:  24%|██▍       | 1443/6000 [00:16&lt;00:23, 194.73it/s]sample:  24%|██▍       | 1463/6000 [00:16&lt;00:23, 191.77it/s]sample:  25%|██▍       | 1487/6000 [00:16&lt;00:22, 204.54it/s]sample:  25%|██▌       | 1508/6000 [00:16&lt;00:22, 198.07it/s]sample:  25%|██▌       | 1528/6000 [00:16&lt;00:22, 196.04it/s]sample:  26%|██▌       | 1548/6000 [00:16&lt;00:22, 194.46it/s]sample:  26%|██▌       | 1572/6000 [00:16&lt;00:21, 206.15it/s]sample:  27%|██▋       | 1596/6000 [00:16&lt;00:20, 214.72it/s]sample:  27%|██▋       | 1620/6000 [00:16&lt;00:19, 220.36it/s]sample:  27%|██▋       | 1643/6000 [00:16&lt;00:20, 209.75it/s]sample:  28%|██▊       | 1668/6000 [00:17&lt;00:19, 217.24it/s]sample:  28%|██▊       | 1691/6000 [00:17&lt;00:19, 220.49it/s]sample:  29%|██▊       | 1714/6000 [00:17&lt;00:19, 215.38it/s]sample:  29%|██▉       | 1736/6000 [00:17&lt;00:20, 209.20it/s]sample:  29%|██▉       | 1760/6000 [00:17&lt;00:19, 215.81it/s]sample:  30%|██▉       | 1782/6000 [00:17&lt;00:20, 207.87it/s]sample:  30%|███       | 1804/6000 [00:17&lt;00:19, 211.11it/s]sample:  30%|███       | 1826/6000 [00:17&lt;00:20, 206.59it/s]sample:  31%|███       | 1847/6000 [00:17&lt;00:20, 199.56it/s]sample:  31%|███       | 1868/6000 [00:18&lt;00:20, 200.93it/s]sample:  31%|███▏      | 1889/6000 [00:18&lt;00:20, 202.69it/s]sample:  32%|███▏      | 1910/6000 [00:18&lt;00:20, 202.37it/s]sample:  32%|███▏      | 1931/6000 [00:18&lt;00:20, 202.42it/s]sample:  33%|███▎      | 1952/6000 [00:18&lt;00:19, 204.16it/s]sample:  33%|███▎      | 1973/6000 [00:18&lt;00:19, 204.46it/s]sample:  33%|███▎      | 1994/6000 [00:18&lt;00:19, 201.22it/s]sample:  34%|███▎      | 2015/6000 [00:18&lt;00:19, 201.42it/s]sample:  34%|███▍      | 2036/6000 [00:18&lt;00:19, 199.34it/s]sample:  34%|███▍      | 2061/6000 [00:18&lt;00:18, 212.62it/s]sample:  35%|███▍      | 2083/6000 [00:19&lt;00:18, 210.96it/s]sample:  35%|███▌      | 2105/6000 [00:19&lt;00:18, 209.99it/s]sample:  35%|███▌      | 2129/6000 [00:19&lt;00:17, 216.91it/s]sample:  36%|███▌      | 2151/6000 [00:19&lt;00:17, 217.00it/s]sample:  36%|███▌      | 2173/6000 [00:19&lt;00:18, 211.97it/s]sample:  37%|███▋      | 2195/6000 [00:19&lt;00:18, 205.92it/s]sample:  37%|███▋      | 2218/6000 [00:19&lt;00:17, 210.48it/s]sample:  37%|███▋      | 2240/6000 [00:19&lt;00:18, 203.49it/s]sample:  38%|███▊      | 2261/6000 [00:19&lt;00:18, 198.88it/s]sample:  38%|███▊      | 2281/6000 [00:20&lt;00:19, 188.18it/s]sample:  38%|███▊      | 2301/6000 [00:20&lt;00:19, 189.71it/s]sample:  39%|███▊      | 2321/6000 [00:20&lt;00:19, 190.88it/s]sample:  39%|███▉      | 2341/6000 [00:20&lt;00:18, 193.22it/s]sample:  39%|███▉      | 2361/6000 [00:20&lt;00:19, 184.85it/s]sample:  40%|███▉      | 2384/6000 [00:20&lt;00:18, 197.33it/s]sample:  40%|████      | 2406/6000 [00:20&lt;00:17, 203.62it/s]sample:  40%|████      | 2427/6000 [00:20&lt;00:17, 202.39it/s]sample:  41%|████      | 2448/6000 [00:20&lt;00:17, 200.81it/s]sample:  41%|████      | 2469/6000 [00:21&lt;00:18, 194.61it/s]sample:  42%|████▏     | 2493/6000 [00:21&lt;00:17, 202.21it/s]sample:  42%|████▏     | 2517/6000 [00:21&lt;00:16, 210.52it/s]sample:  42%|████▏     | 2539/6000 [00:21&lt;00:16, 209.43it/s]sample:  43%|████▎     | 2560/6000 [00:21&lt;00:16, 207.36it/s]sample:  43%|████▎     | 2581/6000 [00:21&lt;00:16, 204.97it/s]sample:  43%|████▎     | 2602/6000 [00:21&lt;00:17, 197.16it/s]sample:  44%|████▍     | 2625/6000 [00:21&lt;00:16, 204.43it/s]sample:  44%|████▍     | 2648/6000 [00:21&lt;00:15, 211.20it/s]sample:  45%|████▍     | 2673/6000 [00:21&lt;00:15, 220.03it/s]sample:  45%|████▍     | 2699/6000 [00:22&lt;00:14, 227.99it/s]sample:  45%|████▌     | 2725/6000 [00:22&lt;00:13, 235.68it/s]sample:  46%|████▌     | 2749/6000 [00:22&lt;00:14, 225.73it/s]sample:  46%|████▌     | 2772/6000 [00:22&lt;00:14, 217.42it/s]sample:  47%|████▋     | 2794/6000 [00:22&lt;00:14, 216.82it/s]sample:  47%|████▋     | 2816/6000 [00:22&lt;00:15, 208.57it/s]sample:  47%|████▋     | 2837/6000 [00:22&lt;00:15, 199.59it/s]sample:  48%|████▊     | 2861/6000 [00:22&lt;00:14, 210.06it/s]sample:  48%|████▊     | 2886/6000 [00:22&lt;00:14, 220.15it/s]sample:  48%|████▊     | 2909/6000 [00:23&lt;00:14, 216.74it/s]sample:  49%|████▉     | 2931/6000 [00:23&lt;00:14, 212.09it/s]sample:  49%|████▉     | 2953/6000 [00:23&lt;00:14, 205.55it/s]sample:  50%|████▉     | 2979/6000 [00:23&lt;00:13, 219.68it/s]sample:  50%|█████     | 3002/6000 [00:23&lt;00:14, 213.52it/s]sample:  50%|█████     | 3024/6000 [00:23&lt;00:14, 209.11it/s]sample:  51%|█████     | 3047/6000 [00:23&lt;00:13, 212.93it/s]sample:  51%|█████     | 3069/6000 [00:23&lt;00:13, 214.71it/s]sample:  52%|█████▏    | 3091/6000 [00:23&lt;00:14, 207.72it/s]sample:  52%|█████▏    | 3112/6000 [00:24&lt;00:14, 192.81it/s]sample:  52%|█████▏    | 3136/6000 [00:24&lt;00:14, 203.71it/s]sample:  53%|█████▎    | 3160/6000 [00:24&lt;00:13, 211.94it/s]sample:  53%|█████▎    | 3182/6000 [00:24&lt;00:13, 211.91it/s]sample:  53%|█████▎    | 3204/6000 [00:24&lt;00:13, 210.05it/s]sample:  54%|█████▍    | 3226/6000 [00:24&lt;00:13, 208.11it/s]sample:  54%|█████▍    | 3247/6000 [00:24&lt;00:13, 200.80it/s]sample:  55%|█████▍    | 3270/6000 [00:24&lt;00:13, 208.14it/s]sample:  55%|█████▍    | 3291/6000 [00:24&lt;00:13, 203.91it/s]sample:  55%|█████▌    | 3312/6000 [00:25&lt;00:13, 204.82it/s]sample:  56%|█████▌    | 3333/6000 [00:25&lt;00:13, 205.03it/s]sample:  56%|█████▌    | 3354/6000 [00:25&lt;00:12, 204.53it/s]sample:  56%|█████▋    | 3378/6000 [00:25&lt;00:12, 214.23it/s]sample:  57%|█████▋    | 3400/6000 [00:25&lt;00:12, 210.91it/s]sample:  57%|█████▋    | 3422/6000 [00:25&lt;00:13, 185.66it/s]sample:  57%|█████▋    | 3442/6000 [00:25&lt;00:13, 185.88it/s]sample:  58%|█████▊    | 3463/6000 [00:25&lt;00:13, 190.59it/s]sample:  58%|█████▊    | 3483/6000 [00:25&lt;00:13, 191.40it/s]sample:  58%|█████▊    | 3505/6000 [00:26&lt;00:12, 197.73it/s]sample:  59%|█████▉    | 3528/6000 [00:26&lt;00:12, 204.83it/s]sample:  59%|█████▉    | 3551/6000 [00:26&lt;00:11, 211.00it/s]sample:  60%|█████▉    | 3573/6000 [00:26&lt;00:11, 213.54it/s]sample:  60%|█████▉    | 3595/6000 [00:26&lt;00:11, 207.29it/s]sample:  60%|██████    | 3616/6000 [00:26&lt;00:12, 197.38it/s]sample:  61%|██████    | 3638/6000 [00:26&lt;00:11, 201.72it/s]sample:  61%|██████    | 3661/6000 [00:26&lt;00:11, 207.40it/s]sample:  61%|██████▏   | 3682/6000 [00:26&lt;00:11, 203.85it/s]sample:  62%|██████▏   | 3705/6000 [00:26&lt;00:10, 210.49it/s]sample:  62%|██████▏   | 3728/6000 [00:27&lt;00:10, 211.01it/s]sample:  63%|██████▎   | 3751/6000 [00:27&lt;00:10, 213.52it/s]sample:  63%|██████▎   | 3773/6000 [00:27&lt;00:10, 212.31it/s]sample:  63%|██████▎   | 3795/6000 [00:27&lt;00:10, 210.49it/s]sample:  64%|██████▎   | 3819/6000 [00:27&lt;00:10, 217.26it/s]sample:  64%|██████▍   | 3841/6000 [00:27&lt;00:10, 213.05it/s]sample:  64%|██████▍   | 3863/6000 [00:27&lt;00:10, 212.87it/s]sample:  65%|██████▍   | 3885/6000 [00:27&lt;00:09, 213.83it/s]sample:  65%|██████▌   | 3907/6000 [00:27&lt;00:10, 206.89it/s]sample:  66%|██████▌   | 3930/6000 [00:28&lt;00:09, 209.89it/s]sample:  66%|██████▌   | 3952/6000 [00:28&lt;00:09, 211.08it/s]sample:  66%|██████▌   | 3974/6000 [00:28&lt;00:09, 206.98it/s]sample:  67%|██████▋   | 3997/6000 [00:28&lt;00:09, 210.03it/s]sample:  67%|██████▋   | 4019/6000 [00:28&lt;00:09, 210.02it/s]sample:  67%|██████▋   | 4041/6000 [00:28&lt;00:09, 205.58it/s]sample:  68%|██████▊   | 4063/6000 [00:28&lt;00:09, 207.00it/s]sample:  68%|██████▊   | 4085/6000 [00:28&lt;00:09, 208.12it/s]sample:  68%|██████▊   | 4106/6000 [00:28&lt;00:09, 204.83it/s]sample:  69%|██████▉   | 4127/6000 [00:28&lt;00:09, 193.92it/s]sample:  69%|██████▉   | 4149/6000 [00:29&lt;00:09, 200.87it/s]sample:  70%|██████▉   | 4170/6000 [00:29&lt;00:09, 202.50it/s]sample:  70%|██████▉   | 4193/6000 [00:29&lt;00:08, 206.68it/s]sample:  70%|███████   | 4217/6000 [00:29&lt;00:08, 215.51it/s]sample:  71%|███████   | 4240/6000 [00:29&lt;00:08, 213.96it/s]sample:  71%|███████   | 4262/6000 [00:29&lt;00:08, 208.36it/s]sample:  71%|███████▏  | 4283/6000 [00:29&lt;00:08, 206.49it/s]sample:  72%|███████▏  | 4304/6000 [00:29&lt;00:08, 197.99it/s]sample:  72%|███████▏  | 4325/6000 [00:29&lt;00:08, 199.54it/s]sample:  72%|███████▏  | 4346/6000 [00:30&lt;00:08, 198.10it/s]sample:  73%|███████▎  | 4371/6000 [00:30&lt;00:07, 209.23it/s]sample:  73%|███████▎  | 4392/6000 [00:30&lt;00:08, 199.71it/s]sample:  74%|███████▎  | 4413/6000 [00:30&lt;00:08, 189.32it/s]sample:  74%|███████▍  | 4433/6000 [00:30&lt;00:08, 187.91it/s]sample:  74%|███████▍  | 4452/6000 [00:30&lt;00:08, 183.70it/s]sample:  75%|███████▍  | 4473/6000 [00:30&lt;00:08, 189.28it/s]sample:  75%|███████▍  | 4495/6000 [00:30&lt;00:07, 195.61it/s]sample:  75%|███████▌  | 4518/6000 [00:30&lt;00:07, 205.39it/s]sample:  76%|███████▌  | 4540/6000 [00:31&lt;00:06, 209.28it/s]sample:  76%|███████▌  | 4562/6000 [00:31&lt;00:07, 200.72it/s]sample:  76%|███████▋  | 4584/6000 [00:31&lt;00:06, 204.50it/s]sample:  77%|███████▋  | 4606/6000 [00:31&lt;00:06, 208.25it/s]sample:  77%|███████▋  | 4627/6000 [00:31&lt;00:06, 207.11it/s]sample:  77%|███████▋  | 4648/6000 [00:31&lt;00:06, 201.03it/s]sample:  78%|███████▊  | 4670/6000 [00:31&lt;00:06, 205.56it/s]sample:  78%|███████▊  | 4691/6000 [00:31&lt;00:06, 203.21it/s]sample:  79%|███████▊  | 4712/6000 [00:31&lt;00:06, 203.40it/s]sample:  79%|███████▉  | 4733/6000 [00:31&lt;00:06, 202.81it/s]sample:  79%|███████▉  | 4758/6000 [00:32&lt;00:05, 215.34it/s]sample:  80%|███████▉  | 4780/6000 [00:32&lt;00:05, 212.95it/s]sample:  80%|████████  | 4804/6000 [00:32&lt;00:05, 219.75it/s]sample:  80%|████████  | 4827/6000 [00:32&lt;00:05, 219.31it/s]sample:  81%|████████  | 4849/6000 [00:32&lt;00:05, 210.43it/s]sample:  81%|████████  | 4872/6000 [00:32&lt;00:05, 215.97it/s]sample:  82%|████████▏ | 4894/6000 [00:32&lt;00:05, 203.30it/s]sample:  82%|████████▏ | 4915/6000 [00:32&lt;00:05, 195.31it/s]sample:  82%|████████▏ | 4935/6000 [00:32&lt;00:05, 188.11it/s]sample:  83%|████████▎ | 4957/6000 [00:33&lt;00:05, 195.85it/s]sample:  83%|████████▎ | 4977/6000 [00:33&lt;00:05, 196.34it/s]sample:  83%|████████▎ | 5000/6000 [00:33&lt;00:04, 205.77it/s]sample:  84%|████████▍ | 5025/6000 [00:33&lt;00:04, 213.45it/s]sample:  84%|████████▍ | 5047/6000 [00:33&lt;00:04, 203.34it/s]sample:  85%|████████▍ | 5073/6000 [00:33&lt;00:04, 218.89it/s]sample:  85%|████████▍ | 5096/6000 [00:33&lt;00:04, 221.02it/s]sample:  85%|████████▌ | 5119/6000 [00:33&lt;00:03, 222.05it/s]sample:  86%|████████▌ | 5143/6000 [00:33&lt;00:03, 225.13it/s]sample:  86%|████████▌ | 5166/6000 [00:34&lt;00:03, 215.98it/s]sample:  86%|████████▋ | 5190/6000 [00:34&lt;00:03, 221.73it/s]sample:  87%|████████▋ | 5213/6000 [00:34&lt;00:03, 215.25it/s]sample:  87%|████████▋ | 5235/6000 [00:34&lt;00:03, 204.76it/s]sample:  88%|████████▊ | 5256/6000 [00:34&lt;00:03, 189.24it/s]sample:  88%|████████▊ | 5279/6000 [00:34&lt;00:03, 199.39it/s]sample:  88%|████████▊ | 5300/6000 [00:34&lt;00:03, 199.67it/s]sample:  89%|████████▊ | 5324/6000 [00:34&lt;00:03, 210.32it/s]sample:  89%|████████▉ | 5349/6000 [00:34&lt;00:02, 221.24it/s]sample:  90%|████████▉ | 5372/6000 [00:35&lt;00:02, 214.01it/s]sample:  90%|████████▉ | 5395/6000 [00:35&lt;00:02, 217.75it/s]sample:  90%|█████████ | 5419/6000 [00:35&lt;00:02, 221.46it/s]sample:  91%|█████████ | 5442/6000 [00:35&lt;00:02, 218.54it/s]sample:  91%|█████████ | 5468/6000 [00:35&lt;00:02, 228.33it/s]sample:  92%|█████████▏| 5491/6000 [00:35&lt;00:02, 224.82it/s]sample:  92%|█████████▏| 5514/6000 [00:35&lt;00:02, 225.80it/s]sample:  92%|█████████▏| 5537/6000 [00:35&lt;00:02, 220.51it/s]sample:  93%|█████████▎| 5562/6000 [00:35&lt;00:01, 226.86it/s]sample:  93%|█████████▎| 5586/6000 [00:35&lt;00:01, 227.66it/s]sample:  93%|█████████▎| 5609/6000 [00:36&lt;00:01, 207.98it/s]sample:  94%|█████████▍| 5631/6000 [00:36&lt;00:01, 208.75it/s]sample:  94%|█████████▍| 5653/6000 [00:36&lt;00:01, 211.27it/s]sample:  95%|█████████▍| 5675/6000 [00:36&lt;00:01, 205.60it/s]sample:  95%|█████████▍| 5696/6000 [00:36&lt;00:01, 197.43it/s]sample:  95%|█████████▌| 5718/6000 [00:36&lt;00:01, 203.35it/s]sample:  96%|█████████▌| 5740/6000 [00:36&lt;00:01, 204.99it/s]sample:  96%|█████████▌| 5762/6000 [00:36&lt;00:01, 206.93it/s]sample:  96%|█████████▋| 5784/6000 [00:36&lt;00:01, 209.24it/s]sample:  97%|█████████▋| 5806/6000 [00:37&lt;00:00, 210.40it/s]sample:  97%|█████████▋| 5828/6000 [00:37&lt;00:00, 209.96it/s]sample:  98%|█████████▊| 5850/6000 [00:37&lt;00:00, 204.30it/s]sample:  98%|█████████▊| 5874/6000 [00:37&lt;00:00, 214.22it/s]sample:  98%|█████████▊| 5896/6000 [00:37&lt;00:00, 206.69it/s]sample:  99%|█████████▊| 5917/6000 [00:37&lt;00:00, 204.86it/s]sample:  99%|█████████▉| 5938/6000 [00:37&lt;00:00, 196.80it/s]sample:  99%|█████████▉| 5959/6000 [00:37&lt;00:00, 200.38it/s]sample: 100%|█████████▉| 5980/6000 [00:37&lt;00:00, 197.71it/s]sample: 100%|██████████| 6000/6000 [00:38&lt;00:00, 157.88it/s]\n\n\nThe NUTS sampler is a variant of the Hamiltonian Monte Carlo (HMC) sampler, which is a powerful tool for sampling from complex probability distributions. HMC also comes with its own set of diagnostics, which can be used to check the convergence of the Markov Chain. The most important ones are the effective sample size and the Gelman-Rubin statistic, which is a measure of the convergence of the Markov Chain.\n\nes_mcmc_0.print_summary()\n\n\n                mean       std    median      5.0%     95.0%     n_eff     r_hat\n        mu      4.26      3.34      4.25     -1.24      9.72   3163.83      1.00\n       tau      3.91      3.17      3.03      0.45      8.08   2151.46      1.01\n  theta[0]      6.26      5.81      5.64     -2.93     15.04   5795.74      1.00\n  theta[1]      4.86      4.78      4.74     -2.97     12.48   6468.43      1.00\n  theta[2]      3.71      5.40      3.97     -4.63     12.37   6937.61      1.00\n  theta[3]      4.66      4.94      4.55     -3.62     12.34   6752.22      1.00\n  theta[4]      3.41      4.71      3.64     -4.15     10.92   5415.63      1.00\n  theta[5]      3.86      4.97      4.01     -4.12     11.83   6923.98      1.00\n  theta[6]      6.36      5.27      5.75     -2.32     14.38   4727.40      1.00\n  theta[7]      4.72      5.50      4.59     -3.78     13.50   7525.65      1.00\n\nNumber of divergences: 1327\n\n\nThe effective sample size for \\(\\tau\\) is low, and judging from the r_hat value, the Markov Chain might not have converged. Also, the large number of divergences is a sign that the model might not be well specified. Here we are looking at a prominent problem in hierarchical modeling, known as Radford’s funnel, where the posterior distribution has a very sharp peak at the center, and a long tail, and the curvature of the distribution is very high. This seriously hinders the performance of HMC, and the divergences are a sign that the sampler is having trouble exploring the space.\nHowever, the issue can be readily rectified using a non-centered parameterization."
  },
  {
    "objectID": "stats/eight.html#manual-reparameterisation",
    "href": "stats/eight.html#manual-reparameterisation",
    "title": "Eight Schools in NumPyro",
    "section": "Manual reparameterisation",
    "text": "Manual reparameterisation\nThe remedy we are proposing is quite simple: replacing\n\\[\n\\theta_n \\sim \\text{N} (\\mu, \\tau)\n\\]\nwith\n\\[\n\\begin{align*}\n\\theta_n &= \\mu + \\tau  \\theta_0 \\\\\n\\theta_0 &\\sim \\text{N} (0, 1).\n\\end{align*}\n\\]\nIn essence, instead of drawing from a Normal distribution whose parameters are themselves variables in the model, we draw from the unit Normal distribution, and transform it to get the variable we want. By doing so we untangled the sampling process of \\(\\theta\\) from that of \\(\\mu\\) and \\(\\tau\\).\n\ndef es_1(J, sigma):\n    mu = numpyro.sample('mu', dist.Normal(0, 5))\n    tau = numpyro.sample('tau', dist.HalfCauchy(5))\n\n    with numpyro.plate('J', J):\n        theta_0 = numpyro.sample('theta_0', dist.Normal(0, 1))\n        theta = numpyro.deterministic('theta', mu + theta_0 * tau)\n        numpyro.sample('obs', dist.Normal(theta, sigma))\n\nHere we use another primitive, numpyro.deterministic, to register the transformed variable, so that its values can be stored and used later.\n\nes_prior_predictive_1 = Predictive(es_1, num_samples=1000)\nes_prior_samples_1 = es_prior_predictive_1(rng_key, J, sigma)\n\nprint_stats('theta_0', es_prior_samples_1['theta_0'])\n\nVariable: theta_0\n  Shape: (1000, 8)\n  Mean: [ 0.035466    0.00418693  0.04043639 -0.01104304  0.03547993  0.0202828\n  0.01902334  0.01258929]\n  Variance: [1.0026257 0.9806318 0.9726549 0.9744275 0.9668162 0.9936419 1.0036482\n 1.0627806]\n\n\n\nCondition on the observed data and do inference.\n\nes_conditioned_1 = condition(es_1, data={'obs': y})\nes_nuts_1 = NUTS(es_conditioned_1)\nes_mcmc_1 = MCMC(es_nuts_1, **mcmc_args)\n\nes_mcmc_1.run(rng_key, J, sigma)\nes_mcmc_1.print_summary(exclude_deterministic=False)\n\n  0%|          | 0/6000 [00:00&lt;?, ?it/s]warmup:   0%|          | 1/6000 [00:01&lt;2:54:13,  1.74s/it]warmup:   0%|          | 27/6000 [00:01&lt;04:57, 20.09it/s] warmup:   1%|          | 52/6000 [00:01&lt;02:20, 42.24it/s]warmup:   1%|▏         | 86/6000 [00:02&lt;01:15, 77.89it/s]warmup:   2%|▏         | 114/6000 [00:02&lt;00:55, 106.91it/s]warmup:   2%|▏         | 143/6000 [00:02&lt;00:42, 137.70it/s]warmup:   3%|▎         | 170/6000 [00:02&lt;00:38, 152.70it/s]warmup:   3%|▎         | 202/6000 [00:02&lt;00:31, 186.86it/s]warmup:   4%|▍         | 239/6000 [00:02&lt;00:25, 227.22it/s]warmup:   5%|▍         | 271/6000 [00:02&lt;00:22, 249.51it/s]warmup:   5%|▌         | 302/6000 [00:02&lt;00:23, 246.00it/s]warmup:   6%|▌         | 333/6000 [00:02&lt;00:21, 262.26it/s]warmup:   6%|▌         | 373/6000 [00:03&lt;00:18, 297.23it/s]warmup:   7%|▋         | 408/6000 [00:03&lt;00:18, 310.28it/s]warmup:   8%|▊         | 455/6000 [00:03&lt;00:15, 352.29it/s]warmup:   8%|▊         | 492/6000 [00:03&lt;00:17, 309.75it/s]warmup:   9%|▉         | 525/6000 [00:03&lt;00:18, 302.64it/s]warmup:   9%|▉         | 568/6000 [00:03&lt;00:16, 334.61it/s]warmup:  10%|█         | 609/6000 [00:03&lt;00:15, 353.73it/s]warmup:  11%|█         | 660/6000 [00:03&lt;00:13, 396.40it/s]warmup:  12%|█▏        | 713/6000 [00:03&lt;00:12, 433.13it/s]warmup:  13%|█▎        | 765/6000 [00:04&lt;00:11, 458.10it/s]warmup:  14%|█▎        | 813/6000 [00:04&lt;00:11, 464.05it/s]warmup:  14%|█▍        | 866/6000 [00:04&lt;00:10, 482.66it/s]warmup:  15%|█▌        | 915/6000 [00:04&lt;00:10, 479.56it/s]warmup:  16%|█▌        | 964/6000 [00:04&lt;00:11, 435.98it/s]sample:  17%|█▋        | 1009/6000 [00:04&lt;00:13, 378.00it/s]sample:  18%|█▊        | 1055/6000 [00:04&lt;00:12, 398.37it/s]sample:  18%|█▊        | 1097/6000 [00:04&lt;00:12, 402.14it/s]sample:  19%|█▉        | 1146/6000 [00:04&lt;00:11, 424.48it/s]sample:  20%|█▉        | 1193/6000 [00:05&lt;00:11, 434.50it/s]sample:  21%|██        | 1240/6000 [00:05&lt;00:10, 443.14it/s]sample:  21%|██▏       | 1288/6000 [00:05&lt;00:10, 448.84it/s]sample:  22%|██▏       | 1335/6000 [00:05&lt;00:10, 453.79it/s]sample:  23%|██▎       | 1382/6000 [00:05&lt;00:10, 456.78it/s]sample:  24%|██▍       | 1431/6000 [00:05&lt;00:09, 465.42it/s]sample:  25%|██▍       | 1478/6000 [00:05&lt;00:09, 453.29it/s]sample:  25%|██▌       | 1525/6000 [00:05&lt;00:09, 457.02it/s]sample:  26%|██▌       | 1571/6000 [00:05&lt;00:09, 452.11it/s]sample:  27%|██▋       | 1617/6000 [00:05&lt;00:09, 445.86it/s]sample:  28%|██▊       | 1669/6000 [00:06&lt;00:09, 466.37it/s]sample:  29%|██▊       | 1716/6000 [00:06&lt;00:09, 452.56it/s]sample:  29%|██▉       | 1762/6000 [00:06&lt;00:09, 447.17it/s]sample:  30%|███       | 1807/6000 [00:06&lt;00:09, 437.75it/s]sample:  31%|███       | 1852/6000 [00:06&lt;00:09, 441.15it/s]sample:  32%|███▏      | 1899/6000 [00:06&lt;00:09, 448.50it/s]sample:  32%|███▏      | 1947/6000 [00:06&lt;00:08, 456.25it/s]sample:  33%|███▎      | 1997/6000 [00:06&lt;00:08, 469.02it/s]sample:  34%|███▍      | 2044/6000 [00:06&lt;00:08, 463.18it/s]sample:  35%|███▍      | 2093/6000 [00:06&lt;00:08, 469.78it/s]sample:  36%|███▌      | 2141/6000 [00:07&lt;00:08, 468.25it/s]sample:  37%|███▋      | 2191/6000 [00:07&lt;00:08, 475.03it/s]sample:  37%|███▋      | 2243/6000 [00:07&lt;00:07, 487.13it/s]sample:  38%|███▊      | 2292/6000 [00:07&lt;00:07, 468.99it/s]sample:  39%|███▉      | 2340/6000 [00:07&lt;00:07, 464.76it/s]sample:  40%|███▉      | 2387/6000 [00:07&lt;00:07, 463.67it/s]sample:  41%|████      | 2434/6000 [00:07&lt;00:07, 460.34it/s]sample:  41%|████▏     | 2482/6000 [00:07&lt;00:07, 463.99it/s]sample:  42%|████▏     | 2529/6000 [00:07&lt;00:07, 456.47it/s]sample:  43%|████▎     | 2578/6000 [00:08&lt;00:07, 464.15it/s]sample:  44%|████▍     | 2625/6000 [00:08&lt;00:07, 461.24it/s]sample:  45%|████▍     | 2672/6000 [00:08&lt;00:07, 462.90it/s]sample:  45%|████▌     | 2720/6000 [00:08&lt;00:07, 462.92it/s]sample:  46%|████▌     | 2767/6000 [00:08&lt;00:07, 457.43it/s]sample:  47%|████▋     | 2813/6000 [00:08&lt;00:07, 453.22it/s]sample:  48%|████▊     | 2860/6000 [00:08&lt;00:06, 455.75it/s]sample:  48%|████▊     | 2906/6000 [00:08&lt;00:06, 453.72it/s]sample:  49%|████▉     | 2952/6000 [00:08&lt;00:06, 453.75it/s]sample:  50%|████▉     | 2998/6000 [00:08&lt;00:06, 447.13it/s]sample:  51%|█████     | 3047/6000 [00:09&lt;00:06, 458.35it/s]sample:  52%|█████▏    | 3093/6000 [00:09&lt;00:06, 448.77it/s]sample:  52%|█████▏    | 3141/6000 [00:09&lt;00:06, 456.88it/s]sample:  53%|█████▎    | 3187/6000 [00:09&lt;00:06, 453.38it/s]sample:  54%|█████▍    | 3237/6000 [00:09&lt;00:05, 465.75it/s]sample:  55%|█████▍    | 3285/6000 [00:09&lt;00:05, 468.01it/s]sample:  56%|█████▌    | 3332/6000 [00:09&lt;00:05, 467.60it/s]sample:  56%|█████▋    | 3379/6000 [00:09&lt;00:05, 464.60it/s]sample:  57%|█████▋    | 3426/6000 [00:09&lt;00:05, 444.57it/s]sample:  58%|█████▊    | 3471/6000 [00:10&lt;00:05, 429.69it/s]sample:  59%|█████▊    | 3520/6000 [00:10&lt;00:05, 445.75it/s]sample:  59%|█████▉    | 3566/6000 [00:10&lt;00:05, 448.57it/s]sample:  60%|██████    | 3613/6000 [00:10&lt;00:05, 451.58it/s]sample:  61%|██████    | 3659/6000 [00:10&lt;00:05, 448.24it/s]sample:  62%|██████▏   | 3708/6000 [00:10&lt;00:05, 458.08it/s]sample:  63%|██████▎   | 3756/6000 [00:10&lt;00:04, 461.94it/s]sample:  63%|██████▎   | 3803/6000 [00:10&lt;00:04, 458.31it/s]sample:  64%|██████▍   | 3853/6000 [00:10&lt;00:04, 468.84it/s]sample:  65%|██████▌   | 3900/6000 [00:10&lt;00:04, 467.56it/s]sample:  66%|██████▌   | 3950/6000 [00:11&lt;00:04, 476.85it/s]sample:  67%|██████▋   | 3998/6000 [00:11&lt;00:04, 476.33it/s]sample:  67%|██████▋   | 4047/6000 [00:11&lt;00:04, 479.09it/s]sample:  68%|██████▊   | 4100/6000 [00:11&lt;00:03, 492.02it/s]sample:  69%|██████▉   | 4150/6000 [00:11&lt;00:03, 484.55it/s]sample:  70%|███████   | 4200/6000 [00:11&lt;00:03, 484.14it/s]sample:  71%|███████   | 4249/6000 [00:11&lt;00:03, 465.35it/s]sample:  72%|███████▏  | 4296/6000 [00:11&lt;00:03, 460.80it/s]sample:  72%|███████▏  | 4345/6000 [00:11&lt;00:03, 464.62it/s]sample:  73%|███████▎  | 4392/6000 [00:11&lt;00:03, 461.71it/s]sample:  74%|███████▍  | 4441/6000 [00:12&lt;00:03, 466.76it/s]sample:  75%|███████▍  | 4489/6000 [00:12&lt;00:03, 467.82it/s]sample:  76%|███████▌  | 4538/6000 [00:12&lt;00:03, 473.45it/s]sample:  76%|███████▋  | 4586/6000 [00:12&lt;00:03, 466.66it/s]sample:  77%|███████▋  | 4635/6000 [00:12&lt;00:02, 468.77it/s]sample:  78%|███████▊  | 4685/6000 [00:12&lt;00:02, 474.70it/s]sample:  79%|███████▉  | 4741/6000 [00:12&lt;00:02, 497.70it/s]sample:  80%|███████▉  | 4792/6000 [00:12&lt;00:02, 499.85it/s]sample:  81%|████████  | 4843/6000 [00:12&lt;00:02, 488.28it/s]sample:  82%|████████▏ | 4892/6000 [00:13&lt;00:02, 476.29it/s]sample:  82%|████████▏ | 4942/6000 [00:13&lt;00:02, 479.23it/s]sample:  83%|████████▎ | 4990/6000 [00:13&lt;00:02, 472.67it/s]sample:  84%|████████▍ | 5038/6000 [00:13&lt;00:02, 468.18it/s]sample:  85%|████████▍ | 5085/6000 [00:13&lt;00:01, 460.90it/s]sample:  86%|████████▌ | 5137/6000 [00:13&lt;00:01, 473.90it/s]sample:  86%|████████▋ | 5188/6000 [00:13&lt;00:01, 483.94it/s]sample:  87%|████████▋ | 5239/6000 [00:13&lt;00:01, 490.27it/s]sample:  88%|████████▊ | 5289/6000 [00:13&lt;00:01, 482.79it/s]sample:  89%|████████▉ | 5338/6000 [00:13&lt;00:01, 474.44it/s]sample:  90%|████████▉ | 5390/6000 [00:14&lt;00:01, 486.40it/s]sample:  91%|█████████ | 5439/6000 [00:14&lt;00:01, 476.95it/s]sample:  91%|█████████▏| 5488/6000 [00:14&lt;00:01, 478.78it/s]sample:  92%|█████████▏| 5537/6000 [00:14&lt;00:00, 481.07it/s]sample:  93%|█████████▎| 5586/6000 [00:14&lt;00:00, 469.55it/s]sample:  94%|█████████▍| 5634/6000 [00:14&lt;00:00, 462.16it/s]sample:  95%|█████████▍| 5681/6000 [00:14&lt;00:00, 460.94it/s]sample:  95%|█████████▌| 5728/6000 [00:14&lt;00:00, 461.84it/s]sample:  96%|█████████▋| 5781/6000 [00:14&lt;00:00, 480.63it/s]sample:  97%|█████████▋| 5830/6000 [00:14&lt;00:00, 478.41it/s]sample:  98%|█████████▊| 5878/6000 [00:15&lt;00:00, 474.54it/s]sample:  99%|█████████▉| 5926/6000 [00:15&lt;00:00, 462.32it/s]sample: 100%|█████████▉| 5974/6000 [00:15&lt;00:00, 467.11it/s]sample: 100%|██████████| 6000/6000 [00:15&lt;00:00, 390.83it/s]\n\n\n\n                mean       std    median      5.0%     95.0%     n_eff     r_hat\n        mu      4.39      3.30      4.40     -1.00      9.83  35585.46      1.00\n       tau      3.62      3.23      2.77      0.00      7.78  28589.16      1.00\n  theta[0]      6.23      5.57      5.66     -2.33     14.72  35893.88      1.00\n  theta[1]      4.91      4.64      4.80     -2.49     12.31  42347.88      1.00\n  theta[2]      3.92      5.30      4.16     -4.44     12.15  34873.20      1.00\n  theta[3]      4.76      4.77      4.72     -2.90     12.33  42643.90      1.00\n  theta[4]      3.61      4.66      3.86     -3.58     11.25  39974.71      1.00\n  theta[5]      4.08      4.77      4.21     -3.52     11.68  39644.57      1.00\n  theta[6]      6.28      5.07      5.79     -1.62     14.28  40117.39      1.00\n  theta[7]      4.85      5.25      4.76     -3.41     13.11  37310.81      1.00\ntheta_0[0]      0.32      0.99      0.34     -1.27      1.97  40503.70      1.00\ntheta_0[1]      0.10      0.93      0.10     -1.41      1.65  43437.47      1.00\ntheta_0[2]     -0.08      0.97     -0.08     -1.71      1.47  41132.58      1.00\ntheta_0[3]      0.06      0.95      0.06     -1.55      1.58  45070.05      1.00\ntheta_0[4]     -0.17      0.93     -0.17     -1.69      1.36  40592.80      1.00\ntheta_0[5]     -0.07      0.94     -0.07     -1.61      1.48  44577.57      1.00\ntheta_0[6]      0.35      0.96      0.36     -1.16      2.01  38809.45      1.00\ntheta_0[7]      0.08      0.98      0.08     -1.58      1.63  45373.39      1.00\n\nNumber of divergences: 5\n\n\nThis looks much better, both the effective sample size and the r_hat have massively improved for the hyperparameter tau, and the number of divergences is also much lower. However, the fact that there are still divergences tells us that reparameterisation might improve the topology of the posterior parameter space, but there is no guarantee that it will completely eliminate the problem. When doing Bayesian inference, especially with models of complex dependency relationships as in hierarchical models, good techniques are never a sufficient replacement for good thinking."
  },
  {
    "objectID": "stats/eight.html#using-numpyros-reparameterisation-handler",
    "href": "stats/eight.html#using-numpyros-reparameterisation-handler",
    "title": "Eight Schools in NumPyro",
    "section": "Using numpyro’s reparameterisation handler",
    "text": "Using numpyro’s reparameterisation handler\nSince this reparameterisation is so widely used, it has already been implemented in NumPyro. And since reparameterisation in general is so important in probabilistic modelling, NumPyro has implemented a wide suite of them.\nIn probabilistic modeling, although it’s always a good practice to separate modeling from inference, it’s not always easy to do so. As we have seen, how we formulate the model can have a significant impact on the inference performance. When building the model, not only do we need to configure the variable transformations, but we also need to inform the inference engine how to handle these transformed variables. This is where the numpyro.handlers.reparam handler comes in.\n\nfrom numpyro.handlers import reparam\nfrom numpyro.infer.reparam import TransformReparam\nfrom numpyro.distributions.transforms import AffineTransform\nfrom numpyro.distributions import TransformedDistribution\n\ndef es_2(J, sigma):\n    mu = numpyro.sample('mu', dist.Normal(0, 5))\n    tau = numpyro.sample('tau', dist.HalfCauchy(5))\n\n    with numpyro.plate('J', J):\n        with reparam(config={'theta': TransformReparam()}):\n            theta = numpyro.sample(\n                'theta',\n                TransformedDistribution(dist.Normal(0., 1.), AffineTransform(mu, tau)))\n        numpyro.sample('obs', dist.Normal(theta, sigma))\n\nThe process of reparameterisation goes as follows:\n\nStart with a standard Normal distribution dist.Normal(0., 1.),\nTransform it using the affine transformation AffineTransform(mu, tau),\nDenote the result as a TransformedDistribution,\nRegister the transformed variable theta using numpyro.sample,\nInform the inference engine of the reparameterisation using reparam.\n\nProceed with prior predictive sampling.\n\nes_prior_predictive_2 = Predictive(es_2, num_samples=1000)\nes_prior_samples_2 = es_prior_predictive_2(rng_key, J, sigma)\n\nprint_stats('theta', es_prior_samples_2['theta'])\n\nVariable: theta\n  Shape: (1000, 8)\n  Mean: [-1.2455076   2.9482472  -5.281853   -0.06498987  2.3573008   0.31068215\n  3.729895   -1.5531777 ]\n  Variance: [11408.184   8016.4224 36956.      8181.7744  7636.8813  9586.139\n  6544.6753 26078.693 ]\n\n\n\nCondition on the observed data and do inference.\n\nes_conditioned_2 = condition(es_2, data={'obs': y})\nes_nuts_2 = NUTS(es_conditioned_2)\nes_mcmc_2 = MCMC(es_nuts_2, **mcmc_args)\n\nes_mcmc_2.run(rng_key, J, sigma)\nes_mcmc_2.print_summary()\n\n  0%|          | 0/6000 [00:00&lt;?, ?it/s]warmup:   0%|          | 1/6000 [00:01&lt;2:53:24,  1.73s/it]warmup:   0%|          | 26/6000 [00:01&lt;05:05, 19.52it/s] warmup:   1%|          | 46/6000 [00:01&lt;02:40, 37.09it/s]warmup:   1%|▏         | 79/6000 [00:02&lt;01:21, 72.74it/s]warmup:   2%|▏         | 108/6000 [00:02&lt;00:56, 104.35it/s]warmup:   2%|▏         | 133/6000 [00:02&lt;00:45, 129.50it/s]warmup:   3%|▎         | 158/6000 [00:02&lt;00:38, 150.90it/s]warmup:   3%|▎         | 183/6000 [00:02&lt;00:34, 167.18it/s]warmup:   4%|▎         | 219/6000 [00:02&lt;00:27, 210.99it/s]warmup:   4%|▍         | 257/6000 [00:02&lt;00:22, 252.83it/s]warmup:   5%|▍         | 288/6000 [00:02&lt;00:23, 243.61it/s]warmup:   5%|▌         | 317/6000 [00:02&lt;00:22, 251.34it/s]warmup:   6%|▌         | 351/6000 [00:03&lt;00:20, 273.98it/s]warmup:   7%|▋         | 392/6000 [00:03&lt;00:18, 310.97it/s]warmup:   7%|▋         | 433/6000 [00:03&lt;00:16, 337.75it/s]warmup:   8%|▊         | 469/6000 [00:03&lt;00:16, 328.40it/s]warmup:   8%|▊         | 503/6000 [00:03&lt;00:17, 307.71it/s]warmup:   9%|▉         | 535/6000 [00:03&lt;00:17, 308.68it/s]warmup:  10%|▉         | 577/6000 [00:03&lt;00:16, 338.74it/s]warmup:  10%|█         | 619/6000 [00:03&lt;00:14, 360.83it/s]warmup:  11%|█         | 671/6000 [00:03&lt;00:13, 405.19it/s]warmup:  12%|█▏        | 721/6000 [00:03&lt;00:12, 432.39it/s]warmup:  13%|█▎        | 769/6000 [00:04&lt;00:11, 445.78it/s]warmup:  14%|█▎        | 817/6000 [00:04&lt;00:11, 455.41it/s]warmup:  14%|█▍        | 869/6000 [00:04&lt;00:10, 471.77it/s]warmup:  15%|█▌        | 917/6000 [00:04&lt;00:10, 471.07it/s]warmup:  16%|█▌        | 965/6000 [00:04&lt;00:11, 427.63it/s]sample:  17%|█▋        | 1009/6000 [00:04&lt;00:13, 367.46it/s]sample:  18%|█▊        | 1054/6000 [00:04&lt;00:12, 386.48it/s]sample:  18%|█▊        | 1096/6000 [00:04&lt;00:12, 393.05it/s]sample:  19%|█▉        | 1145/6000 [00:04&lt;00:11, 416.99it/s]sample:  20%|█▉        | 1191/6000 [00:05&lt;00:11, 428.01it/s]sample:  21%|██        | 1237/6000 [00:05&lt;00:10, 435.05it/s]sample:  21%|██▏       | 1286/6000 [00:05&lt;00:10, 450.58it/s]sample:  22%|██▏       | 1332/6000 [00:05&lt;00:10, 450.77it/s]sample:  23%|██▎       | 1378/6000 [00:05&lt;00:10, 453.47it/s]sample:  24%|██▍       | 1427/6000 [00:05&lt;00:09, 461.69it/s]sample:  25%|██▍       | 1474/6000 [00:05&lt;00:09, 454.81it/s]sample:  25%|██▌       | 1521/6000 [00:05&lt;00:09, 458.74it/s]sample:  26%|██▌       | 1567/6000 [00:05&lt;00:09, 459.09it/s]sample:  27%|██▋       | 1614/6000 [00:05&lt;00:09, 461.35it/s]sample:  28%|██▊       | 1665/6000 [00:06&lt;00:09, 474.87it/s]sample:  29%|██▊       | 1713/6000 [00:06&lt;00:09, 468.50it/s]sample:  29%|██▉       | 1760/6000 [00:06&lt;00:09, 462.37it/s]sample:  30%|███       | 1807/6000 [00:06&lt;00:09, 453.63it/s]sample:  31%|███       | 1854/6000 [00:06&lt;00:09, 457.83it/s]sample:  32%|███▏      | 1902/6000 [00:06&lt;00:08, 463.80it/s]sample:  33%|███▎      | 1952/6000 [00:06&lt;00:08, 474.05it/s]sample:  33%|███▎      | 2002/6000 [00:06&lt;00:08, 480.34it/s]sample:  34%|███▍      | 2051/6000 [00:06&lt;00:08, 476.93it/s]sample:  35%|███▌      | 2102/6000 [00:07&lt;00:08, 485.69it/s]sample:  36%|███▌      | 2153/6000 [00:07&lt;00:07, 491.42it/s]sample:  37%|███▋      | 2206/6000 [00:07&lt;00:07, 500.20it/s]sample:  38%|███▊      | 2258/6000 [00:07&lt;00:07, 501.94it/s]sample:  38%|███▊      | 2309/6000 [00:07&lt;00:07, 487.69it/s]sample:  39%|███▉      | 2358/6000 [00:07&lt;00:07, 478.91it/s]sample:  40%|████      | 2407/6000 [00:07&lt;00:07, 479.13it/s]sample:  41%|████      | 2457/6000 [00:07&lt;00:07, 483.84it/s]sample:  42%|████▏     | 2506/6000 [00:07&lt;00:07, 477.62it/s]sample:  43%|████▎     | 2556/6000 [00:07&lt;00:07, 483.60it/s]sample:  43%|████▎     | 2605/6000 [00:08&lt;00:07, 476.49it/s]sample:  44%|████▍     | 2653/6000 [00:08&lt;00:07, 474.64it/s]sample:  45%|████▌     | 2701/6000 [00:08&lt;00:07, 468.79it/s]sample:  46%|████▌     | 2748/6000 [00:08&lt;00:07, 460.25it/s]sample:  47%|████▋     | 2795/6000 [00:08&lt;00:06, 461.85it/s]sample:  47%|████▋     | 2842/6000 [00:08&lt;00:06, 461.08it/s]sample:  48%|████▊     | 2890/6000 [00:08&lt;00:06, 463.00it/s]sample:  49%|████▉     | 2938/6000 [00:08&lt;00:06, 466.11it/s]sample:  50%|████▉     | 2985/6000 [00:08&lt;00:06, 457.25it/s]sample:  51%|█████     | 3033/6000 [00:08&lt;00:06, 463.22it/s]sample:  51%|█████▏    | 3080/6000 [00:09&lt;00:06, 456.56it/s]sample:  52%|█████▏    | 3131/6000 [00:09&lt;00:06, 469.63it/s]sample:  53%|█████▎    | 3179/6000 [00:09&lt;00:06, 463.81it/s]sample:  54%|█████▍    | 3229/6000 [00:09&lt;00:05, 474.20it/s]sample:  55%|█████▍    | 3278/6000 [00:09&lt;00:05, 476.66it/s]sample:  55%|█████▌    | 3327/6000 [00:09&lt;00:05, 478.12it/s]sample:  56%|█████▋    | 3375/6000 [00:09&lt;00:05, 475.22it/s]sample:  57%|█████▋    | 3423/6000 [00:09&lt;00:05, 455.69it/s]sample:  58%|█████▊    | 3469/6000 [00:09&lt;00:05, 441.57it/s]sample:  59%|█████▊    | 3521/6000 [00:10&lt;00:05, 458.80it/s]sample:  59%|█████▉    | 3569/6000 [00:10&lt;00:05, 462.90it/s]sample:  60%|██████    | 3616/6000 [00:10&lt;00:05, 464.00it/s]sample:  61%|██████    | 3663/6000 [00:10&lt;00:05, 458.85it/s]sample:  62%|██████▏   | 3712/6000 [00:10&lt;00:04, 465.93it/s]sample:  63%|██████▎   | 3759/6000 [00:10&lt;00:04, 467.11it/s]sample:  63%|██████▎   | 3806/6000 [00:10&lt;00:04, 451.03it/s]sample:  64%|██████▍   | 3853/6000 [00:10&lt;00:04, 454.12it/s]sample:  65%|██████▍   | 3899/6000 [00:10&lt;00:04, 451.29it/s]sample:  66%|██████▌   | 3947/6000 [00:10&lt;00:04, 458.05it/s]sample:  67%|██████▋   | 3995/6000 [00:11&lt;00:04, 461.08it/s]sample:  67%|██████▋   | 4043/6000 [00:11&lt;00:04, 464.11it/s]sample:  68%|██████▊   | 4091/6000 [00:11&lt;00:04, 468.56it/s]sample:  69%|██████▉   | 4138/6000 [00:11&lt;00:04, 464.79it/s]sample:  70%|██████▉   | 4185/6000 [00:11&lt;00:03, 463.96it/s]sample:  71%|███████   | 4232/6000 [00:11&lt;00:03, 448.10it/s]sample:  71%|███████▏  | 4277/6000 [00:11&lt;00:03, 440.27it/s]sample:  72%|███████▏  | 4323/6000 [00:11&lt;00:03, 445.45it/s]sample:  73%|███████▎  | 4368/6000 [00:11&lt;00:03, 432.83it/s]sample:  74%|███████▎  | 4412/6000 [00:12&lt;00:03, 427.08it/s]sample:  74%|███████▍  | 4459/6000 [00:12&lt;00:03, 438.82it/s]sample:  75%|███████▌  | 4506/6000 [00:12&lt;00:03, 447.25it/s]sample:  76%|███████▌  | 4551/6000 [00:12&lt;00:03, 445.39it/s]sample:  77%|███████▋  | 4597/6000 [00:12&lt;00:03, 449.38it/s]sample:  77%|███████▋  | 4642/6000 [00:12&lt;00:03, 445.80it/s]sample:  78%|███████▊  | 4689/6000 [00:12&lt;00:02, 452.08it/s]sample:  79%|███████▉  | 4743/6000 [00:12&lt;00:02, 476.45it/s]sample:  80%|███████▉  | 4793/6000 [00:12&lt;00:02, 482.92it/s]sample:  81%|████████  | 4842/6000 [00:12&lt;00:02, 474.63it/s]sample:  82%|████████▏ | 4890/6000 [00:13&lt;00:02, 466.45it/s]sample:  82%|████████▏ | 4939/6000 [00:13&lt;00:02, 472.34it/s]sample:  83%|████████▎ | 4987/6000 [00:13&lt;00:02, 470.31it/s]sample:  84%|████████▍ | 5035/6000 [00:13&lt;00:02, 464.48it/s]sample:  85%|████████▍ | 5082/6000 [00:13&lt;00:01, 460.32it/s]sample:  86%|████████▌ | 5135/6000 [00:13&lt;00:01, 478.37it/s]sample:  86%|████████▋ | 5184/6000 [00:13&lt;00:01, 478.56it/s]sample:  87%|████████▋ | 5236/6000 [00:13&lt;00:01, 489.32it/s]sample:  88%|████████▊ | 5285/6000 [00:13&lt;00:01, 479.80it/s]sample:  89%|████████▉ | 5334/6000 [00:13&lt;00:01, 475.55it/s]sample:  90%|████████▉ | 5387/6000 [00:14&lt;00:01, 487.88it/s]sample:  91%|█████████ | 5436/6000 [00:14&lt;00:01, 477.41it/s]sample:  91%|█████████▏| 5485/6000 [00:14&lt;00:01, 480.83it/s]sample:  92%|█████████▏| 5534/6000 [00:14&lt;00:00, 481.06it/s]sample:  93%|█████████▎| 5583/6000 [00:14&lt;00:00, 475.80it/s]sample:  94%|█████████▍| 5631/6000 [00:14&lt;00:00, 463.87it/s]sample:  95%|█████████▍| 5679/6000 [00:14&lt;00:00, 468.36it/s]sample:  95%|█████████▌| 5727/6000 [00:14&lt;00:00, 468.57it/s]sample:  96%|█████████▋| 5781/6000 [00:14&lt;00:00, 486.74it/s]sample:  97%|█████████▋| 5831/6000 [00:15&lt;00:00, 489.37it/s]sample:  98%|█████████▊| 5880/6000 [00:15&lt;00:00, 482.09it/s]sample:  99%|█████████▉| 5929/6000 [00:15&lt;00:00, 473.17it/s]sample: 100%|█████████▉| 5979/6000 [00:15&lt;00:00, 479.65it/s]sample: 100%|██████████| 6000/6000 [00:15&lt;00:00, 390.50it/s]\n\n\n\n                   mean       std    median      5.0%     95.0%     n_eff     r_hat\n           mu      4.39      3.30      4.40     -1.00      9.83  35585.46      1.00\n          tau      3.62      3.23      2.77      0.00      7.78  28589.16      1.00\ntheta_base[0]      0.32      0.99      0.34     -1.27      1.97  40503.70      1.00\ntheta_base[1]      0.10      0.93      0.10     -1.41      1.65  43437.47      1.00\ntheta_base[2]     -0.08      0.97     -0.08     -1.71      1.47  41132.58      1.00\ntheta_base[3]      0.06      0.95      0.06     -1.55      1.58  45070.05      1.00\ntheta_base[4]     -0.17      0.93     -0.17     -1.69      1.36  40592.80      1.00\ntheta_base[5]     -0.07      0.94     -0.07     -1.61      1.48  44577.57      1.00\ntheta_base[6]      0.35      0.96      0.36     -1.16      2.01  38809.45      1.00\ntheta_base[7]      0.08      0.98      0.08     -1.58      1.63  45373.39      1.00\n\nNumber of divergences: 5\n\n\nThe results are consistent with the manual reparameterisation. It might seem uncessarily complicated to use the reparameterisation handler in this simple example, but in more complex models, especially those with many layers of dependencies, the reparameterisation handler can greatly facilitate the model building process."
  },
  {
    "objectID": "culture/2021-06-09-emma.html",
    "href": "culture/2021-06-09-emma.html",
    "title": "Emma Zunz",
    "section": "",
    "text": "Emma laissa tomber la lettre. Le premier sentiment, c’est un éboulement dans l’estomac et un tremblement dans les genoux ; et puis, un sentiment d’une culpabilité aveugle, d’une irréalité, d’un froid, et de la peur ; et puis, une envie que la journée soit déjà passée. Puis elle se rendit compte qu’une telle souhaite n’a pas de sens, la mort de son père était le seule évènement qui s’était produit dans le monde, et il aurait perduré sans fin, pour toujours. Elle releva la feuille de papier et alla dans sa chambre. Subrepticement elle la rangea à la garde dans un tiroir, comme si elle connaissait déjà ce qui viendrait. Il se peut qu’elle ait déjà commencé à envisager ce qui se passerait ensuite ; elle était déjà celle qu’elle deviendrait.\n(Des paragraphes omis. Emma prépare son plan de vengeance, une vengeance pour un père qui ne le mérite guère.)\nPendant ce temps, qui est hors du temps, dans le déferlement des sensations fragmentées et horribles, a-t-elle pensé, même pour une seule fois, à la mort qui a inspiré son sacrifice ? A mon avis, elle a réfléchi une fois, et c’était assez pour mettre son but désespéré en danger. Elle pensait (elle ne pouvait pas s’en empêcher) que son père avait fait à sa mère la même chose horrible comme ce qu’était fait à elle en ce moment. Elle y réfléchit avec un étonnement faible et puis, immédiatement, se mit à l’abri dans la vertige. L’homme-un Suédois ou un finlandais, peu importe-ne parle pas l’espagnol; il était un simple instrument pour Emma, tout comme elle pour lui-mais elle était utilisée pour le plaisir et lui, il était pour la justice.\n(Des paragraphes omis. Emma exécute son plan de vengeance qui s’agit un viol par un homme différent qu’était identifié, et une meurtre pour une cause différente qu’était présentée.)\nL’histoire était incroyable, c’est vrai-mais elle a toutefois convaincu tout le monde, car en substance elle était vraie. Le ton d’Emma Zunz était vrai, sa honte était vraie, sa haine était vraie. L’indignation qui était faite à elle était aussi vraie ; tout ce qu’était faux, c’étaient les circonstances, le temps, et un ou deux noms propres."
  },
  {
    "objectID": "stats/2023-06-19-attention.html",
    "href": "stats/2023-06-19-attention.html",
    "title": "Understanding the attention mechanism",
    "section": "",
    "text": "This post explains how the attention mechanism in modern language models works, and how it is accomplished through matrix multiplication."
  },
  {
    "objectID": "stats/2023-06-19-attention.html#token-interaction-as-matrix-multiplication",
    "href": "stats/2023-06-19-attention.html#token-interaction-as-matrix-multiplication",
    "title": "Understanding the attention mechanism",
    "section": "Token interaction as matrix multiplication",
    "text": "Token interaction as matrix multiplication\nIn language modeling, a chuck of text can be tokenised and embedded into a data matrix. In the following (2, 3) data matrix, each row represent the embedding of one token\n\\[\n\\mathbf{X} = \\begin{pmatrix}\nx_{11} & x_{12} & x_{13} \\\\\nx_{21} & x_{22} & x_{23}\n\\end{pmatrix}\n\\]\nHowever, we know that to model the human natural language, it’s not enough to have each token independently embedded; the tokens are also semantically related to each other. For example in the sentence “the art of writting is the art of discovering what you believe”, the meaning of the sentence is not only in the individual words, but also in how they are methodically arranged together. With the words and sentences tokenized as the above data matrix, interactions can easily be realized by left and right multiplying the data matrix with interaction matrices.\nFor example left multiplication it with an (2, 2) interaction matrix \\[\n\\mathbf{L} = \\begin{pmatrix}\nl_{11} & l_{12} \\\\\nl_{21} & l_{22}\n\\end{pmatrix}\n\\]\nwill give us\n\\[\n\\mathbf{L} \\mathbf{X} = \\begin{pmatrix}\nl_{11} & l_{12} \\\\\nl_{21} & l_{22}\n\\end{pmatrix}\n\\begin{pmatrix}\nx_{11} & x_{12} & x_{13} \\\\\nx_{21} & x_{22} & x_{23}\n\\end{pmatrix}\n= \\begin{pmatrix}\nl_{11}x_{11} + l_{12}x_{21} & l_{11}x_{12} + l_{12}x_{22} & l_{11}x_{13} + l_{12}x_{23} \\\\\nl_{21}x_{11} + l_{22}x_{21} & l_{21}x_{12} + l_{22}x_{22} & l_{21}x_{13} + l_{22}x_{23}\n\\end{pmatrix},\n\\]\nwhich is again a (2, 3) matrix, the same as the original data matrix. We notice that each entry in the new data matrix is the weighted sum of the corresponding entries in the original matrix, frome different rows, but from the same column. In this way we achieved the goal of interacting the different tokens with each other. However we should also note that there is no interaction between the different columns of the original matrix.\nSimilarly, right multiplication it with an (3, 2) interaction matrix\n\\[\n\\mathbf{R} = \\begin{pmatrix}\nr_{11} & r_{12} \\\\\nr_{21} & r_{22} \\\\\nr_{31} & r_{32}\n\\end{pmatrix}\n\\]\nwill give us\n\\[\n\\mathbf{X} \\mathbf{R} = \\begin{pmatrix}\nx_{11} & x_{12} & x_{13} \\\\\nx_{21} & x_{22} & x_{23}\n\\end{pmatrix}\n\\begin{pmatrix}\nr_{11} & r_{12} \\\\\nr_{21} & r_{22} \\\\\nr_{31} & r_{32}\n\\end{pmatrix}\n= \\begin{pmatrix}\nx_{11}r_{11} + x_{12}r_{21} + x_{13}r_{31} & x_{11}r_{12} + x_{12}r_{22} + x_{13}r_{32} \\\\\nx_{21}r_{11} + x_{22}r_{21} + x_{23}r_{31} & x_{21}r_{12} + x_{22}r_{22} + x_{23}r_{32}\n\\end{pmatrix},\n\\]\nwhich is a (2, 2) matrix. It has the same number of rows as the original data matrix, but the number of columns is determined by the interaction matrix. We notice that each entry in the new data matrix is the weighted sum of the corresponding entries in the original matrix, from different columns, but from the same row. And there is no interaction between the different rows of the original matrix. In this way we have updated the embedding of each token, but without interacting the tokens with each other.\nCombining left and right multiplication, we can thus both make the tokens interact with each other, and update the embedding of each token. In transformer language models, it’s the repeated application of this operation that allows the model to “understand” the context of the sentence, and represent it in the most meaningful way. This is the backbone of all the large language models.\nThe final interaction mechanism will look like this: \\[\n\\mathbf{Y} = \\mathbf{L} \\mathbf{X} \\mathbf{R}.\n\\]"
  },
  {
    "objectID": "stats/2023-06-19-attention.html#interaction-matrices-in-practice",
    "href": "stats/2023-06-19-attention.html#interaction-matrices-in-practice",
    "title": "Understanding the attention mechanism",
    "section": "Interaction matrices in practice",
    "text": "Interaction matrices in practice\nTo see this in action let’s first define a data matrix.\n\nimport numpy as np\nnp.random.seed(0)\n\nT, E = 4, 5  # sequence length, embedding dimension\nx = np.random.randn(T, E)  # input tensor\nprint(x)\n\n[[ 1.76405235  0.40015721  0.97873798  2.2408932   1.86755799]\n [-0.97727788  0.95008842 -0.15135721 -0.10321885  0.4105985 ]\n [ 0.14404357  1.45427351  0.76103773  0.12167502  0.44386323]\n [ 0.33367433  1.49407907 -0.20515826  0.3130677  -0.85409574]]\n\n\nHere each row is an embedding of one token as a 5 dimensional vector (E=5), and 4 tokens (T=4) are stacked together, from top to bottom. This way we end up with a (4, 5) data matrix.\nNext, let’s start with left multiplication interactions. A diagonal interaction matrix, which means that each token only interacts with itself (i.e. no interaction at all), is the simplest of all the interaction matrices.\n\nL = np.eye(T)\nprint(L, L @ x, sep=\"\\n\")\n\n[[1. 0. 0. 0.]\n [0. 1. 0. 0.]\n [0. 0. 1. 0.]\n [0. 0. 0. 1.]]\n[[ 1.76405235  0.40015721  0.97873798  2.2408932   1.86755799]\n [-0.97727788  0.95008842 -0.15135721 -0.10321885  0.4105985 ]\n [ 0.14404357  1.45427351  0.76103773  0.12167502  0.44386323]\n [ 0.33367433  1.49407907 -0.20515826  0.3130677  -0.85409574]]\n\n\nAs we expected, the embedding matrix is the same as the original x. Next we can make all the tokens equally affecting all others by setting the interaction matrix to be of all ones.\n\nL = np.ones((T, T)) \nprint(L, L @ x, sep=\"\\n\")\n\n[[1. 1. 1. 1.]\n [1. 1. 1. 1.]\n [1. 1. 1. 1.]\n [1. 1. 1. 1.]]\n[[1.26449236 4.29859821 1.38326024 2.57241707 1.86792399]\n [1.26449236 4.29859821 1.38326024 2.57241707 1.86792399]\n [1.26449236 4.29859821 1.38326024 2.57241707 1.86792399]\n [1.26449236 4.29859821 1.38326024 2.57241707 1.86792399]]\n\n\nNow all the rows of the embedding matrix are the same, since we effectively summed up all the rows in the original matrix to create the new ones. A slightly more interesting interaction matrix is the lower triangular matrix:\n\nL = np.tril(np.ones((T, T))) \nprint(L, L @ x, sep=\"\\n\")\n\n[[1. 0. 0. 0.]\n [1. 1. 0. 0.]\n [1. 1. 1. 0.]\n [1. 1. 1. 1.]]\n[[1.76405235 0.40015721 0.97873798 2.2408932  1.86755799]\n [0.78677447 1.35024563 0.82738078 2.13767435 2.27815649]\n [0.93081804 2.80451913 1.5884185  2.25934936 2.72201972]\n [1.26449236 4.29859821 1.38326024 2.57241707 1.86792399]]\n\n\nAfter left multiplying L, the first row (embeddings for the first word) is left as is, the new second row is the sum of the first two rows, the new third row is the sum of the first three, etc. This is a one way interaction mechanism where the tokens come before affect the tokens that come after, but not the other way around.\nOf course the interaction matrix can be any matrix, not just the ones we’ve seen above.\n\nL = np.random.randn(T, T)\nprint(L, L @ x, sep=\"\\n\")\n\n[[-2.55298982  0.6536186   0.8644362  -0.74216502]\n [ 2.26975462 -1.45436567  0.04575852 -0.18718385]\n [ 1.53277921  1.46935877  0.15494743  0.37816252]\n [-0.88778575 -1.98079647 -0.34791215  0.15634897]]\n[[-5.26549961 -0.25232838 -1.78750815 -5.91561089 -3.48191029]\n [ 5.36941815 -0.68663938  2.51485007  5.18336211  3.82192147]\n [ 1.41643325  2.79971404  1.31812887  3.42037269  3.21165905]\n [ 0.37174317 -2.50954735 -0.86595236 -1.77836191 -2.75926583]]\n\n\nWe can no longer identify the relationship between the tokens since, as the name implied, we are now randomly mixing the tokens up.\nOne extra thing to note is that it’s generally not a good idea to simply summing vectors up: when the vectors get extremely long, as is often the case in modern deep learning (to accurately capture the meaning of a word we need some context, the longer the better), the sum of vectors can often explode, which affects the stability and efficiency of the model training. So we’ll use weights the instead, and make sure each row of L sum to one. Things are easy if all the values are positive, say 1, 2, 2, 3, then the sum is 8, and the weights are simply 1/8, 2/8, 2/8, and 3/8. But, it’s not always easy to get meaningful weights, if the entries in L are not always positive. What if the values are 1, -2, 2, 3? And they may even sum to zero, like -1, 2, 2, -3, leaving us with a devided by zero error. The solution we use is simple: first exponentiate all the entries to make sure they are all positive, then compute the weights using these positive numbers.\n\nL = np.random.randn(T, T)\nL = np.exp(L)\nL = L / L.sum(axis=1, keepdims=True)\nprint(L, L @ x, sep=\"\\n\")\n\n[[0.41896738 0.40743534 0.08311088 0.09048641]\n [0.04488369 0.03095732 0.02325121 0.90090777]\n [0.16201575 0.17403762 0.07705739 0.58688924]\n [0.0689366  0.27987549 0.14140365 0.50978426]]\n[[ 0.38306743  0.8108122   0.39307749  0.93524704  0.90934403]\n [ 0.35288226  1.4272138  -0.12788986  0.3822584  -0.6626072 ]\n [ 0.32265065  1.21910435  0.07046752  0.53820807 -0.09302326]\n [ 0.03856185  1.26078951  0.02813675  0.30239341 -0.12898112]]\n\n\nSimilarly we can create a right interaction matrix R to make the columns interact, and thus update the embedding of each token. We can of course apply the same softmax treatment to each column of R so that each column sums to one, but for some reason (which is still beyond my knowledge) this has never been done in practice. So here we’ll stick to the common wisdom. When applying the right interaction matrix, we need to decide on the new embedding dimension H1. (It’s named H1 because we are going to need another H2, as we’ll see shortly.)\n\nH1 = 6\nR = np.random.randn(E, H1)\nprint(R, x @ R, sep=\"\\n\")\n\n[[-0.51080514 -1.18063218 -0.02818223  0.42833187  0.06651722  0.3024719 ]\n [-0.63432209 -0.36274117 -0.67246045 -0.35955316 -0.81314628 -1.7262826 ]\n [ 0.17742614 -0.40178094 -1.63019835  0.46278226 -0.90729836  0.0519454 ]\n [ 0.72909056  0.12898291  1.13940068 -1.23482582  0.40234164 -0.68481009]\n [-0.87079715 -0.57884966 -0.31155253  0.05616534 -1.16514984  0.90082649]]\n[[-0.97371195 -3.41308712  0.05709096 -1.59755613 -2.3704341   0.04139219]\n [-0.56312213  0.61899371 -0.61014338 -0.67973328 -1.22017855 -1.50301919]\n [-1.15883076 -1.24459388 -2.22229344 -0.23431315 -2.33165625 -2.11086604]\n [-0.18257152 -0.31870854 -0.05685886 -0.92377577  0.11453969 -3.47271662]]\n\n\nWe end up with a new embedding matrix of dimension (T, H1)."
  },
  {
    "objectID": "stats/2023-06-19-attention.html#parameterizing-query-key-and-value",
    "href": "stats/2023-06-19-attention.html#parameterizing-query-key-and-value",
    "title": "Understanding the attention mechanism",
    "section": "Parameterizing query, key, and value",
    "text": "Parameterizing query, key, and value\nWe have seen how the left and right interaction matrices can be used to update the token embedding, now it’s time to determine how to populate them. Since we are talking about neural networks, we can just treat L and R as parameters of the network, and learn them from the data. However, as things stand now, once learned, the same parameter matrices will be applied to all sentences, which effectively means that all the sentences will be forced to interact in the same way, which is clearly not what we want. On the contrary the interaction matrices should be bespoke for each sentence; each sentence should has its own way of interaction between the tokens. The natural way to achieve this is to make the interaction matrices not only have learnable parameters, but are also functions of the input tokens.\nWe finally settle on the following interaction mechanism: \\[\n\\begin{align*}\n\\mathbf{Y} &= \\mathbf{L} \\mathbf{X} \\mathbf{R} \\\\\n\\mathbf{L} &= \\text{softmax} \\left( \\mathbf{Q} \\mathbf{K}^\\top \\right) \\\\\n\\mathbf{Q} &= \\mathbf{X} \\mathbf{W}^Q \\\\\n\\mathbf{K} &= \\mathbf{X} \\mathbf{W}^K \\\\\n\\mathbf{V} &= \\mathbf{X} \\mathbf{W}^V = \\mathbf{X} \\mathbf{R}\n\\end{align*}\n\\]\nThe left interaction matrix L is the product of two matrices, the query matrix Q and the key matrix K, each in turn is the product of two matrices, the data matrix and a parameter matrix (Ignore softmax, which is for turning random matrices into weights). Since Q and K are calculated exactly the same way, their difference in naming is only an conventional. Note that since Q and K (and V) are all calculated by right multiplying a parameter matrix, they are all updated embeddings for each token, with no interactions between them, as we’ve discussed earlier. However by multiplying them together\n\\[\n\\mathbf{Q} \\mathbf{K}^\\top = \\left( \\mathbf{X} \\mathbf{W}^Q \\right) \\left( \\mathbf{X} \\mathbf{W}^K \\right)^\\top = \\mathbf{X} \\mathbf{W}^Q {\\mathbf{W}^K}^\\top \\mathbf{X}^\\top,\n\\]\nWe see that the weight matrices are both appearing on the left (for the second X) and right (for the first X), the resulting matrix thus captures all possible information flows among the tokens. To make sure the query and key have compatible dimensions, their parameter matrices should have the same dimensions.\nThe value matrix \\(\\mathbf{V}\\) is just a rewrite of the right interaction \\(\\mathbf{X} \\mathbf{R}\\).\nWe can now implement the whole process.\n\nT, E, H1, H2 = 4, 5, 6, 7\n\nX = np.random.randn(T, E)\n\nW_Q = np.random.randn(E, H2)\nW_K = np.random.randn(E, H2)\nW_V = np.random.randn(E, H1)\n\nQ = X @ W_Q\nK = X @ W_K\nV = X @ W_V\n\nL = np.exp(Q @ K.T)\nL = L / L.sum(axis=1, keepdims=True)\n\nprint(\"Q shape:\", Q.shape)\nprint(\"K shape:\", K.shape)\nprint(\"V shape:\", V.shape)\nprint(\"L shape:\", L.shape)\nprint(\"L @ V shape:\", (L @ V).shape)\nprint(L, V, L @ V, sep='\\n')\n\nQ shape: (4, 7)\nK shape: (4, 7)\nV shape: (4, 6)\nL shape: (4, 4)\nL @ V shape: (4, 6)\n[[9.99996706e-01 3.28680000e-06 7.50610498e-09 2.85006607e-14]\n [9.78226751e-01 1.28560219e-03 2.02652246e-02 2.22422213e-04]\n [3.07277486e-05 1.29535875e-01 8.07295984e-01 6.31374132e-02]\n [2.82603173e-01 6.82619752e-03 7.10526096e-01 4.45339485e-05]]\n[[ 0.48796236 -1.23724751  0.89411441  1.86818403  0.0708713   4.78357836]\n [ 2.45759555 -0.6918528   2.06088201  3.50938285 -0.60840164  3.90835192]\n [-0.94901715 -0.49210324 -0.95973845 -1.99359209 -0.69361156 -1.88892325]\n [-1.75111978 -2.41142737 -5.32539656 -2.93730234 -0.33078805 -0.89377003]]\n[[ 0.48796882 -1.23724571  0.89411823  1.86818939  0.07086906  4.78357544]\n [ 0.46087579 -1.221707    0.85666231  1.79096535  0.05441627  4.64597066]\n [-0.55833713 -0.63918203 -0.84403913 -1.34022418 -0.65964259 -1.07493171]\n [-0.5197037  -0.70413238 -0.41540882 -0.86471954 -0.47696846  0.03636454]]\n\n\nNote that we have used different embedding dimensions for the data matrix(E=5), right interaction matrix(H1=6), and the query/key embedding for the left interaction matrix(H2=7). In practice, since transformer models usually have multiple attention layers stacked together, and each attention layer also has an extra residual connection, the embedding dimensions for the data matrix and the right interaction matrix are usually the same (E=H1). Besides, the query/key embedding for the left interaction matrix is usually also kept the same as the embedding dimension for the data matrix (E=H2).\n\nT, E = 4, 5\n\nX = np.random.randn(T, E)\n\nW_Q = np.random.randn(E, E)\nW_K = np.random.randn(E, E)\nW_V = np.random.randn(E, E)\n\nQ = X @ W_Q\nK = X @ W_K\nV = X @ W_V\n\nL = np.exp(Q @ K.T)\nL = L / L.sum(axis=1, keepdims=True)\n\nprint(\"Q shape:\", Q.shape)\nprint(\"K shape:\", K.shape)\nprint(\"V shape:\", V.shape)\nprint(\"L shape:\", L.shape)\nprint(\"L @ V shape:\", (L @ V).shape)\nprint(L, V, L @ V, sep='\\n')\n\nQ shape: (4, 5)\nK shape: (4, 5)\nV shape: (4, 5)\nL shape: (4, 4)\nL @ V shape: (4, 5)\n[[4.17764387e-09 2.57135620e-06 9.99997419e-01 5.39037660e-09]\n [8.83647632e-01 3.61057169e-02 1.10946675e-05 8.02355563e-02]\n [3.27298619e-01 1.62202688e-02 4.33976229e-01 2.22504884e-01]\n [8.16727896e-01 2.61363556e-04 1.81996886e-01 1.01385435e-03]]\n[[-0.38265795 -1.0078002  -1.28306344  0.87532503  1.31775286]\n [ 0.42510411 -0.71553386  2.17308255 -0.01319503 -0.38305383]\n [ 1.70990215 -0.46195207 -2.27786996 -0.59268283  1.09518226]\n [-1.44638454 -4.79764402  3.20740801 -0.21663068  2.96228223]]\n[[ 1.70989882 -0.46195275 -2.27785848 -0.59268133  1.09517847]\n [-0.4388186  -1.30132189 -0.79799237  0.75561442  1.38829128]\n [ 0.30188115 -1.60943321 -0.65957438 -0.019133    1.55949079]\n [-0.00268587 -0.9122235  -1.45865913  0.60681286  1.27846849]]\n\n\nAnd this is usually what attention in language models looks like."
  },
  {
    "objectID": "stats/2023-06-19-attention.html#masked-attention",
    "href": "stats/2023-06-19-attention.html#masked-attention",
    "title": "Understanding the attention mechanism",
    "section": "Masked attention",
    "text": "Masked attention\nModern Large language models (basically all LLMs from GPT2 on, the so called “decoder only transformers”) are mostly next-word-prediction machines. That is, given a word, we want the model to predict what the next word should be, but we are not interested in what has come before. The information only flows one way, like the rivers to the sea.\nWe have already seen how such one-way forward interaction can be achieved: by making L a lower triangle matrix. We’ll create a mask matrix the same shape as L, and after applying the mask, the lower left part of L will be intact, while the upper right part above the diagonal will be set to negative infinity. This way, after exponentiation, the upper right part of the left interaction matrix will be set to zero, which effectively forbids the later tokens affecting the earlier ones.\n\nT, E = 4, 5\n\nX = np.random.randn(T, E)\n\nW_Q = np.random.randn(E, E)\nW_K = np.random.randn(E, E)\nW_V = np.random.randn(E, E)\n\nQ = X @ W_Q\nK = X @ W_K\nV = X @ W_V\n\nL = Q @ K.T\nmask = np.tril(np.ones(L.shape), k=0)  # Create a lower triangular mask\nL = np.where(mask == 1, L, -np.inf)    # Apply the mask to L, setting upper right entries to -inf\nL = np.exp(L)\nL = L / L.sum(axis=1, keepdims=True)\n\nprint(L, V, L @ V, sep='\\n')\n\n[[1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n [8.73730868e-01 1.26269132e-01 0.00000000e+00 0.00000000e+00]\n [8.75161032e-02 4.22552421e-01 4.89931476e-01 0.00000000e+00]\n [2.44179030e-04 2.07785297e-03 1.42193742e-02 9.83458594e-01]]\n[[-0.64112576  3.93575956 -0.12979446  2.44301203  0.22577084]\n [ 1.43055231 -1.6645132   0.44924118 -1.49425456  0.68901062]\n [ 0.0815931   2.13348808  1.5848282   1.0688065   1.61374919]\n [ 0.28371997  0.39761655  0.89286344 -0.20351501  0.41637134]]\n[[-0.64112576  3.93575956 -0.12979446  2.44301203  0.22577084]\n [-0.37953677  3.22861798 -0.05668014  1.94585679  0.28426373]\n [ 0.58834954  0.68636122  0.95492606  0.10604396  1.10152821]\n [ 0.28300297  0.4188787   0.90153125 -0.18745914  0.43391727]]\n\n\nWe can see that L is in fact a lower triangle matrix."
  },
  {
    "objectID": "stats/2023-06-19-attention.html#conclusion",
    "href": "stats/2023-06-19-attention.html#conclusion",
    "title": "Understanding the attention mechanism",
    "section": "Conclusion",
    "text": "Conclusion\nAnd voilà, we now have gone through the single most import component is modern large language models."
  },
  {
    "objectID": "stats/2022-06-22-eight.html",
    "href": "stats/2022-06-22-eight.html",
    "title": "Eight Schools in NumPyro",
    "section": "",
    "text": "This is an introduction to NumPyro, using the Eight Schools model as example.\nHere we demonstrate the effects of model reparameterisation. Reparameterisation is especially important in hierarchical models, where the joint density tend to have high curvatures.\nimport numpy as np\nimport jax.numpy as jnp\nimport numpyro\nimport numpyro.distributions as dist\nfrom jax import random\nfrom numpyro.infer import MCMC, NUTS, Predictive\n\nrng_key = random.PRNGKey(0)\n\n2024-09-10 12:53:32.254532: W external/xla/xla/service/gpu/nvptx_compiler.cc:836] The NVIDIA driver's CUDA version is 12.2 which is older than the PTX compiler version (12.6.20). Because the driver is older than the PTX compiler version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\nHere we are using the classic eight schools dataset from Gelman et al. We have collected the test score statistics for eight schools, including the mean and standard error. The goal, is to determine whether some schools have done better than others. Note that since we are working with the mean and standard error of eight different schools, we are actually modeling the statistical analysis resutls of some other people: this is essentially a meta analysis problem.\nJ = 8\ny = np.array([28.0, 8.0, -3.0, 7.0, -1.0, 1.0, 18.0, 12.0])\nsigma = np.array([15.0, 10.0, 16.0, 11.0, 9.0, 11.0, 10.0, 18.0])\nVisualize the data.\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import norm\n\nsns.set_theme()\nsns.set_palette(\"Set2\")\n\nfig, ax = plt.subplots(figsize=(8, 4))\nx = np.linspace(-50, 70, 1000)\nfor mean, sd in zip(y, sigma):\n    ax.plot(x, norm.pdf(x, mean, sd))\n\nplt.title('Test Score from Eight Schools')\nplt.savefig('fig/eight.png')"
  },
  {
    "objectID": "stats/2022-06-22-eight.html#the-baseline-model",
    "href": "stats/2022-06-22-eight.html#the-baseline-model",
    "title": "Eight Schools in NumPyro",
    "section": "The baseline model",
    "text": "The baseline model\nThe model we are building is a standard hierarchical model. We assume that the observed school means represent their true mean \\(\\theta_n\\), but corrupted with some Normal noise, and the true means are themselves drawn from another district level distribution, with mean \\(\\mu\\) and standard deviation \\(\\tau\\), which are also modeled with suitable distributions. Essentially it’s Gaussian all the way up, and we have three different levels to consider: student, school, and the whole district level population.\n\\[\n\\begin{align*}\ny_n &\\sim \\text{N} (\\theta_n, \\sigma_n) \\\\\n\\theta_n &\\sim \\text{N} (\\mu, \\tau) \\\\\n\\mu &\\sim \\text{N} (0, 5) \\\\\n\\tau &\\sim \\text{HalfCauchy} (5).\n\\end{align*}\n\\]\nIn NumPyro models are coded as functions.\n\ndef es_0(J, sigma):\n    mu = numpyro.sample('mu', dist.Normal(0, 5))\n    tau = numpyro.sample('tau', dist.HalfCauchy(5))\n\n    with numpyro.plate('J', J):\n        theta = numpyro.sample('theta', dist.Normal(mu, tau))\n        numpyro.sample('obs', dist.Normal(theta, sigma))\n\nNote that the code and the mathematical model are almost identical, except that they go in different directions. In the mathematical model, we start with the observed data, and reason backward to determine how they might be generated. In the code we start with the hyperparameters, and move forward to generate the observed data.\nJ and sigma are data we used to build our model, but they are not part of the model, in the sense that they are not assigned any probability distribution. y, on the other hand, is the central variable of the model, and is named obs in the model.\nnumpyro.plate is used to denote that the variables inside the plate are conditionally independent. Probability distributions are the building blocks of Bayesian models, and NumPyro has a lot of them. In NunPyro, probability distributions are wrappers of JAX random number generators, and they are translated into sampling statements using the numpyro.sample primitive.\nHowever numpyro.sample is used to define the model, not to draw samples from the distribution. To actually draw samples from a distribution, we use numpyro.infer.Predictive. In numpyro each model defines a joint distribution, and since it’s a probability distribution, we can draw samples from it. And since we haven’t conditioned on any data, the samples we draw are from the prior distribution.\nSampling directly from the prior distribution, and inspect the samples, is a good way to check if the model is correctly defined.\n\nes_prior_predictive_0 = Predictive(es_0, num_samples=1000)\nes_prior_samples_0 = es_prior_predictive_0(rng_key, J, sigma)\n\ndef print_stats(name, samples):\n    print(f\"Variable: {name}\")\n    print(f\"  Shape: {samples.shape}\")\n    print(f\"  Mean: {jnp.mean(samples, axis=0)}\")\n    print(f\"  Variance: {jnp.var(samples, axis=0)}\\n\")\n\n# Print statistics for each variable\nprint_stats('mu', es_prior_samples_0['mu'])\nprint_stats('tau', es_prior_samples_0['tau'])\nprint_stats('theta', es_prior_samples_0['theta'])\nprint_stats('obs', es_prior_samples_0['obs'])\n\nVariable: mu\n  Shape: (1000,)\n  Mean: -0.16857339441776276\n  Variance: 26.169570922851562\n\nVariable: tau\n  Shape: (1000,)\n  Mean: 19.337650299072266\n  Variance: 10626.4833984375\n\nVariable: theta\n  Shape: (1000, 8)\n  Mean: [-1.2455076   2.9482472  -5.281853   -0.06498987  2.3573008   0.31068215\n  3.729895   -1.5531777 ]\n  Variance: [11408.184   8016.4224 36956.      8181.7744  7636.8813  9586.139\n  6544.6753 26078.693 ]\n\nVariable: obs\n  Shape: (1000, 8)\n  Mean: [-1.7466605   2.9530644  -4.942445    0.10836948  2.5812457   0.34208587\n  3.1065757  -2.218382  ]\n  Variance: [11653.146   8119.2095 37439.16    8326.833   7699.3613  9800.354\n  6728.329  26469.115 ]\n\n\n\nWhen the samples of some variables are observed, we can condition on these observations, and infer the conditional distributions of the other variables. This process is called inference, and is commonly done using MCMC methods. The conditioning is done using the numpyro.handlers.condition primitive, by feeding it a data dict and the model.\nSince we have a GPU available, we will also configure the MCMC sampler to use vectorized chains.\n\nfrom numpyro.handlers import condition\n\nmcmc_args = {\n    'num_warmup': 1000,\n    'num_samples': 5000,\n    'num_chains': 8,\n    'progress_bar': True,\n    'chain_method': 'vectorized'\n}\n\nes_conditioned_0 = condition(es_0, data={'obs': y})\nes_nuts_0 = NUTS(es_conditioned_0)\nes_mcmc_0 = MCMC(es_nuts_0, **mcmc_args)\n\nes_mcmc_0.run(rng_key, J, sigma)\n\n  0%|          | 0/6000 [00:00&lt;?, ?it/s]warmup:   0%|          | 1/6000 [00:01&lt;2:56:25,  1.76s/it]warmup:   0%|          | 6/6000 [00:01&lt;24:17,  4.11it/s]  warmup:   0%|          | 12/6000 [00:02&lt;11:01,  9.05it/s]warmup:   0%|          | 17/6000 [00:02&lt;07:24, 13.46it/s]warmup:   0%|          | 22/6000 [00:02&lt;05:35, 17.80it/s]warmup:   0%|          | 28/6000 [00:02&lt;04:04, 24.38it/s]warmup:   1%|          | 36/6000 [00:02&lt;02:53, 34.33it/s]warmup:   1%|          | 45/6000 [00:02&lt;02:10, 45.76it/s]warmup:   1%|          | 54/6000 [00:02&lt;01:50, 53.83it/s]warmup:   1%|          | 61/6000 [00:02&lt;01:47, 55.00it/s]warmup:   1%|          | 68/6000 [00:02&lt;01:45, 56.44it/s]warmup:   1%|▏         | 75/6000 [00:03&lt;01:46, 55.54it/s]warmup:   1%|▏         | 83/6000 [00:03&lt;01:35, 61.67it/s]warmup:   2%|▏         | 90/6000 [00:03&lt;01:35, 61.84it/s]warmup:   2%|▏         | 99/6000 [00:03&lt;01:25, 68.81it/s]warmup:   2%|▏         | 121/6000 [00:03&lt;00:53, 109.14it/s]warmup:   2%|▏         | 142/6000 [00:03&lt;00:42, 136.66it/s]warmup:   3%|▎         | 161/6000 [00:03&lt;00:38, 150.02it/s]warmup:   3%|▎         | 177/6000 [00:03&lt;00:39, 148.57it/s]warmup:   3%|▎         | 202/6000 [00:03&lt;00:32, 177.23it/s]warmup:   4%|▎         | 221/6000 [00:04&lt;00:33, 174.99it/s]warmup:   4%|▍         | 244/6000 [00:04&lt;00:30, 190.67it/s]warmup:   4%|▍         | 267/6000 [00:04&lt;00:28, 198.94it/s]warmup:   5%|▍         | 288/6000 [00:04&lt;00:45, 126.24it/s]warmup:   5%|▌         | 305/6000 [00:04&lt;01:00, 93.54it/s] warmup:   5%|▌         | 318/6000 [00:04&lt;00:58, 97.35it/s]warmup:   6%|▌         | 341/6000 [00:05&lt;00:46, 120.61it/s]warmup:   6%|▌         | 369/6000 [00:05&lt;00:36, 153.95it/s]warmup:   7%|▋         | 400/6000 [00:05&lt;00:29, 189.98it/s]warmup:   7%|▋         | 433/6000 [00:05&lt;00:24, 224.31it/s]warmup:   8%|▊         | 459/6000 [00:05&lt;00:25, 217.87it/s]warmup:   8%|▊         | 484/6000 [00:05&lt;00:29, 189.38it/s]warmup:   8%|▊         | 506/6000 [00:05&lt;00:29, 184.35it/s]warmup:   9%|▉         | 537/6000 [00:05&lt;00:25, 213.75it/s]warmup:   9%|▉         | 566/6000 [00:06&lt;00:23, 231.53it/s]warmup:  10%|█         | 600/6000 [00:06&lt;00:20, 259.90it/s]warmup:  11%|█         | 634/6000 [00:06&lt;00:19, 280.08it/s]warmup:  11%|█         | 664/6000 [00:06&lt;00:18, 284.00it/s]warmup:  12%|█▏        | 696/6000 [00:06&lt;00:18, 294.21it/s]warmup:  12%|█▏        | 730/6000 [00:06&lt;00:17, 306.05it/s]warmup:  13%|█▎        | 762/6000 [00:06&lt;00:17, 305.22it/s]warmup:  13%|█▎        | 793/6000 [00:06&lt;00:17, 293.75it/s]warmup:  14%|█▍        | 827/6000 [00:06&lt;00:17, 302.73it/s]warmup:  14%|█▍        | 858/6000 [00:06&lt;00:17, 301.64it/s]warmup:  15%|█▍        | 895/6000 [00:07&lt;00:16, 318.82it/s]warmup:  15%|█▌        | 928/6000 [00:07&lt;00:16, 314.52it/s]warmup:  16%|█▌        | 960/6000 [00:07&lt;00:16, 310.15it/s]warmup:  17%|█▋        | 992/6000 [00:07&lt;00:19, 260.49it/s]sample:  17%|█▋        | 1027/6000 [00:07&lt;00:17, 282.19it/s]sample:  18%|█▊        | 1066/6000 [00:07&lt;00:15, 309.57it/s]sample:  18%|█▊        | 1104/6000 [00:07&lt;00:15, 325.16it/s]sample:  19%|█▉        | 1140/6000 [00:07&lt;00:14, 334.86it/s]sample:  20%|█▉        | 1175/6000 [00:07&lt;00:14, 336.58it/s]sample:  20%|██        | 1213/6000 [00:08&lt;00:13, 348.33it/s]sample:  21%|██        | 1249/6000 [00:08&lt;00:13, 344.27it/s]sample:  22%|██▏       | 1290/6000 [00:08&lt;00:13, 361.91it/s]sample:  22%|██▏       | 1327/6000 [00:08&lt;00:12, 360.87it/s]sample:  23%|██▎       | 1364/6000 [00:08&lt;00:12, 359.77it/s]sample:  23%|██▎       | 1401/6000 [00:08&lt;00:12, 361.39it/s]sample:  24%|██▍       | 1438/6000 [00:08&lt;00:12, 360.32it/s]sample:  25%|██▍       | 1475/6000 [00:08&lt;00:12, 362.79it/s]sample:  25%|██▌       | 1512/6000 [00:08&lt;00:12, 354.32it/s]sample:  26%|██▌       | 1548/6000 [00:08&lt;00:12, 352.36it/s]sample:  27%|██▋       | 1591/6000 [00:09&lt;00:11, 373.50it/s]sample:  27%|██▋       | 1629/6000 [00:09&lt;00:11, 374.66it/s]sample:  28%|██▊       | 1667/6000 [00:09&lt;00:11, 370.82it/s]sample:  28%|██▊       | 1705/6000 [00:09&lt;00:11, 369.07it/s]sample:  29%|██▉       | 1742/6000 [00:09&lt;00:11, 362.57it/s]sample:  30%|██▉       | 1779/6000 [00:09&lt;00:11, 356.03it/s]sample:  30%|███       | 1816/6000 [00:09&lt;00:11, 358.07it/s]sample:  31%|███       | 1852/6000 [00:09&lt;00:11, 353.28it/s]sample:  31%|███▏      | 1889/6000 [00:09&lt;00:11, 355.46it/s]sample:  32%|███▏      | 1925/6000 [00:10&lt;00:11, 349.90it/s]sample:  33%|███▎      | 1962/6000 [00:10&lt;00:11, 354.37it/s]sample:  33%|███▎      | 1998/6000 [00:10&lt;00:11, 353.08it/s]sample:  34%|███▍      | 2034/6000 [00:10&lt;00:11, 354.33it/s]sample:  35%|███▍      | 2074/6000 [00:10&lt;00:10, 367.15it/s]sample:  35%|███▌      | 2113/6000 [00:10&lt;00:10, 372.92it/s]sample:  36%|███▌      | 2152/6000 [00:10&lt;00:10, 376.60it/s]sample:  36%|███▋      | 2190/6000 [00:10&lt;00:10, 369.78it/s]sample:  37%|███▋      | 2228/6000 [00:10&lt;00:10, 367.90it/s]sample:  38%|███▊      | 2265/6000 [00:10&lt;00:10, 360.60it/s]sample:  38%|███▊      | 2302/6000 [00:11&lt;00:10, 345.14it/s]sample:  39%|███▉      | 2337/6000 [00:11&lt;00:10, 346.37it/s]sample:  40%|███▉      | 2372/6000 [00:11&lt;00:10, 343.46it/s]sample:  40%|████      | 2410/6000 [00:11&lt;00:10, 350.77it/s]sample:  41%|████      | 2446/6000 [00:11&lt;00:10, 351.44it/s]sample:  41%|████▏     | 2482/6000 [00:11&lt;00:09, 352.65it/s]sample:  42%|████▏     | 2520/6000 [00:11&lt;00:09, 359.00it/s]sample:  43%|████▎     | 2556/6000 [00:11&lt;00:09, 356.57it/s]sample:  43%|████▎     | 2592/6000 [00:11&lt;00:09, 354.29it/s]sample:  44%|████▍     | 2628/6000 [00:12&lt;00:09, 354.52it/s]sample:  44%|████▍     | 2668/6000 [00:12&lt;00:09, 365.91it/s]sample:  45%|████▌     | 2708/6000 [00:12&lt;00:08, 375.90it/s]sample:  46%|████▌     | 2747/6000 [00:12&lt;00:08, 377.68it/s]sample:  46%|████▋     | 2785/6000 [00:12&lt;00:08, 377.68it/s]sample:  47%|████▋     | 2823/6000 [00:12&lt;00:08, 367.04it/s]sample:  48%|████▊     | 2863/6000 [00:12&lt;00:08, 374.49it/s]sample:  48%|████▊     | 2903/6000 [00:12&lt;00:08, 381.45it/s]sample:  49%|████▉     | 2942/6000 [00:12&lt;00:08, 375.67it/s]sample:  50%|████▉     | 2984/6000 [00:12&lt;00:07, 386.20it/s]sample:  50%|█████     | 3023/6000 [00:13&lt;00:07, 375.53it/s]sample:  51%|█████     | 3063/6000 [00:13&lt;00:07, 381.42it/s]sample:  52%|█████▏    | 3102/6000 [00:13&lt;00:08, 359.13it/s]sample:  52%|█████▏    | 3139/6000 [00:13&lt;00:07, 359.58it/s]sample:  53%|█████▎    | 3177/6000 [00:13&lt;00:07, 364.69it/s]sample:  54%|█████▎    | 3214/6000 [00:13&lt;00:07, 353.20it/s]sample:  54%|█████▍    | 3250/6000 [00:13&lt;00:07, 352.18it/s]sample:  55%|█████▍    | 3287/6000 [00:13&lt;00:07, 356.46it/s]sample:  55%|█████▌    | 3323/6000 [00:13&lt;00:07, 357.41it/s]sample:  56%|█████▌    | 3362/6000 [00:13&lt;00:07, 365.88it/s]sample:  57%|█████▋    | 3399/6000 [00:14&lt;00:07, 367.00it/s]sample:  57%|█████▋    | 3436/6000 [00:14&lt;00:07, 339.03it/s]sample:  58%|█████▊    | 3472/6000 [00:14&lt;00:07, 342.28it/s]sample:  58%|█████▊    | 3510/6000 [00:14&lt;00:07, 351.65it/s]sample:  59%|█████▉    | 3551/6000 [00:14&lt;00:06, 365.24it/s]sample:  60%|█████▉    | 3588/6000 [00:14&lt;00:06, 363.23it/s]sample:  60%|██████    | 3625/6000 [00:14&lt;00:06, 354.99it/s]sample:  61%|██████    | 3664/6000 [00:14&lt;00:06, 362.97it/s]sample:  62%|██████▏   | 3701/6000 [00:14&lt;00:06, 360.94it/s]sample:  62%|██████▏   | 3738/6000 [00:15&lt;00:06, 362.44it/s]sample:  63%|██████▎   | 3775/6000 [00:15&lt;00:06, 361.03it/s]sample:  64%|██████▎   | 3813/6000 [00:15&lt;00:05, 365.52it/s]sample:  64%|██████▍   | 3851/6000 [00:15&lt;00:05, 369.20it/s]sample:  65%|██████▍   | 3889/6000 [00:15&lt;00:05, 370.49it/s]sample:  65%|██████▌   | 3927/6000 [00:15&lt;00:05, 370.24it/s]sample:  66%|██████▌   | 3965/6000 [00:15&lt;00:05, 364.54it/s]sample:  67%|██████▋   | 4003/6000 [00:15&lt;00:05, 367.68it/s]sample:  67%|██████▋   | 4040/6000 [00:15&lt;00:05, 363.75it/s]sample:  68%|██████▊   | 4077/6000 [00:15&lt;00:05, 363.29it/s]sample:  69%|██████▊   | 4114/6000 [00:16&lt;00:05, 363.71it/s]sample:  69%|██████▉   | 4151/6000 [00:16&lt;00:05, 359.68it/s]sample:  70%|██████▉   | 4190/6000 [00:16&lt;00:04, 365.62it/s]sample:  71%|███████   | 4231/6000 [00:16&lt;00:04, 375.54it/s]sample:  71%|███████   | 4269/6000 [00:16&lt;00:04, 368.07it/s]sample:  72%|███████▏  | 4306/6000 [00:16&lt;00:04, 359.10it/s]sample:  72%|███████▏  | 4342/6000 [00:16&lt;00:04, 358.88it/s]sample:  73%|███████▎  | 4378/6000 [00:16&lt;00:04, 358.51it/s]sample:  74%|███████▎  | 4414/6000 [00:16&lt;00:04, 349.85it/s]sample:  74%|███████▍  | 4450/6000 [00:17&lt;00:04, 344.07it/s]sample:  75%|███████▍  | 4485/6000 [00:17&lt;00:04, 344.59it/s]sample:  75%|███████▌  | 4523/6000 [00:17&lt;00:04, 354.43it/s]sample:  76%|███████▌  | 4559/6000 [00:17&lt;00:04, 351.98it/s]sample:  77%|███████▋  | 4597/6000 [00:17&lt;00:03, 359.29it/s]sample:  77%|███████▋  | 4634/6000 [00:17&lt;00:03, 361.06it/s]sample:  78%|███████▊  | 4671/6000 [00:17&lt;00:03, 358.32it/s]sample:  78%|███████▊  | 4707/6000 [00:17&lt;00:03, 354.99it/s]sample:  79%|███████▉  | 4745/6000 [00:17&lt;00:03, 358.24it/s]sample:  80%|███████▉  | 4785/6000 [00:17&lt;00:03, 368.12it/s]sample:  80%|████████  | 4825/6000 [00:18&lt;00:03, 374.32it/s]sample:  81%|████████  | 4863/6000 [00:18&lt;00:03, 373.06it/s]sample:  82%|████████▏ | 4901/6000 [00:18&lt;00:03, 357.02it/s]sample:  82%|████████▏ | 4937/6000 [00:18&lt;00:03, 345.55it/s]sample:  83%|████████▎ | 4973/6000 [00:18&lt;00:02, 347.70it/s]sample:  84%|████████▎ | 5013/6000 [00:18&lt;00:02, 360.43it/s]sample:  84%|████████▍ | 5050/6000 [00:18&lt;00:02, 357.37it/s]sample:  85%|████████▍ | 5091/6000 [00:18&lt;00:02, 371.87it/s]sample:  86%|████████▌ | 5130/6000 [00:18&lt;00:02, 374.76it/s]sample:  86%|████████▌ | 5168/6000 [00:18&lt;00:02, 370.66it/s]sample:  87%|████████▋ | 5207/6000 [00:19&lt;00:02, 375.37it/s]sample:  87%|████████▋ | 5245/6000 [00:19&lt;00:02, 356.83it/s]sample:  88%|████████▊ | 5281/6000 [00:19&lt;00:02, 354.05it/s]sample:  89%|████████▊ | 5319/6000 [00:19&lt;00:01, 361.06it/s]sample:  89%|████████▉ | 5359/6000 [00:19&lt;00:01, 369.43it/s]sample:  90%|████████▉ | 5397/6000 [00:19&lt;00:01, 371.74it/s]sample:  91%|█████████ | 5436/6000 [00:19&lt;00:01, 375.16it/s]sample:  91%|█████████▏| 5476/6000 [00:19&lt;00:01, 382.36it/s]sample:  92%|█████████▏| 5515/6000 [00:19&lt;00:01, 381.99it/s]sample:  93%|█████████▎| 5554/6000 [00:20&lt;00:01, 379.83it/s]sample:  93%|█████████▎| 5593/6000 [00:20&lt;00:01, 371.27it/s]sample:  94%|█████████▍| 5631/6000 [00:20&lt;00:01, 365.11it/s]sample:  94%|█████████▍| 5668/6000 [00:20&lt;00:00, 365.91it/s]sample:  95%|█████████▌| 5705/6000 [00:20&lt;00:00, 355.23it/s]sample:  96%|█████████▌| 5742/6000 [00:20&lt;00:00, 355.68it/s]sample:  96%|█████████▋| 5779/6000 [00:20&lt;00:00, 357.38it/s]sample:  97%|█████████▋| 5818/6000 [00:20&lt;00:00, 365.50it/s]sample:  98%|█████████▊| 5855/6000 [00:20&lt;00:00, 357.41it/s]sample:  98%|█████████▊| 5893/6000 [00:20&lt;00:00, 360.50it/s]sample:  99%|█████████▉| 5930/6000 [00:21&lt;00:00, 360.57it/s]sample:  99%|█████████▉| 5967/6000 [00:21&lt;00:00, 356.93it/s]sample: 100%|██████████| 6000/6000 [00:21&lt;00:00, 282.02it/s]\n\n\nThe NUTS sampler is a variant of the Hamiltonian Monte Carlo (HMC) sampler, which is a powerful tool for sampling from complex probability distributions. HMC also comes with its own set of diagnostics, which can be used to check the convergence of the Markov Chain. The most important ones are the effective sample size and the Gelman-Rubin statistic, which is a measure of the convergence of the Markov Chain.\n\nes_mcmc_0.print_summary()\n\n\n                mean       std    median      5.0%     95.0%     n_eff     r_hat\n        mu      4.26      3.34      4.25     -1.24      9.72   3163.83      1.00\n       tau      3.91      3.17      3.03      0.45      8.08   2151.46      1.01\n  theta[0]      6.26      5.81      5.64     -2.93     15.04   5795.74      1.00\n  theta[1]      4.86      4.78      4.74     -2.97     12.48   6468.43      1.00\n  theta[2]      3.71      5.40      3.97     -4.63     12.37   6937.61      1.00\n  theta[3]      4.66      4.94      4.55     -3.62     12.34   6752.22      1.00\n  theta[4]      3.41      4.71      3.64     -4.15     10.92   5415.63      1.00\n  theta[5]      3.86      4.97      4.01     -4.12     11.83   6923.98      1.00\n  theta[6]      6.36      5.27      5.75     -2.32     14.38   4727.40      1.00\n  theta[7]      4.72      5.50      4.59     -3.78     13.50   7525.65      1.00\n\nNumber of divergences: 1327\n\n\nThe effective sample size for \\(\\tau\\) is low, and judging from the r_hat value, the Markov Chain might not have converged. Also, the large number of divergences is a sign that the model might not be well specified. Here we are looking at a prominent problem in hierarchical modeling, known as Radford’s funnel, where the posterior distribution has a very sharp peak at the center, and a long tail, and the curvature of the distribution is very high. This seriously hinders the performance of HMC, and the divergences are a sign that the sampler is having trouble exploring the space.\nHowever, the issue can be readily rectified using a non-centered parameterization."
  },
  {
    "objectID": "stats/2022-06-22-eight.html#manual-reparameterisation",
    "href": "stats/2022-06-22-eight.html#manual-reparameterisation",
    "title": "Eight Schools in NumPyro",
    "section": "Manual reparameterisation",
    "text": "Manual reparameterisation\nThe remedy we are proposing is quite simple: replacing\n\\[\n\\theta_n \\sim \\text{N} (\\mu, \\tau)\n\\]\nwith\n\\[\n\\begin{align*}\n\\theta_n &= \\mu + \\tau  \\theta_0 \\\\\n\\theta_0 &\\sim \\text{N} (0, 1).\n\\end{align*}\n\\]\nIn essence, instead of drawing from a Normal distribution whose parameters are themselves variables in the model, we draw from the unit Normal distribution, and transform it to get the variable we want. By doing so we untangled the sampling process of \\(\\theta\\) from that of \\(\\mu\\) and \\(\\tau\\).\n\ndef es_1(J, sigma):\n    mu = numpyro.sample('mu', dist.Normal(0, 5))\n    tau = numpyro.sample('tau', dist.HalfCauchy(5))\n\n    with numpyro.plate('J', J):\n        theta_0 = numpyro.sample('theta_0', dist.Normal(0, 1))\n        theta = numpyro.deterministic('theta', mu + theta_0 * tau)\n        numpyro.sample('obs', dist.Normal(theta, sigma))\n\nHere we use another primitive, numpyro.deterministic, to register the transformed variable, so that its values can be stored and used later.\n\nes_prior_predictive_1 = Predictive(es_1, num_samples=1000)\nes_prior_samples_1 = es_prior_predictive_1(rng_key, J, sigma)\n\nprint_stats('theta_0', es_prior_samples_1['theta_0'])\n\nVariable: theta_0\n  Shape: (1000, 8)\n  Mean: [ 0.035466    0.00418693  0.04043639 -0.01104304  0.03547993  0.0202828\n  0.01902334  0.01258929]\n  Variance: [1.0026257 0.9806318 0.9726549 0.9744275 0.9668162 0.9936419 1.0036482\n 1.0627806]\n\n\n\nCondition on the observed data and do inference.\n\nes_conditioned_1 = condition(es_1, data={'obs': y})\nes_nuts_1 = NUTS(es_conditioned_1)\nes_mcmc_1 = MCMC(es_nuts_1, **mcmc_args)\n\nes_mcmc_1.run(rng_key, J, sigma)\nes_mcmc_1.print_summary(exclude_deterministic=False)\n\n  0%|          | 0/6000 [00:00&lt;?, ?it/s]warmup:   0%|          | 1/6000 [00:01&lt;2:49:21,  1.69s/it]warmup:   1%|          | 40/6000 [00:01&lt;03:13, 30.87it/s] warmup:   1%|▏         | 89/6000 [00:01&lt;01:17, 76.66it/s]warmup:   2%|▏         | 130/6000 [00:01&lt;00:49, 117.99it/s]warmup:   3%|▎         | 168/6000 [00:02&lt;00:37, 157.27it/s]warmup:   4%|▎         | 219/6000 [00:02&lt;00:26, 219.49it/s]warmup:   5%|▍         | 271/6000 [00:02&lt;00:20, 278.86it/s]warmup:   5%|▌         | 316/6000 [00:02&lt;00:18, 313.88it/s]warmup:   6%|▋         | 375/6000 [00:02&lt;00:14, 378.05it/s]warmup:   7%|▋         | 438/6000 [00:02&lt;00:12, 441.77it/s]warmup:   8%|▊         | 491/6000 [00:02&lt;00:12, 441.30it/s]warmup:   9%|▉         | 543/6000 [00:02&lt;00:11, 461.49it/s]warmup:  10%|█         | 606/6000 [00:02&lt;00:10, 506.33it/s]warmup:  11%|█▏        | 679/6000 [00:03&lt;00:09, 568.96it/s]warmup:  13%|█▎        | 756/6000 [00:03&lt;00:08, 625.37it/s]warmup:  14%|█▍        | 829/6000 [00:03&lt;00:07, 655.46it/s]warmup:  15%|█▌        | 900/6000 [00:03&lt;00:07, 670.43it/s]warmup:  16%|█▌        | 969/6000 [00:03&lt;00:07, 644.71it/s]sample:  17%|█▋        | 1035/6000 [00:03&lt;00:08, 597.46it/s]sample:  18%|█▊        | 1099/6000 [00:03&lt;00:08, 607.09it/s]sample:  19%|█▉        | 1168/6000 [00:03&lt;00:07, 629.50it/s]sample:  21%|██        | 1235/6000 [00:03&lt;00:07, 639.45it/s]sample:  22%|██▏       | 1305/6000 [00:03&lt;00:07, 654.66it/s]sample:  23%|██▎       | 1371/6000 [00:04&lt;00:07, 650.03it/s]sample:  24%|██▍       | 1439/6000 [00:04&lt;00:06, 658.12it/s]sample:  25%|██▌       | 1506/6000 [00:04&lt;00:06, 656.97it/s]sample:  26%|██▋       | 1575/6000 [00:04&lt;00:06, 665.20it/s]sample:  27%|██▋       | 1644/6000 [00:04&lt;00:06, 670.59it/s]sample:  29%|██▊       | 1714/6000 [00:04&lt;00:06, 676.79it/s]sample:  30%|██▉       | 1782/6000 [00:04&lt;00:06, 667.70it/s]sample:  31%|███       | 1849/6000 [00:04&lt;00:06, 665.51it/s]sample:  32%|███▏      | 1919/6000 [00:04&lt;00:06, 674.07it/s]sample:  33%|███▎      | 1991/6000 [00:04&lt;00:05, 686.42it/s]sample:  34%|███▍      | 2060/6000 [00:05&lt;00:05, 682.15it/s]sample:  36%|███▌      | 2132/6000 [00:05&lt;00:05, 692.86it/s]sample:  37%|███▋      | 2205/6000 [00:05&lt;00:05, 703.76it/s]sample:  38%|███▊      | 2277/6000 [00:05&lt;00:05, 707.33it/s]sample:  39%|███▉      | 2348/6000 [00:05&lt;00:05, 687.87it/s]sample:  40%|████      | 2417/6000 [00:05&lt;00:05, 685.97it/s]sample:  41%|████▏     | 2489/6000 [00:05&lt;00:05, 693.33it/s]sample:  43%|████▎     | 2560/6000 [00:05&lt;00:04, 697.98it/s]sample:  44%|████▍     | 2630/6000 [00:05&lt;00:04, 693.00it/s]sample:  45%|████▌     | 2701/6000 [00:06&lt;00:04, 696.82it/s]sample:  46%|████▌     | 2771/6000 [00:06&lt;00:04, 694.22it/s]sample:  47%|████▋     | 2841/6000 [00:06&lt;00:04, 695.12it/s]sample:  49%|████▊     | 2911/6000 [00:06&lt;00:04, 693.80it/s]sample:  50%|████▉     | 2981/6000 [00:06&lt;00:04, 686.49it/s]sample:  51%|█████     | 3051/6000 [00:06&lt;00:04, 690.30it/s]sample:  52%|█████▏    | 3121/6000 [00:06&lt;00:04, 682.51it/s]sample:  53%|█████▎    | 3190/6000 [00:06&lt;00:04, 677.32it/s]sample:  54%|█████▍    | 3260/6000 [00:06&lt;00:04, 683.92it/s]sample:  56%|█████▌    | 3332/6000 [00:06&lt;00:03, 692.53it/s]sample:  57%|█████▋    | 3402/6000 [00:07&lt;00:03, 679.85it/s]sample:  58%|█████▊    | 3471/6000 [00:07&lt;00:03, 664.04it/s]sample:  59%|█████▉    | 3542/6000 [00:07&lt;00:03, 677.12it/s]sample:  60%|██████    | 3611/6000 [00:07&lt;00:03, 678.11it/s]sample:  61%|██████▏   | 3679/6000 [00:07&lt;00:03, 674.24it/s]sample:  62%|██████▏   | 3749/6000 [00:07&lt;00:03, 679.44it/s]sample:  64%|██████▎   | 3817/6000 [00:07&lt;00:03, 674.91it/s]sample:  65%|██████▍   | 3886/6000 [00:07&lt;00:03, 676.62it/s]sample:  66%|██████▌   | 3959/6000 [00:07&lt;00:02, 688.60it/s]sample:  67%|██████▋   | 4030/6000 [00:07&lt;00:02, 692.98it/s]sample:  68%|██████▊   | 4102/6000 [00:08&lt;00:02, 700.35it/s]sample:  70%|██████▉   | 4173/6000 [00:08&lt;00:02, 691.62it/s]sample:  71%|███████   | 4243/6000 [00:08&lt;00:02, 683.16it/s]sample:  72%|███████▏  | 4312/6000 [00:08&lt;00:02, 676.49it/s]sample:  73%|███████▎  | 4380/6000 [00:08&lt;00:02, 675.39it/s]sample:  74%|███████▍  | 4448/6000 [00:08&lt;00:02, 672.14it/s]sample:  75%|███████▌  | 4516/6000 [00:08&lt;00:02, 672.23it/s]sample:  76%|███████▋  | 4584/6000 [00:08&lt;00:02, 667.97it/s]sample:  78%|███████▊  | 4654/6000 [00:08&lt;00:01, 676.12it/s]sample:  79%|███████▉  | 4728/6000 [00:08&lt;00:01, 694.89it/s]sample:  80%|████████  | 4802/6000 [00:09&lt;00:01, 706.84it/s]sample:  81%|████████  | 4873/6000 [00:09&lt;00:01, 684.89it/s]sample:  82%|████████▏ | 4943/6000 [00:09&lt;00:01, 688.55it/s]sample:  84%|████████▎ | 5012/6000 [00:09&lt;00:01, 681.95it/s]sample:  85%|████████▍ | 5081/6000 [00:09&lt;00:01, 667.94it/s]sample:  86%|████████▌ | 5152/6000 [00:09&lt;00:01, 680.12it/s]sample:  87%|████████▋ | 5225/6000 [00:09&lt;00:01, 691.18it/s]sample:  88%|████████▊ | 5295/6000 [00:09&lt;00:01, 685.10it/s]sample:  89%|████████▉ | 5364/6000 [00:09&lt;00:00, 685.47it/s]sample:  91%|█████████ | 5433/6000 [00:10&lt;00:00, 685.31it/s]sample:  92%|█████████▏| 5503/6000 [00:10&lt;00:00, 687.00it/s]sample:  93%|█████████▎| 5572/6000 [00:10&lt;00:00, 679.74it/s]sample:  94%|█████████▍| 5641/6000 [00:10&lt;00:00, 670.24it/s]sample:  95%|█████████▌| 5709/6000 [00:10&lt;00:00, 673.05it/s]sample:  96%|█████████▋| 5782/6000 [00:10&lt;00:00, 687.45it/s]sample:  98%|█████████▊| 5854/6000 [00:10&lt;00:00, 695.72it/s]sample:  99%|█████████▊| 5924/6000 [00:10&lt;00:00, 677.24it/s]sample: 100%|█████████▉| 5993/6000 [00:10&lt;00:00, 679.18it/s]sample: 100%|██████████| 6000/6000 [00:10&lt;00:00, 552.68it/s]\n\n\n\n                mean       std    median      5.0%     95.0%     n_eff     r_hat\n        mu      4.39      3.30      4.40     -1.00      9.83  35585.46      1.00\n       tau      3.62      3.23      2.77      0.00      7.78  28589.16      1.00\n  theta[0]      6.23      5.57      5.66     -2.33     14.72  35893.88      1.00\n  theta[1]      4.91      4.64      4.80     -2.49     12.31  42347.88      1.00\n  theta[2]      3.92      5.30      4.16     -4.44     12.15  34873.20      1.00\n  theta[3]      4.76      4.77      4.72     -2.90     12.33  42643.90      1.00\n  theta[4]      3.61      4.66      3.86     -3.58     11.25  39974.71      1.00\n  theta[5]      4.08      4.77      4.21     -3.52     11.68  39644.57      1.00\n  theta[6]      6.28      5.07      5.79     -1.62     14.28  40117.39      1.00\n  theta[7]      4.85      5.25      4.76     -3.41     13.11  37310.81      1.00\ntheta_0[0]      0.32      0.99      0.34     -1.27      1.97  40503.70      1.00\ntheta_0[1]      0.10      0.93      0.10     -1.41      1.65  43437.47      1.00\ntheta_0[2]     -0.08      0.97     -0.08     -1.71      1.47  41132.58      1.00\ntheta_0[3]      0.06      0.95      0.06     -1.55      1.58  45070.05      1.00\ntheta_0[4]     -0.17      0.93     -0.17     -1.69      1.36  40592.80      1.00\ntheta_0[5]     -0.07      0.94     -0.07     -1.61      1.48  44577.57      1.00\ntheta_0[6]      0.35      0.96      0.36     -1.16      2.01  38809.45      1.00\ntheta_0[7]      0.08      0.98      0.08     -1.58      1.63  45373.39      1.00\n\nNumber of divergences: 5\n\n\nThis looks much better, both the effective sample size and the r_hat have massively improved for the hyperparameter tau, and the number of divergences is also much lower. However, the fact that there are still divergences tells us that reparameterisation might improve the topology of the posterior parameter space, but there is no guarantee that it will completely eliminate the problem. When doing Bayesian inference, especially with models of complex dependency relationships as in hierarchical models, good techniques are never a sufficient replacement for good thinking."
  },
  {
    "objectID": "stats/2022-06-22-eight.html#using-numpyros-reparameterisation-handler",
    "href": "stats/2022-06-22-eight.html#using-numpyros-reparameterisation-handler",
    "title": "Eight Schools in NumPyro",
    "section": "Using numpyro’s reparameterisation handler",
    "text": "Using numpyro’s reparameterisation handler\nSince this reparameterisation is so widely used, it has already been implemented in NumPyro. And since reparameterisation in general is so important in probabilistic modelling, NumPyro has implemented a wide suite of them.\nIn probabilistic modeling, although it’s always a good practice to separate modeling from inference, it’s not always easy to do so. As we have seen, how we formulate the model can have a significant impact on the inference performance. When building the model, not only do we need to configure the variable transformations, but we also need to inform the inference engine how to handle these transformed variables. This is where the numpyro.handlers.reparam handler comes in.\n\nfrom numpyro.handlers import reparam\nfrom numpyro.infer.reparam import TransformReparam\nfrom numpyro.distributions.transforms import AffineTransform\nfrom numpyro.distributions import TransformedDistribution\n\ndef es_2(J, sigma):\n    mu = numpyro.sample('mu', dist.Normal(0, 5))\n    tau = numpyro.sample('tau', dist.HalfCauchy(5))\n\n    with numpyro.plate('J', J):\n        with reparam(config={'theta': TransformReparam()}):\n            theta = numpyro.sample(\n                'theta',\n                TransformedDistribution(dist.Normal(0., 1.), AffineTransform(mu, tau)))\n        numpyro.sample('obs', dist.Normal(theta, sigma))\n\nThe process of reparameterisation goes as follows:\n\nStart with a standard Normal distribution dist.Normal(0., 1.),\nTransform it using the affine transformation AffineTransform(mu, tau),\nDenote the result as a TransformedDistribution,\nRegister the transformed variable theta using numpyro.sample,\nInform the inference engine of the reparameterisation using reparam.\n\nProceed with prior predictive sampling.\n\nes_prior_predictive_2 = Predictive(es_2, num_samples=1000)\nes_prior_samples_2 = es_prior_predictive_2(rng_key, J, sigma)\n\nprint_stats('theta', es_prior_samples_2['theta'])\n\nVariable: theta\n  Shape: (1000, 8)\n  Mean: [-1.2455076   2.9482472  -5.281853   -0.06498987  2.3573008   0.31068215\n  3.729895   -1.5531777 ]\n  Variance: [11408.184   8016.4224 36956.      8181.7744  7636.8813  9586.139\n  6544.6753 26078.693 ]\n\n\n\nCondition on the observed data and do inference.\n\nes_conditioned_2 = condition(es_2, data={'obs': y})\nes_nuts_2 = NUTS(es_conditioned_2)\nes_mcmc_2 = MCMC(es_nuts_2, **mcmc_args)\n\nes_mcmc_2.run(rng_key, J, sigma)\nes_mcmc_2.print_summary()\n\n  0%|          | 0/6000 [00:00&lt;?, ?it/s]warmup:   0%|          | 1/6000 [00:01&lt;2:58:45,  1.79s/it]warmup:   1%|          | 40/6000 [00:01&lt;03:22, 29.37it/s] warmup:   2%|▏         | 90/6000 [00:01&lt;01:19, 74.28it/s]warmup:   2%|▏         | 135/6000 [00:02&lt;00:49, 119.09it/s]warmup:   3%|▎         | 174/6000 [00:02&lt;00:36, 157.82it/s]warmup:   4%|▍         | 229/6000 [00:02&lt;00:25, 224.14it/s]warmup:   5%|▍         | 276/6000 [00:02&lt;00:21, 272.23it/s]warmup:   5%|▌         | 324/6000 [00:02&lt;00:17, 316.78it/s]warmup:   6%|▋         | 386/6000 [00:02&lt;00:14, 388.23it/s]warmup:   8%|▊         | 450/6000 [00:02&lt;00:12, 450.84it/s]warmup:   8%|▊         | 505/6000 [00:02&lt;00:12, 444.11it/s]warmup:   9%|▉         | 561/6000 [00:02&lt;00:11, 473.22it/s]warmup:  10%|█         | 627/6000 [00:03&lt;00:10, 522.86it/s]warmup:  12%|█▏        | 703/6000 [00:03&lt;00:09, 588.10it/s]warmup:  13%|█▎        | 777/6000 [00:03&lt;00:08, 630.63it/s]warmup:  14%|█▍        | 850/6000 [00:03&lt;00:07, 658.48it/s]warmup:  15%|█▌        | 922/6000 [00:03&lt;00:07, 673.72it/s]warmup:  17%|█▋        | 991/6000 [00:03&lt;00:08, 602.15it/s]sample:  18%|█▊        | 1058/6000 [00:03&lt;00:08, 617.55it/s]sample:  19%|█▊        | 1122/6000 [00:03&lt;00:07, 622.47it/s]sample:  20%|█▉        | 1191/6000 [00:03&lt;00:07, 637.69it/s]sample:  21%|██        | 1258/6000 [00:03&lt;00:07, 645.94it/s]sample:  22%|██▏       | 1326/6000 [00:04&lt;00:07, 653.96it/s]sample:  23%|██▎       | 1394/6000 [00:04&lt;00:06, 661.26it/s]sample:  24%|██▍       | 1461/6000 [00:04&lt;00:06, 657.73it/s]sample:  26%|██▌       | 1530/6000 [00:04&lt;00:06, 666.22it/s]sample:  27%|██▋       | 1597/6000 [00:04&lt;00:06, 665.52it/s]sample:  28%|██▊       | 1670/6000 [00:04&lt;00:06, 682.62it/s]sample:  29%|██▉       | 1739/6000 [00:04&lt;00:06, 677.93it/s]sample:  30%|███       | 1807/6000 [00:04&lt;00:06, 662.44it/s]sample:  31%|███▏      | 1876/6000 [00:04&lt;00:06, 670.32it/s]sample:  32%|███▏      | 1946/6000 [00:05&lt;00:05, 678.70it/s]sample:  34%|███▎      | 2014/6000 [00:05&lt;00:05, 678.44it/s]sample:  35%|███▍      | 2084/6000 [00:05&lt;00:05, 683.93it/s]sample:  36%|███▌      | 2156/6000 [00:05&lt;00:05, 693.12it/s]sample:  37%|███▋      | 2231/6000 [00:05&lt;00:05, 707.72it/s]sample:  38%|███▊      | 2302/6000 [00:05&lt;00:05, 695.40it/s]sample:  40%|███▉      | 2372/6000 [00:05&lt;00:05, 691.40it/s]sample:  41%|████      | 2442/6000 [00:05&lt;00:05, 689.02it/s]sample:  42%|████▏     | 2511/6000 [00:05&lt;00:05, 688.74it/s]sample:  43%|████▎     | 2581/6000 [00:05&lt;00:04, 689.30it/s]sample:  44%|████▍     | 2650/6000 [00:06&lt;00:04, 689.04it/s]sample:  45%|████▌     | 2720/6000 [00:06&lt;00:04, 689.86it/s]sample:  46%|████▋     | 2789/6000 [00:06&lt;00:04, 687.98it/s]sample:  48%|████▊     | 2858/6000 [00:06&lt;00:04, 683.87it/s]sample:  49%|████▉     | 2927/6000 [00:06&lt;00:04, 678.14it/s]sample:  50%|████▉     | 2995/6000 [00:06&lt;00:04, 675.40it/s]sample:  51%|█████     | 3063/6000 [00:06&lt;00:04, 674.69it/s]sample:  52%|█████▏    | 3134/6000 [00:06&lt;00:04, 683.40it/s]sample:  53%|█████▎    | 3203/6000 [00:06&lt;00:04, 680.60it/s]sample:  55%|█████▍    | 3273/6000 [00:06&lt;00:03, 686.28it/s]sample:  56%|█████▌    | 3344/6000 [00:07&lt;00:03, 691.04it/s]sample:  57%|█████▋    | 3414/6000 [00:07&lt;00:03, 676.40it/s]sample:  58%|█████▊    | 3482/6000 [00:07&lt;00:03, 665.85it/s]sample:  59%|█████▉    | 3551/6000 [00:07&lt;00:03, 672.72it/s]sample:  60%|██████    | 3619/6000 [00:07&lt;00:03, 672.73it/s]sample:  61%|██████▏   | 3687/6000 [00:07&lt;00:03, 668.20it/s]sample:  63%|██████▎   | 3757/6000 [00:07&lt;00:03, 675.10it/s]sample:  64%|██████▍   | 3825/6000 [00:07&lt;00:03, 673.79it/s]sample:  65%|██████▍   | 3894/6000 [00:07&lt;00:03, 676.31it/s]sample:  66%|██████▌   | 3965/6000 [00:07&lt;00:02, 685.94it/s]sample:  67%|██████▋   | 4035/6000 [00:08&lt;00:02, 687.12it/s]sample:  68%|██████▊   | 4106/6000 [00:08&lt;00:02, 693.19it/s]sample:  70%|██████▉   | 4176/6000 [00:08&lt;00:02, 687.88it/s]sample:  71%|███████   | 4245/6000 [00:08&lt;00:02, 675.05it/s]sample:  72%|███████▏  | 4313/6000 [00:08&lt;00:02, 670.00it/s]sample:  73%|███████▎  | 4381/6000 [00:08&lt;00:02, 672.18it/s]sample:  74%|███████▍  | 4449/6000 [00:08&lt;00:02, 669.00it/s]sample:  75%|███████▌  | 4517/6000 [00:08&lt;00:02, 666.91it/s]sample:  76%|███████▋  | 4584/6000 [00:08&lt;00:02, 661.05it/s]sample:  78%|███████▊  | 4654/6000 [00:08&lt;00:02, 670.89it/s]sample:  79%|███████▉  | 4728/6000 [00:09&lt;00:01, 689.69it/s]sample:  80%|████████  | 4801/6000 [00:09&lt;00:01, 701.25it/s]sample:  81%|████████  | 4872/6000 [00:09&lt;00:01, 686.39it/s]sample:  82%|████████▏ | 4942/6000 [00:09&lt;00:01, 690.31it/s]sample:  84%|████████▎ | 5012/6000 [00:09&lt;00:01, 680.57it/s]sample:  85%|████████▍ | 5081/6000 [00:09&lt;00:01, 676.28it/s]sample:  86%|████████▌ | 5152/6000 [00:09&lt;00:01, 685.22it/s]sample:  87%|████████▋ | 5224/6000 [00:09&lt;00:01, 693.68it/s]sample:  88%|████████▊ | 5294/6000 [00:09&lt;00:01, 682.65it/s]sample:  89%|████████▉ | 5363/6000 [00:10&lt;00:00, 684.07it/s]sample:  91%|█████████ | 5432/6000 [00:10&lt;00:00, 682.21it/s]sample:  92%|█████████▏| 5501/6000 [00:10&lt;00:00, 680.31it/s]sample:  93%|█████████▎| 5570/6000 [00:10&lt;00:00, 679.17it/s]sample:  94%|█████████▍| 5638/6000 [00:10&lt;00:00, 666.87it/s]sample:  95%|█████████▌| 5706/6000 [00:10&lt;00:00, 669.51it/s]sample:  96%|█████████▋| 5778/6000 [00:10&lt;00:00, 683.00it/s]sample:  97%|█████████▋| 5848/6000 [00:10&lt;00:00, 687.24it/s]sample:  99%|█████████▊| 5917/6000 [00:10&lt;00:00, 669.91it/s]sample: 100%|█████████▉| 5986/6000 [00:10&lt;00:00, 672.94it/s]sample: 100%|██████████| 6000/6000 [00:10&lt;00:00, 547.77it/s]\n\n\n\n                   mean       std    median      5.0%     95.0%     n_eff     r_hat\n           mu      4.39      3.30      4.40     -1.00      9.83  35585.46      1.00\n          tau      3.62      3.23      2.77      0.00      7.78  28589.16      1.00\ntheta_base[0]      0.32      0.99      0.34     -1.27      1.97  40503.70      1.00\ntheta_base[1]      0.10      0.93      0.10     -1.41      1.65  43437.47      1.00\ntheta_base[2]     -0.08      0.97     -0.08     -1.71      1.47  41132.58      1.00\ntheta_base[3]      0.06      0.95      0.06     -1.55      1.58  45070.05      1.00\ntheta_base[4]     -0.17      0.93     -0.17     -1.69      1.36  40592.80      1.00\ntheta_base[5]     -0.07      0.94     -0.07     -1.61      1.48  44577.57      1.00\ntheta_base[6]      0.35      0.96      0.36     -1.16      2.01  38809.45      1.00\ntheta_base[7]      0.08      0.98      0.08     -1.58      1.63  45373.39      1.00\n\nNumber of divergences: 5\n\n\nThe results are consistent with the manual reparameterisation. It might seem uncessarily complicated to use the reparameterisation handler in this simple example, but in more complex models, especially those with many layers of dependencies, the reparameterisation handler can greatly facilitate the model building process."
  },
  {
    "objectID": "culture/2021-01-24-hanfei.html",
    "href": "culture/2021-01-24-hanfei.html",
    "title": "Han Fei",
    "section": "",
    "text": "Modifié le 2024-09-11.\nAvant Han Fei, l’école des légistes comptait déjà trois grandes factions, chacune ayant sa propre ligne de pensée.\nLa première faction, menée par Shendao, contemporain de Mencius, considérait le SHI (势), l’autorité et la puissance, comme le facteur le plus important en politique et dans la gouvernance. La deuxième faction, dirigée par Shen Buhai, estimait que le SHU (术), l’art de se servir des gens et de démêler les affaires, était primordial. Quant à la troisième faction, conduite par Shang Yang, elle accordait la plus grande importance au FA (法), la législation.\nHan Fei, dernier et plus grand théoricien de cette école, a combiné les idées de ces trois factions en une seule doctrine.\nSelon lui, un souverain doit être comme le ciel lorsqu’il rédige et applique la loi : juste et désintéressé. Face aux gens, il doit être comme le diable, les manipulant sans qu’ils en aient conscience. Comme le ciel, pour ne jamais avoir tort ; comme le diable, pour ne jamais être mis dans l’embarras. De plus, il faut qu’il soit puissant pour avoir l’autorité nécessaire à l’application de sa volonté et de ses ordres."
  }
]