[{"content":"I took the Solve It with Code course from answer.ai, and Advent of Code is used as practice.\nI always thought my Python skills were quite mediocre; I always used to in a \u0026ldquo;just get things done\u0026rdquo; style, so this is also a chance to improve my Python. For this reason I want to mostly stick with the standard packages and pythonic styles, and only use extra packages when it\u0026rsquo;s too cumbersome otherwise.\nThe way AOC works, we have one new puzzle each day, which has two parts: part 1 is often simpler, and can be tackled with brute force; but part 2 would need some more structured thinking, and better algorithmic design.\nAccompanying each day\u0026rsquo;s input there is also a smaller example input, which is worked out to explain the problem, understand the data and show the expected solution patterns. It\u0026rsquo;s very helpful as a stepstone to solving the real problem. But it can also be a double edged sword, because there might be patterns or complexities that are present in the real data but not in the example data, so we might be led astray if we rely completely on the example data to construct the solutions.\nhouse keeping The SolveIt method has 4 steps:\nunderstand the problem make a plan carry out the plan review, reflect, refactor The key is to think clearly, go slowly, stop and reflect constantly.\nWe use the aocd package to interact with Advent of Code programmatically.\n1 2 3 import os session = os.getenv(\u0026#39;AOC_SESSION\u0026#39;) from aocd import get_data, submit, models 1 current_day is only available in December (EST) 01 rotating dials at the entrance We are at the entrance, and there we have a circular dial; its range is 0 to 99; we can turn left (L) or right (R); keep turning right after 99 we arrive at 0, and keep turning left after 0 we arrive at 99; we start at 50, and the input is a series of records for turning left or right; for part 1 we count the total number of times we arrive at 0, after turning the dial; for part 2 we count the total number of times we pass across 0, after turning the dial. 1 2 3 4 5 6 7 8 def parse_2501(data: str) -\u0026gt; list[int]: \u0026#34;\u0026#34;\u0026#34;Parse rotations: L becomes negative, R becomes positive.\u0026#34;\u0026#34;\u0026#34; return [-int(line[1:]) if line[0] == \u0026#39;L\u0026#39; else int(line[1:]) for line in data.split(\u0026#39;\\n\u0026#39;)] D01 = models.Puzzle(year=2025, day=1) rotations = parse_2501(D01.input_data) print(len(rotations), rotations[:5]) 1 4042 [29, -3, -46, -25, -38] One of my goals for AOC is to use, as much as possible, only the standard libraries. I felt that there are a lot I can learn in the standard libraries, and my experience with AOC has proven that I was quite right. For part 1, Python\u0026rsquo;s modulo operator elegantly handles the circular arithmetic—even for negative numbers. (50 - 68) % 100 gives 82, and (-5) % 100 gives 95. This means we don\u0026rsquo;t need to manually handle wrap-around; just track cumulative position modulo 100 and count zeros.\nSince the landing position can be calculated by accumulatively adding the new turns to the starting position, we can use itertools.accumulate. The function apply a binary function (default to sum) accumulatively to a function, then return a generator of the sequence of results.\n1 2 from itertools import accumulate print(list(accumulate([1, 2, 3, 4], initial=0))) 1 [0, 1, 3, 6, 10] After the accumulative sum we do modulo to see if it lands on zeros.\n1 2 3 4 5 6 7 8 9 10 11 def solve_250101(data: str) -\u0026gt; int: \u0026#34;\u0026#34;\u0026#34;Part 1: Count positions landing exactly on 0. Uses accumulate with initial=50, then applies %100 to get dial positions. O(n) time, O(n) space. \u0026#34;\u0026#34;\u0026#34; rotations = parse_2501(data) positions = [p % 100 for p in accumulate(rotations, initial=50)][1:] return sum(1 for p in positions if p == 0) print(solve_250101(D01.input_data)) 1 969 For part 2, we need to count every time the dial crosses 0 during a rotation, not just when it lands there. My initial idea was to add a large offset (1e6) to convert circular to linear arithmetic, then count \u0026ldquo;hundred boundaries\u0026rdquo; crossed. However this turned out to be unnecessary. However we have to be careful to avoid double counting landing positions for a previous turn and the starting position for the next.\nAfter quite some trial and error it turns out, for a move from s to e, the number of zeros crossed depends only on direction:\nGoing right (positive turn): e//100 - s//100 Going left (negative turn): (s-1)//100 - (e-1)//100 The -1 adjustment for leftward moves ensures we count correctly at boundaries: moving left from 10000 to 9999 shouldn\u0026rsquo;t count as crossing (we started at the boundary), but moving right from 9999 to 10000 should. The logic seems simple in retrospect, but I got stuck here for quite a while!\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 def solve_250102(data: str) -\u0026gt; int: \u0026#34;\u0026#34;\u0026#34;Part 2: Count all zero-crossings during rotations. Adds offset to linearize circular dial, then counts hundred-boundaries crossed using integer division. Direction matters for boundary handling. O(n) time, O(n) space. \u0026#34;\u0026#34;\u0026#34; rotations = parse_2501(data) ends = list(accumulate(rotations, initial=50)) total = 0 s = ends[0] for t, e in zip(rotations, ends[1:]): if t \u0026gt; 0: # right turn, count as usual total += e // 100 - s // 100 else: # left turn, -1 to avoid double counting total += (s - 1) // 100 - (e - 1) // 100 s = e return total print(solve_250102(D01.input_data)) 1 5887 02 invalid product IDs in the gift shop We are in the gift shop, and we are given a list of product ID ranges; some IDs in those ranges are considered invalid; for part 1 it\u0026rsquo;s IDs with double repetitions, e.g. 55, 6464, 321321, etc. for part 2 it\u0026rsquo;s IDs with any number of repetitions, e.g. 55, 555, 64646464, etc. we need to find and sum up all the invalid IDS. When I started doing AOC, data parsing was a huge effort, because I had to sharpen my list comprehension and looping skills. But this got easier as I worked through more puzzles. Come to think about it, perhaps the only thing that computers can do better than humans, are fast iterations? As such it should be a central focus in computer programming. As Alan Perlis would say, \u0026ldquo;a program without a loop and a structured variable isn\u0026rsquo;t worth writing.\u0026rdquo; (Still working on the later!)\n1 2 3 4 5 6 7 8 9 10 11 import numpy as np def parse_2502(data: str) -\u0026gt; list[tuple[str, str]]: \u0026#34;\u0026#34;\u0026#34;Parse input into list of (start, end) string tuples. Keep as strings to preserve digit length information.\u0026#34;\u0026#34;\u0026#34; return [tuple(rng.split(\u0026#39;-\u0026#39;)) for rng in data.split(\u0026#39;,\u0026#39;)] D02 = models.Puzzle(year=2025, day=2) data = D02.input_data ranges = parse_2502(data) print(len(ranges),sum(int(e) - int(s) + 1 for s, e in ranges)) 1 38 2178105 For part 1, since we are filtering for double repetitions, only even-length IDs can be invalid, so I filtered the ranges to even-length ranges, and then used integer division/modulo to compare halves. The filtering was made easy by the fact that the maximum length difference between start and end is only 1. This upfront filtering is perhaps unnecessary; I did it mainly to reduce the cognitive load for solving the problem.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 def solve_250201(data: str) -\u0026gt; int: \u0026#34;\u0026#34;\u0026#34;Part 1: Sum all double-repetition IDs in ranges. Approach: expand ranges to arrays, use vectorized arithmetic to check if first_half == second_half via division and modulo. O(n) where n = total numbers across all ranges. \u0026#34;\u0026#34;\u0026#34; ranges = parse_2502(data) def filter_range(start: str, end: str) -\u0026gt; tuple[str, str] | None: \u0026#34;\u0026#34;\u0026#34;Filter to even-length numbers only.\u0026#34;\u0026#34;\u0026#34; len_s, len_e = len(start), len(end) s_even, e_even = len_s % 2 == 0, len_e % 2 == 0 if s_even and e_even: return (start, end) elif s_even and not e_even: return (start, \u0026#39;9\u0026#39; * len_s) elif not s_even and e_even: return (\u0026#39;1\u0026#39; + \u0026#39;0\u0026#39; * (len_e - 1), end) else: return None filtered = [r for r in map(lambda x: filter_range(*x), ranges) if r] total = 0 for start, end in filtered: a = np.arange(int(start), int(end) + 1) divider = 10 ** (len(start) // 2) mask = a // divider == a % divider total += a[mask].sum() return total print(solve_250201(D02.input_data)) 1 30608905813 For part 2 any length can be invalid as long as the digits are a repetition of a shorter base (at least two repeats), so we cannot filter out any number and have to check them all. The approach I settled on, is to build up all possible repetition patterns for all number lengths, and then check whether each pattern is found in each range. That\u0026rsquo;s a huge amount of loops!\nget all the lengths for all ranges. for all lengths, get all their factors. We need to exclude the lengths themselves, since repeating once doesn\u0026rsquo;t count. for all factor lengths, build up all repetition bases. for all length bases, build up all repetitions that match the target length. since all above work only involves the lengths of the numbers, not the ranges themselves, they can be done upfront. check whether the repets are in any of our ranges. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 from sympy import divisors def solve_250202(data: str) -\u0026gt; int: \u0026#34;\u0026#34;\u0026#34;Part 2: Sum all repeated-pattern IDs (≥2 reps) in ranges. Approach: generate all possible repeated-pattern numbers, then filter to those within any range. Much faster than checking every number when candidates are sparse. O(c * r) where c = candidates, r = ranges. \u0026#34;\u0026#34;\u0026#34; ranges = parse_2502(data) # Collect all digit lengths and their proper divisors all_lens = set(len(x) for rng in ranges for x in rng) all_divs = set() for l in all_lens: all_divs.update(divisors(l)[:-1]) # exclude self # Build bases for each divisor length bases = {} for f in all_divs: if f == 1: bases[f] = list(range(1, 10)) else: bases[f] = list(range(10**(f-1), 10**f)) # Generate all candidate repeated-pattern numbers candidates = set() for div in all_divs: for base in bases[div]: for target_len in all_lens: if target_len % div == 0 and target_len \u0026gt; div: candidates.add(int(str(base) * (target_len // div))) # Sum candidates that fall in any range total = 0 for start, end in ranges: s, e = int(start), int(end) for c in candidates: if s \u0026lt;= c \u0026lt;= e: total += c return total print(solve_250202(D02.input_data)) 1 31898925685 I think my solution is quite complicated. I\u0026rsquo;m not versed in regular expressions, this is one of my weak points. In retrospect, since we are literally just finding patterns in a string, regex seems to be the natural choice. I plan to come back for a regex solution later.\n03 maximum joltages of the lobby escalator we need to get the escalator working by setting it to a maximum power level; we have a list of battery \u0026ldquo;banks\u0026rdquo;, each bank in turn is another long list of labelled batteries, we can only choose a certain number of them; the power that each bank supplies is given by concatenating the chosen numbers, e.g. choosing 5 and 6 from 45623 gives us 56; for part 1 we choose 2 of them, for part 2 we choose 12. First parse the data into lists of integers.\n1 2 3 4 5 6 def parse_2503(data: str) -\u0026gt; list[list[int]]: return [[int(c) for c in l] for l in data.splitlines()] D03 = models.Puzzle(year=2025, day=3) banks = parse_2503(D03.input_data) print(len(banks), len(banks[0]), len(banks[-1])) 1 200 100 100 Part 1 looks quite simple: pick the best possible tens digit while leaving one digit for units, then after choosing the tens, choose the largest units digit to its right. It\u0026rsquo;s all about indexing.\n1 2 3 4 5 6 7 8 9 10 11 12 def solve_250301(data: str) -\u0026gt; int: \u0026#34;\u0026#34;\u0026#34;Part 1: sum of maximum 2-digit joltage per bank.\u0026#34;\u0026#34;\u0026#34; banks = parse_2503(data) total = 0 for line in banks: tens = max(line[:-1]) tens_idx = line.index(tens) units = max(line[tens_idx+1:]) total += int(str(tens) + str(units)) return total print(solve_250301(D03.input_data)) 1 17155 line.index(val) only finds the first index for val, which happens to be exactly what we want.\nPart 2 is the same logic, but with 12 digits, we have to be more careful with the indexing. This is a typical case where the data being processed is changing as the loop progresses, so the most important issue is to keep the target data correctly updated. In this case, since the updated allowed data is a slice of the original list, we only have to keep updating the start and end of the indexing. The key is to always leaving enough room the the rest of the digits.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 def solve_250302(data: str) -\u0026gt; int: \u0026#34;\u0026#34;\u0026#34;Part 2: sum of maximum 12-digit joltage per bank.\u0026#34;\u0026#34;\u0026#34; banks = parse_2503(data) total = 0 for line in banks: # loop through each bank start, end = 0, -12+1 # allowed indexing range picks = [] for _ in range(12): # loop through the digits if end == 0: # the last digit requires separate slicing syntax l = line[start:] m = max(l) else: # all digits except the last l = line[start:end] m = max(l) idx = l.index(m) start += idx + 1 end += 1 picks.append(m) total += int(\u0026#39;\u0026#39;.join(map(str, picks))) return total print(solve_250302(D03.input_data)) 1 169685670469164 All in all this one is fairly simple, but correctly updating the indices can be tricky if we kept it all in head. I found my current indexing approach rather ugly, especially the -12+1 part, and the fact that I have to use a different indexing syntax for the last digit. Finding each digit is actually indexing a slice of the original list, so we have to accumulate the indices correctly. I had to draw some schematics on paper to get the indexing logic right. I intend to find a better indexing logic once I got the time.\n04 accessible paper rolls in the print shop we have paper rolls in the print shop, arranged on a 2D grid; rolls are denoted by @, empty spaces by .; each roll has up to 8 neighbors (the 3x3 grid around it); a roll is accessible if it has fewer than 4 neighboring rolls; in part 1 we count all accessible rolls in the initial grid; in part 2 we repeatedly remove all accessible rolls, and count how many are removed in total. Parsing the data into a 2D grid.\n1 2 3 4 5 6 7 def parse_2504(data: str) -\u0026gt; list[list[int]]: \u0026#34;\u0026#34;\u0026#34;Parse grid: @ -\u0026gt; 1, . -\u0026gt; 0.\u0026#34;\u0026#34;\u0026#34; return [[1 if c == \u0026#39;@\u0026#39; else 0 for c in line] for line in data.splitlines()] D04 = models.Puzzle(year=2025, day=4) grid = parse_2504(D04.input_data) print(len(grid), len(grid[0])) 1 135 135 Initially I thought about using a tuple of indices for the data, but apparently using a 2D Numpy array would be much simpler. The key is the 3x3 kernel with a zero in the center.\nThis problem is quite similar to the convolution operation in computer vision, so I turned to scipy.ndimage.convolve. But actually the convolution used in computer vision packages, such as PyTorch, is not convolution, but correlation, since mathematically convolution involves flipping the input data first. So the right operation is actually scipy.ndimage.correlate, and padding all borders with 0 is natively supported with the constant mode, and the constant value being set to 0.\nAnd then finding the targets satisfying the creterion is just filtering the results for paper rolls with less than 4 neighbors.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 import numpy as np from scipy.ndimage import correlate def solve_250401(data: str) -\u0026gt; int: \u0026#34;\u0026#34;\u0026#34;Part 1: count rolls with \u0026lt;4 neighbors. Uses 8-neighbor correlation with a 3x3 kernel. O(H*W) time, O(H*W) space. \u0026#34;\u0026#34;\u0026#34; grid = parse_2504(data) arr = np.array(grid, dtype=int) kernel = np.array([[1, 1, 1], [1, 0, 1], [1, 1, 1]], dtype=int) neighbor_counts = correlate(arr, kernel, mode=\u0026#39;constant\u0026#39;, cval=0) return int(((arr == 1) \u0026amp; (neighbor_counts \u0026lt; 4)).sum()) print(solve_250401(D04.input_data)) 1 1437 Part 2 is the same neighbor test, but applied repeatedly. Each pass removes the rolls that are currently accessible; we keep looping until no more rolls qualify.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 def solve_250402(data: str) -\u0026gt; int: \u0026#34;\u0026#34;\u0026#34;Part 2: iteratively remove accessible rolls and count total removed. Each iteration is O(H*W); the number of iterations depends on structure. \u0026#34;\u0026#34;\u0026#34; grid = parse_2504(data) arr = np.array(grid, dtype=int) kernel = np.array([[1, 1, 1], [1, 0, 1], [1, 1, 1]], dtype=int) total = 0 while True: neighbor_counts = correlate(arr, kernel, mode=\u0026#39;constant\u0026#39;, cval=0) mask = (arr == 1) \u0026amp; (neighbor_counts \u0026lt; 4) count = int(mask.sum()) if count == 0: break total += count arr[mask] = 0 return int(total) print(solve_250402(D04.input_data)) 1 8765 I intend to implement the indexing solution later, without NumPy. In retrospect the problem is quite easy, once we connected it to image processing in computer vision. But this doesn\u0026rsquo;t come up right away; I only thought of it after spending quite some effort considering how to pad the grid. This shows that I still need practice on some of the basic algorithms.\n05 fresh items in the cafeteria we are in the cafeteria, which keeps a LARGE inventory (large enough to feed a nation!) we have two lists, the first is ranges of possible fresh items, possibly overlapping; the second is the individual items that are currently actually in inventory, some fresh, some probably rotten; for part 1 we need to find all the fresh items in inventory, and count them; for part 2 we need to count how many items are actually in the overlapping fresh item ranges. My original plan was quite simplistic, just turn the lists of ranges into one big set with all the elements, which perfectly solves the partially overlapping problem; then part 1 is simply checking set membership, while part 2 is checking the size of the set. But set is atomic, which means that every element in it has to be independently initiated, and this requires a huge amount of memory: I tried and exhausted the memory before finishing the set initialization.\nThe cleanest practical solution should be deduplicating and merging the overlapping ranges, after ordering them one after another, and then for part 1 we can find the range corresponding to a certain item and check its membership, for part 2 just sum up the range lengths.\n1 2 3 4 5 6 7 8 9 10 def parse_2505(data: str) -\u0026gt; tuple[list[tuple[int, int]], list[int]]: \u0026#34;\u0026#34;\u0026#34;Parse input into (ranges, ids). Ranges are inclusive (lo, hi) tuples.\u0026#34;\u0026#34;\u0026#34; ranges, ids = [part.split(\u0026#39;\\n\u0026#39;) for part in data.split(\u0026#39;\\n\\n\u0026#39;)] ranges = [tuple(map(int, rng.split(\u0026#39;-\u0026#39;))) for rng in ranges] ids = list(map(int, ids)) return ranges, ids D05 = models.Puzzle(year=2025, day=5) ranges, ids = parse_2505(D05.input_data) print(len(ranges), len(ids)) 1 189 1000 However for part 1, since we only have 1000 items, and 189 ranges to check, we might just check them one by one using brute force. Nonetheless, note that an item is fresh if it\u0026rsquo;s in any range, so we could stop checking for that item once an enclosing range is found. And any() with a generator expression is quite neat for readable short-circuit membership testing.\n1 2 3 4 5 6 7 8 9 def solve_250501(data: str) -\u0026gt; int: \u0026#34;\u0026#34;\u0026#34;Part 1: Count IDs that fall within any range. O(n*m) where n=IDs, m=ranges. \u0026#34;\u0026#34;\u0026#34; ranges, ids = parse_2505(data) return sum(any(lo \u0026lt;= inv \u0026lt;= hi for lo, hi in ranges) for inv in ids) print(solve_250501(D05.input_data)) 1 840 Once data parsed to the correct format, the solution is just a one-liner. Using generators avoids keeping all the items in memory, and using any we can early stop the membership check once a match is found.\nFor part 2, saying that we need to deduplicate the ranges is neither clear on what the desired result should be like, nor how we could get there. We first need to sort them by starting value, which can be easily dealt with with sorted. Then starting with the first range, we can merge the remaining ranges to it, one by one. The merged result can be a single range, e.g. merging (1, 3) to (1,2) gives simply (1, 3), or a number of disjoint ranges, e.g. merging (4,6) to (1,3) gives [(1,3), (4,6)]. For this reason the starting object must be a list with only one element, not the element itself, i.e. [(1,2),] not (1,2).\nMy initial impression of reduce is largely limited to summary statistics, min, max, argmin, etc. But reduce is actually a general procedure that applies a function of two arguments cumulatively to the items of a sequence or iterable, from left to right, so as to reduce the iterable to a single value. So it\u0026rsquo;s a perfect fit for what we are trying to achieve.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 from functools import reduce def solve_250502(data: str) -\u0026gt; int: \u0026#34;\u0026#34;\u0026#34;Part 2: Count total unique IDs covered by union of all ranges. Classic interval merging algorithm: 1. Sort ranges by start 2. Merge overlapping/adjacent ranges via reduce 3. Sum lengths (+1 for inclusive bounds) \u0026#34;\u0026#34;\u0026#34; ranges, _ = parse_2505(data) sorted_ranges = sorted(ranges) def merge_step(acc, rng): last_start, last_end = acc[-1] next_start, next_end = rng if last_end \u0026lt; next_start: # no overlap acc.append(rng) else: # overlap: extend current range acc[-1] = (last_start, max(last_end, next_end)) return acc merged = reduce(merge_step, sorted_ranges[1:], sorted_ranges[:1]) return sum(hi - lo + 1 for lo, hi in merged) print(solve_250502(D05.input_data)) 1 359913027576322 The use of reduce in this puzzle is inspirational to me. More and more I\u0026rsquo;m convinced that programming is simply about iteration, once we have the object to be iterated over, and the target to be produced correctly configured (that is, the data structure properly laied out.) All other procedural programs are boilerplates. So again, \u0026ldquo;a program without a loop and a structured variable is not worth writing\u0026rdquo;.\n06 cephalopod math in the trash compactor we are given a set of arithmetic problems laid out in a wide grid; the last row has the operators, the rows above are digits for operands; problems are separated by full columns of spaces; part 1 reads numbers by rows (split on whitespace), part 2 reads numbers by columns (trimming the leading or ending whitespaces); once parsed, each problem is just a sum or a product, then we add all results. This is the first time where the difficulty of solving the puzzle lays in parsing the data, rather than solving the problems. There are only 2 operators, multiplication and addition, which should not pose any problem in any modern programming language. In fact, after parsing the data correctly the solution for both parts should be identical.\nFor part 1 the key problem is to split the numbers into groups of problems. This seemingly simple problem is complicated by the fact that all data processing functions I know of (in Python) all process data row by row, while the numbers for each problem are arranged by columns. However this is made easy by a combination of zip(*numbers), which is effectively the transpose function in standard Python. The operator row gives the operator for each problem.\n1 2 3 4 5 6 7 8 9 10 11 def parse_250601(data: str) -\u0026gt; tuple[list[tuple[int, ...]], list[str]]: \u0026#34;\u0026#34;\u0026#34;Part 1 parse: split each row on whitespace, then zip into problems.\u0026#34;\u0026#34;\u0026#34; *number_rows, operator_row = data.splitlines() numbers = [list(map(int, row.split())) for row in number_rows] problems = list(zip(*numbers)) operators = operator_row.split() return problems, operators D06 = models.Puzzle(year=2025, day=6) problems, operators = parse_250601(D06.examples[0].input_data) print(len(problems), problems[0], operators[0]) 1 4 (123, 45, 6) * After parsing the data all that\u0026rsquo;s left is to compute each problem using its operator and sum everything. Since this exact logic can be used in both parts, I\u0026rsquo;ve refactored the solver function so that in addition to the data, it also accepts a parser argument.\n1 2 3 4 5 6 7 8 9 10 11 from math import prod def solve_2506(data: str, parser) -\u0026gt; int: \u0026#34;\u0026#34;\u0026#34;Shared solver: apply the operator to each problem, then sum.\u0026#34;\u0026#34;\u0026#34; problems, operators = parser(data) return sum( prod(nums) if op == \u0026#39;*\u0026#39; else sum(nums) for nums, op in zip(problems, operators) ) print(solve_2506(D06.input_data, parse_250601)) 1 6957525317641 Actually we can understand this solution as another parser: parsing the * and + symbols into Python prod and sum functions. But this is rather simple. I plan to study some similar use cases but with more complexity later (Norvig\u0026rsquo;s lisp interpreters).\nThe parsing logic for part 2 is a bit more convoluted. The problems in part 1 are stored in different columns, and we have to process them into rows (thus the need for transpose), because that\u0026rsquo;s what our computer is accustomed to process. In part 2, apart from the problems being separated in columns, the operands for each problem are also in columns, so this creates an extra layer of complexity. We can see that most of the difficulty in this puzzle arises because of the fact that we are only accustomed to processing information in a certain fashion, i.e. one row after another. It\u0026rsquo;s curious that we are so used to processing information row by row, from left to right, and almost all our computer tools are built around this procedure. It\u0026rsquo;s quite possible that some other civilizations might invent some other procedures that work differently. Classic Chinese, for example, reads column by column, and from right to left.\nBack to the problem, the key is to transpose the character grid, then group contiguous non-blank columns into problems.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 from itertools import groupby def parse_250602(data: str) -\u0026gt; tuple[list[tuple[int, ...]], list[str]]: \u0026#34;\u0026#34;\u0026#34;Part 2 parse: read numbers by columns (character-wise).\u0026#34;\u0026#34;\u0026#34; *number_rows, operator_row = data.splitlines() columns = [\u0026#39;\u0026#39;.join(chars) for chars in zip(*number_rows)] groups = [ list(group) for is_digit, group in groupby(columns, key=lambda col: col.strip() != \u0026#39;\u0026#39;) if is_digit ] problems = [tuple(int(col.strip()) for col in group) for group in groups] operators = operator_row.split() return problems, operators problems, operators = parse_250602(D06.examples[0].input_data) print(len(problems), problems[0], operators[0]) 1 4 (1, 24, 356) * The main novelty here is itertools.groupby, which separates the individual columns into problem groups, by detecting columns that are all whitespaces.\n1 print(solve_2506(D06.input_data, parse_250602)) 1 13215665360076 zip(*list) and itertools.groupby are the new weapons in the arsenal!\n07 splitting beams in the lab Tachyon manifolds we are in a Tachyon manifold, which is simply a 2D grid field; there is (only) one beam of light passing through the field; the beam passes straight through if there is no hindrance, and splits to left and right if encountering a splitter; part 1 needs to count how many splitters have been hit on; part 2 needs to count how many individual paths the beam passe through. Since we need to get the indices of the beams and the splitters, we can parse the data using list comprehension with enumeration. This keep the indices whose values meet a certain creterion. And we can discard the splitter rows that don\u0026rsquo;t actually have splitters.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 def parse_2507(data: str) -\u0026gt; tuple[set[int], list[set[int]]]: \u0026#34;\u0026#34;\u0026#34;Parse manifold into start column and splitter rows.\u0026#34;\u0026#34;\u0026#34; lines = data.splitlines() start = {i for i, ch in enumerate(lines[0]) if ch == \u0026#39;S\u0026#39;} splitter_rows = [ {i for i, ch in enumerate(line) if ch == \u0026#39;^\u0026#39;} for line in lines[1:] ] splitter_rows = [s for s in splitter_rows if s] return start, splitter_rows D07 = models.Puzzle(year=2025, day=7) lines = D07.input_data.splitlines() start, splitter_rows = parse_2507(D07.input_data) print(len(lines), len(lines[0])) print(len(splitter_rows), sum(line.count(\u0026#39;^\u0026#39;) for line in lines)) 1 2 142 141 70 1650 For part 1, a splitter is activated only if:\nthere is a splitter in the first place; a beam is present in the previous row. So this check can be done by comparing the existence of beams in one row, with the existence of splitters in the next row, and this can be done elegantly with set operations. Concretely, A-B are kept as is, and B-A updated with shifts, then we take the union of the two to pass to the next step.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 def step_beams(beams: set[int], splitters: set[int]) -\u0026gt; tuple[set[int], int]: \u0026#34;\u0026#34;\u0026#34;Advance beams through one splitter row and count activations.\u0026#34;\u0026#34;\u0026#34; hits = beams \u0026amp; splitters misses = beams - splitters spawned = {col + d for col in hits for d in (-1, 1)} return misses | spawned, len(hits) def solve_250701(data: str) -\u0026gt; int: \u0026#34;\u0026#34;\u0026#34;Part 1: count activated splitters. O(sum(len(beams) + len(splitters))) across rows. \u0026#34;\u0026#34;\u0026#34; beams, splitter_rows = parse_2507(data) total = 0 for splitters in splitter_rows: beams, activations = step_beams(beams, splitters) total += activations return total print(solve_250701(D07.input_data)) 1 1524 I haven\u0026rsquo;t used the set operations before, so this has been a useful exercise.\nFor part 2, we need to count the number of distinct paths (timelines) the beam took. My original solution was to store the complete path trajectories, but it turned out to be too memory intensive. Then it occurred to me that we only need to count the number of paths, not trace all of them. This is a big difference, since counting them only requires some summary statistics of the paths, not the paths themselves. In fact, at each step in the manifold, we can count the total spawned paths by counting how many end points there are, and how many distinct paths pass through each ending point. And this info can easily be stored in a dict.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 def step_counts(paths: dict[int, int], splitters: set[int]) -\u0026gt; dict[int, int]: \u0026#34;\u0026#34;\u0026#34;Advance all timeline counts through one splitter row.\u0026#34;\u0026#34;\u0026#34; new_paths: dict[int, int] = {} for col, count in paths.items(): if col in splitters: for ncol in (col - 1, col + 1): new_paths[ncol] = new_paths.get(ncol, 0) + count else: new_paths[col] = new_paths.get(col, 0) + count return new_paths def solve_250702(data: str) -\u0026gt; int: \u0026#34;\u0026#34;\u0026#34;Part 2: count timelines after all splits. O(sum(len(paths) + len(splitters))) across rows. \u0026#34;\u0026#34;\u0026#34; beams, splitter_rows = parse_2507(data) paths = {col: 1 for col in beams} for splitters in splitter_rows: # print(paths) paths = step_counts(paths, splitters) return sum(paths.values()) print(solve_250702(D07.input_data)) 1 32982105837605 I had originally had some trouble updating the paths dict, because with splitting beams we have to asign values to keys that doesn\u0026rsquo;t yet exist. But of course such a common scenario is already covered by an existing dict method dict.get(key, default), which returns the corresponding value is the key exists, and returns a default value otherwise.\nIncidentally today\u0026rsquo;s puzzle has some terminologies that I don\u0026rsquo;t know, like Tachyon manifolds, and some connections that I\u0026rsquo;m not aware of, like quantum fields. Nevertheless this does not in anyway interferes with solving the puzzle. This is a common pattern in computer science, or more specifically algorithm design. A specific problem from a specific domain can be narrated in a specific domain language, but all those specificity can often be abstracted away when devising the algorithm and solving the problem. However this is not saying all the specificities do not matter: on the contrary, simply running a computer program to get an answer is useless, if we can\u0026rsquo;t apply it back to the specific problems.\n08 connecting junction boxes in the playground We are in a playground with junction boxes, each with a unique 3D coordinate. We repeatedly connect the closest pairs of boxes (by straight-line distance), creating connected groups. For part 1, we process the first 1000 closest pairs (even if some are already connected), then multiply the sizes of the three largest circuits. For part 2, we keep processing closest pairs until all boxes are in one group; then multiply the x coordinates of the last pair that merges the final two circuits. A subtle but important point that confused me in the beginning: the problem says \u0026ldquo;closest pairs\u0026rdquo;, not \u0026ldquo;successful merges.\u0026rdquo; In the example, after \u0026ldquo;ten shortest connections\u0026rdquo; only nine merges happen because one pair was already in the same circuit. So we must process the pairs in order, allowing no-ops.\nWe need to connect individual nodes into separate graphs. The key issues to solve include: 1, which nodes to connect; 2, how to connect; 3, what information to extract from the connected nodes. After figuring out which nodes to connect, which simply calculates the paired distances and rank them, the key problem is graph manipulation. The right abstraction here is Union-Find (disjoint set union). Union, on surface, is connecting two nodes, but in reality it\u0026rsquo;s connecting the roots of the two nodes, and for this reason we need to know which circuit each box belongs to, and the sizes of those circuits; the actual edges are irrelevant.\n1 2 3 4 5 6 7 def parse_2508(data: str) -\u0026gt; list[tuple[int, int, int]]: \u0026#34;\u0026#34;\u0026#34;Parse coordinates from \u0026#39;x,y,z\u0026#39; lines.\u0026#34;\u0026#34;\u0026#34; return [tuple(map(int, line.split(\u0026#39;,\u0026#39;))) for line in data.splitlines()] D08 = models.Puzzle(year=2025, day=8) points = parse_2508(D08.input_data) print(len(points), points[0]) 1 1000 (54996, 20819, 75067) Getting the sorted distance together with the corresponding indices is easy. The distances are used for sorting, while the indices are used later to guide which points to connect, in which order.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 from itertools import combinations def sorted_pairs_2508(points: list[tuple[int, int, int]]) -\u0026gt; list[tuple[int, int, int]]: \u0026#34;\u0026#34;\u0026#34;Return (dist2, i, j) for all pairs, sorted by dist2. dist2 is squared Euclidean distance (ordering is unchanged). \u0026#34;\u0026#34;\u0026#34; pairs = [] for i, j in combinations(range(len(points)), 2): x1, y1, z1 = points[i] x2, y2, z2 = points[j] d2 = (x1 - x2) ** 2 + (y1 - y2) ** 2 + (z1 - z2) ** 2 pairs.append((d2, i, j)) return sorted(pairs) The key is the union-find algorithm. Each node is initiated as an independent group, of size 1, with itself as its parent. find traces a graph to its root, it follows parents and compresses the path so future lookups are faster. union merges two roots, updates the size of the new root, and decrements components. We use group_sizes to read off the sizes of all current circuits.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 class UnionFind: \u0026#34;\u0026#34;\u0026#34;Disjoint set union with path compression. We keep it simple: always attach one root under the other. \u0026#34;\u0026#34;\u0026#34; def __init__(self, n: int): self.parent = list(range(n)) self.size = [1] * n self.components = n def find(self, x: int) -\u0026gt; int: if self.parent[x] != x: self.parent[x] = self.find(self.parent[x]) return self.parent[x] def union(self, x: int, y: int) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;Merge sets containing x and y. Return True if a merge happened.\u0026#34;\u0026#34;\u0026#34; rx, ry = self.find(x), self.find(y) if rx == ry: return False self.parent[ry] = rx # simply attanch the second group to the first self.size[rx] += self.size[ry] self.components -= 1 return True def group_sizes(self) -\u0026gt; list[int]: return [self.size[i] for i in range(len(self.parent)) if self.parent[i] == i] uf = UnionFind(5) uf.union(0, 1) uf.union(2, 3) print(sorted(uf.group_sizes(), reverse=True), uf.components) # [2, 2, 1], 3 uf.union(1, 3) # merges the {0,1} and {2,3} circuits print(sorted(uf.group_sizes(), reverse=True), uf.components) # [4, 1], 2 1 2 [2, 2, 1] 3 [4, 1] 2 For part 1, process the first 1000 closest pairs, then take the top three circuit sizes.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from math import prod def solve_250801(data: str) -\u0026gt; int: \u0026#34;\u0026#34;\u0026#34;Part 1: product of sizes of the three largest circuits. O(n^2 log n) time for sorting all pairs, O(n^2) space. \u0026#34;\u0026#34;\u0026#34; points = parse_2508(data) pairs = sorted_pairs_2508(points) uf = UnionFind(len(points)) for _, i, j in pairs[:1000]: uf.union(i, j) sizes = sorted(uf.group_sizes(), reverse=True) return prod(sizes[:3]) print(solve_250801(D08.input_data)) 1 112230 Part 2 only changes the stopping condition: keep processing closest pairs until there\u0026rsquo;s a single circuit, and return the product of the x-coordinates of the last successful merge.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 def solve_250802(data: str) -\u0026gt; int: \u0026#34;\u0026#34;\u0026#34;Part 2: product of x-coordinates from the last merging pair. O(n^2 log n) time for sorting all pairs, O(n^2) space. \u0026#34;\u0026#34;\u0026#34; points = parse_2508(data) pairs = sorted_pairs_2508(points) uf = UnionFind(len(points)) for _, i, j in pairs: if uf.union(i, j) and uf.components == 1: return points[i][0] * points[j][0] print(solve_250802(D08.input_data)) 1 2573952864 It is actually quite difficult to get the right level of abstraction. I wandered about for quite a while till it\u0026rsquo;s clear that the union-find algorithm should only operate on the indices; and all of the indice operations, with memory, should be included. However, everything else, including the distances and the ranked pair of indices, should NOT be part of this abstraction. This is evident from the fact that the UnionFind class is initiated with only the #nodes and nothing else. This might feel obvious in retrospect, but leaky abstractions caused me a lot of headache before I came up to the right abstraction.\nOnce Union-Find is in place, part 2 just change the stopping rule.\n09 rectangular tile patterns in the movie theatre We are now in the movie theatre, and we are to redecorate the floor, which is covered in tiles of various colors. There are red tiles; between consecutive red tiles there are green tiles; and the interior enclosed by the red/green loop is also green. For part 1, we choose two red tiles as opposite corners of a rectangle and want the maximum area. For part 2, we still want the maximum area, but with the extra requirement that the rectangle must lie entirely on red or green tiles. Parsing the data is trivial.\n1 2 3 4 5 6 7 def parse_2509(data: str) -\u0026gt; list[tuple[int, int]]: \u0026#34;\u0026#34;\u0026#34;Parse red tile coordinates as (x, y) pairs.\u0026#34;\u0026#34;\u0026#34; return [tuple(map(int, line.split(\u0026#39;,\u0026#39;))) for line in data.splitlines()] D09 = models.Puzzle(year=2025, day=9) red = parse_2509(D09.input_data) print(len(red), red[:3]) 1 496 [(98026, 50027), (98026, 51253), (98365, 51253)] Part 1 is simple. With less than 500 points, try every pair of red tiles as opposite corners is computationally quite feasible.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 from itertools import combinations def solve_250901(data: str) -\u0026gt; int: \u0026#34;\u0026#34;\u0026#34;Part 1: maximum rectangle area from two red corners. O(n^2) time, O(1) extra space. \u0026#34;\u0026#34;\u0026#34; red = parse_2509(data) return max( (abs(x2 - x1) + 1) * (abs(y2 - y1) + 1) for (x1, y1), (x2, y2) in combinations(red, 2) ) print(solve_250901(D09.input_data)) 1 4750297200 The only thing worth taking note of is how the area is calculated. The red tiles are squares with areas themselves, not just points (which have no area), and thus the +1 in the formula.\nPart 2 is the real problem of the day. We can still iterate over all pairs of opposite corners, but then we need to check all tiles of the resulting rectangle is in the red-green encircled area. This entails several problems:\nwhat this enclosed area is actually like. This is actually a problem for part 1, but with part 1 being so easy we didn\u0026rsquo;t even bother to deal with it. how to check if a point is in the enclosed area. This is the core problem. which points we have to check for such enclosion. The enclosed area turns out to be a rectilinear polygon, but I only figured it out after quite some data explorations, including the term rectilinear itself. And to check for enclosion there is a well established algorithm called ray casting, but I know nothing about it. Besides, we are now in a (100K, 100K) area, each rectangle thus has a vast area, so there are a HUGE amount of points to check (if we avoid being clever), the computation has increased immensely.\nI tried to implement the ray casting algorithm, but got buried in the implementation details and edge cases, and I got stuck on the problem for a whole week. Finally I reworked the puzzle after having finished day 10 and 11, and simply used an existing package, shapely, which has established and well polished methods for all 3 issues. So in the end I solved the problem by not solving it.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 from shapely.geometry import Polygon from shapely.prepared import prep def solve_250902(data: str) -\u0026gt; int: \u0026#34;\u0026#34;\u0026#34;Part 2: maximum rectangle area fully covered by red/green tiles. Uses shapely for containment checks (non-stdlib). O(n^2) rectangle checks; containment is fast with a prepared polygon. \u0026#34;\u0026#34;\u0026#34; red = parse_2509(data) poly = Polygon(red) prepared = prep(poly) best = 0 for (x1, y1), (x2, y2) in combinations(red, 2): xmin, xmax = (x1, x2) if x1 \u0026lt;= x2 else (x2, x1) ymin, ymax = (y1, y2) if y1 \u0026lt;= y2 else (y2, y1) rect = Polygon([(xmin, ymin), (xmin, ymax), (xmax, ymax), (xmax, ymin)]) if prepared.covers(rect): area = (xmax - xmin + 1) * (ymax - ymin + 1) if area \u0026gt; best: best = area return best print(solve_250902(D09.input_data)) 1 2 3 4 5 6 7 8 --------------------------------------------------------------------------- ModuleNotFoundError Traceback (most recent call last) Cell In[73], line 1 ----\u0026gt; 1 from shapely.geometry import Polygon 2 from shapely.prepared import prep 4 def solve_250902(data: str) -\u0026gt; int: ModuleNotFoundError: No module named \u0026#39;shapely\u0026#39; I still need to come back and figure out how to implement the algorithm properly. I tried to look into the shapely implementations, but they are buried in optimized code God knows where. This remains the biggest challange of AOC 2025 for me, and the only case that I considered a failure.\nThere is one solution, which I stumbled upon in the forums, that is a data-compression first approach. This starts by noticing that while the area is large (100K times 100K), we only have less than 500 points, which means that there must by many in-the-middle points that we don\u0026rsquo;t really have to check. The idea is to do a coordinate compression first, finding out which points in the 100K field actually matters (are the edges and corners), solve the problem on this compressed grid first, and then project back to the original grid to find the areas. The reduced grid being small enough, that we might be able to just brute-force through it. While I still don\u0026rsquo;t know how to solve it exactly, this is a brilliant approach to simplify the problem, and I intend to try it later.\n10 combining machine switches in the factory We are in the factory, which has a big amount of machines and yet none of them are working. Each machine has N indicator lights, N joltage levels, and M switches; The switches can manipulate both the lights and the joltages; Part 1: find the minimum number of button presses to reach the target on/off pattern; multiple presses of the same buton cancels out. Part 2: ignore the lights and use the same switches to increment counters to target joltage values; now presses are nonnegative integers. For this one, I\u0026rsquo;m quite proud of myself, for being able to carefully comb through the word soup and parse it to clear matrix multiplication operations and optimization problems. Both parts use the same data format, just different variables, so when refactoring I combined them together with a boolean switch. We have till now mostly parsed data with splitlines or split, but for more complex data patterns we need regular expressions.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 import re import numpy as np def parse_2510(data: str, use_joltage: bool = False) -\u0026gt; list[tuple[np.ndarray, np.ndarray]]: \u0026#34;\u0026#34;\u0026#34;Return list of (y, XT) where y is target, XT is N x M toggle matrix.\u0026#34;\u0026#34;\u0026#34; parsed = [] for line in data.splitlines(): if use_joltage: target = re.search(r\u0026#39;\\{([0-9,]+)\\}\u0026#39;, line).group(1) target = [int(x) for x in target.split(\u0026#39;,\u0026#39;)] else: diagram = re.search(r\u0026#39;\\[([.#]+)\\]\u0026#39;, line).group(1) target = [1 if c == \u0026#39;#\u0026#39; else 0 for c in diagram] switches = re.findall(r\u0026#39;\\(([0-9,]+)\\)\u0026#39;, line) indices_list = [[int(s) for s in sw.split(\u0026#39;,\u0026#39;)] for sw in switches] N = len(target) M = len(indices_list) X = np.zeros((M, N), dtype=int) for i, inds in enumerate(indices_list): X[i, inds] = 1 y = np.array(target, dtype=int).reshape(-1, 1) parsed.append((y, X.T)) return parsed D10 = models.Puzzle(year=2025, day=10) ex = D10.examples[0].input_data data = D10.input_data For part 1 we resorted to brute force exhaustive search, but with early stopping. This is feasible, firstly because the number of switches for each machine is limited. But more importantly because pressing the same switches twice cancels out, we should either press a button, or don\u0026rsquo;t; the search space for each switch is thus just {0, 1}, so we have a closed exploration space.\nSince we aim to find the minimum number of presses, we search by the number of presses, starting from 1, and early stop once a matching combination is found.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 from itertools import combinations from math import comb def construct_ws(M: int, m: int) -\u0026gt; np.ndarray: \u0026#34;\u0026#34;\u0026#34;Create [M, C(M,m)] matrix of switch vectors with exactly m presses.\u0026#34;\u0026#34;\u0026#34; K = comb(M, m) ws = np.zeros((M, K), dtype=int) for i, combo in enumerate(combinations(range(M), m)): ws[list(combo), i] = 1 return ws def solve_251001(data: str) -\u0026gt; int: \u0026#34;\u0026#34;\u0026#34;Part 1: minimum presses to match lights (mod 2). Uses NumPy for matrix multiplication (non-stdlib). \u0026#34;\u0026#34;\u0026#34; parsed = parse_2510(data, use_joltage=False) total = 0 for y, XT in parsed: _, M = XT.shape for m in range(0, M + 1): ws = construct_ws(M, m) ys = (XT @ ws) % 2 if np.any(np.all(ys == y, axis=0)): total += m break return total print(solve_251001(data)) 1 542 The same method won\u0026rsquo;t work on part 2. Now the targets are positive integers, and each press increases the counter, so the search space is now infinite, which renders exhaustive search pointless. However by formulating the problem as minimizing sum(w) subject to XT @ w = y and w \u0026gt;= 0, it becomes an standard integer linear programming problem, and I simply used SciPy\u0026rsquo;s linprog with integrality=1.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from scipy.optimize import linprog def solve_251002(data: str) -\u0026gt; int: \u0026#34;\u0026#34;\u0026#34;Part 2: minimum presses to match joltage counters (ILP). Uses SciPy linprog with integrality=1. \u0026#34;\u0026#34;\u0026#34; parsed = parse_2510(data, use_joltage=True) total = 0 for y, XT in parsed: M = XT.shape[1] res = linprog(c=[1] * M, A_eq=XT, b_eq=y.ravel(), integrality=1) total += int(round(res.fun)) return total print(solve_251002(data)) 1 20871 I still don\u0026rsquo;t quite understand why it works, but with some trial and error on some example problems I\u0026rsquo;ve managed to make it work. I\u0026rsquo;m always interested in how linear programming works; I intend to implement the algorithm myself later.\n11 counting server paths for the reactor We are given a directed graph of server devices, one line per device; each line is node: child1 child2 ... meaning data flows from node to children; part 1: count all distinct paths from you to out; part 2: count all distinct paths from svr to out that pass through both fft and dac. We simply parse the data into a dict of parent-children pairs, with the children being a set.\n1 2 3 4 5 6 7 8 9 10 11 def parse_2511(data: str) -\u0026gt; dict[str, set[str]]: \u0026#34;\u0026#34;\u0026#34;Parse graph as parent -\u0026gt; children sets.\u0026#34;\u0026#34;\u0026#34; graph: dict[str, set[str]] = {} for line in data.splitlines(): parent, children = line.split(\u0026#39;:\u0026#39;) graph[parent] = set(children.split()) return graph D11 = models.Puzzle(year=2025, day=11) graph = parse_2511(D11.input_data) print(len(graph), sum(len(v) for v in graph.values())) 1 643 1738 The core of the problem is counting paths in a directed acyclic graph. This is a classic dynamic programming problem, where the problem can be decomposed into multiple subproblems, and each subproblem of the same format as the overall problem. Besides, in each of the subproblem and sub-subproblems, the same computation has to be carried out repeatedly.\nBecause of this, we need to carefully manage two things, formulating the solution in a way that the same problem-solving logic can be reused, and memoization.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 from functools import lru_cache def count_paths(graph: dict[str, set[str]], start: str, end: str) -\u0026gt; int: \u0026#34;\u0026#34;\u0026#34;Count paths from start to end in a DAG.\u0026#34;\u0026#34;\u0026#34; @lru_cache(maxsize=None) def dfs(node: str) -\u0026gt; int: if node == end: return 1 return sum(dfs(child) for child in graph.get(node, ())) return dfs(start) def solve_251101(data: str) -\u0026gt; int: \u0026#34;\u0026#34;\u0026#34;Part 1: count paths from you to out. DFS + memoization on node. O(V+E) time, O(V) space. \u0026#34;\u0026#34;\u0026#34; graph = parse_2511(data) return count_paths(graph, start=\u0026#39;you\u0026#39;, end=\u0026#39;out\u0026#39;) print(solve_251101(D11.input_data)) 1 574 I originally solved part 1 without considering any edge cases, since there was none; and I didn\u0026rsquo;t even bother with memoization, since the computation is manageable without it. And then for part 2 I had to patch it up again and again to fix all the edge cases.\nRecursive functions are hard to understand and reason with. It\u0026rsquo;s better to print out the result of each iteration to understand how it\u0026rsquo;s stacked up.\nPart 2 adds some constraint: the path must pass through two specific nodes, fft and dac. Since one must be before the other, I used a rather silly approach to determine which is before which, but it worked. My original implementation for part 1 is quite limited, I used it to count #(fft -\u0026gt; dac), if fft is before dac it will not finish, since I didn\u0026rsquo;t use memoization and it\u0026rsquo;s very slow; and if dac is before fft it will throw an error, since I didn\u0026rsquo;t take such reverse direction cases into consideration. So using this weird method I figured out their order in the directed graph, and fft is upstream of dac.\nThat means every valid path factors into three independent segments: and we can count the unique paths of each segment independently, then multiply them together, i.e. #(svr -\u0026gt; fft) * #(fft -\u0026gt; dac) * #(dac -\u0026gt; out).\n1 2 3 4 5 6 7 8 9 10 11 12 13 def solve_251102(data: str) -\u0026gt; int: \u0026#34;\u0026#34;\u0026#34;Part 2: paths from svr to out passing through fft and dac. Factor into three independent path counts. O(V+E) per segment, O(V) space. \u0026#34;\u0026#34;\u0026#34; graph = parse_2511(data) count0 = count_paths(graph, start=\u0026#39;svr\u0026#39;, end=\u0026#39;fft\u0026#39;) count1 = count_paths(graph, start=\u0026#39;fft\u0026#39;, end=\u0026#39;dac\u0026#39;) count2 = count_paths(graph, start=\u0026#39;dac\u0026#39;, end=\u0026#39;out\u0026#39;) return count0 * count1 * count2 print(solve_251102(D11.input_data)) 1 306594217920240 12 fitting present shapes in the Christmas tree farm We are in the Christmas tree farm, we have a number of Christmas presents, in one of the different shapes, all fitted in a 3x3 grid; we also have a list of regions, each a rectangle of a given size, to accomodate the gifts; gifts can be rotated or flipped; to be fitted into the given regions, but without overlap; day 12 has only one problem, to count how many of the given regions can fit all required shapes. Parse the data:\nshapes into a dict of index:shape pairs, regions into a nesting list of region width, length, and a tuple of required shape counts. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 def parse_2512(data: str) -\u0026gt; tuple[dict[int, list[str]], list[tuple[int, int, tuple[int, ...]]]]: \u0026#34;\u0026#34;\u0026#34;Parse shapes and region specs.\u0026#34;\u0026#34;\u0026#34; shapes: dict[int, list[str]] = {} regions: list[tuple[int, int, tuple[int, ...]]] = [] current = None rows: list[str] = [] for line in data.splitlines(): if not line: continue if line.endswith(\u0026#39;:\u0026#39;) and line[:-1].isdigit(): # starting new shapes if current is not None: shapes[current] = rows current = int(line[:-1]) rows = [] elif \u0026#39;x\u0026#39; in line and \u0026#39;:\u0026#39; in line: # starting new regions area, nums = line.split(\u0026#39;:\u0026#39;) w, h = map(int, area.split(\u0026#39;x\u0026#39;)) counts = tuple(map(int, nums.split())) regions.append((w, h, counts)) else: # accumulating lines for current shape rows.append(line) if current is not None: shapes[current] = rows return shapes, regions D12 = models.Puzzle(year=2025, day=12) shapes, regions = parse_2512(D12.input_data) print(len(shapes), len(regions)) 1 6 1000 This is an interesting parsing problem, because the input data have two distinct parts which require different parsing logic. Also, the data parsing is line by line, but each shape takes up multiple lines, so we also need a way to accumulate lines for each shape.\nAs to the problem itself, this is a hard problem with a simple solution. We want to fit a number of irregular shaped presents into a regular rectangular region, there must be a minimum area requirement, where all the shapes fit perfectly together; and a maximum area requirement, where none of the shapes can fit together, and we thus have to spare a 3x3 grid for each of them. We can first check whether each region\u0026rsquo;s area is in this (minimum, maximum) range, and filter out the regions that ain\u0026rsquo;t. For regions with area less than the minimum no fitting is possible; for regions with areas bigger than the maximum we can simply put the gifts consecutively, each in a 3x3 grid, so the solution is trivial.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 shape_ids = sorted(shapes) filled = [sum(c == \u0026#39;#\u0026#39; for c in \u0026#39;\u0026#39;.join(shapes[i])) for i in shape_ids] box = [len(shapes[i]) * len(shapes[i][0]) for i in shape_ids] # 3x3 here classifications = [0] * len(regions) for i, (w, h, counts) in enumerate(regions): area = w * h compact = sum(n * f for n, f in zip(counts, filled)) loose = sum(n * b for n, b in zip(counts, box)) if compact \u0026gt; area: # no solution possible classifications[i] = 1 if loose \u0026lt;= area: # no work necessay classifications[i] = 2 for i in [0, 1, 2]: print(i, sum(c==i for c in classifications)) 1 2 3 0 0 1 519 2 481 As it turns out, of the 1000 problem regions, 519 of them are not possible, 481 of them are trivial, and none of them requires actual work. So the feasible ones are identical to the trivial ones.\n1 2 3 4 def solve_2512(data: str) -\u0026gt; int: return sum(c==2 for c in classifications) print(solve_2512(D12.input_data)) 1 481 ","permalink":"https://riversdark.github.io/posts/2026/01/solving-advent-of-code-2025/","summary":"\u003cp\u003eI took the Solve It with Code course from answer.ai, and Advent of Code is used as practice.\u003c/p\u003e\n\u003cp\u003eI always thought my Python skills were quite mediocre; I always used to in a \u0026ldquo;just get things done\u0026rdquo; style, so this is also a chance to improve my Python.\nFor this reason I want to mostly stick with the standard packages and pythonic styles, and only use extra packages when it\u0026rsquo;s too cumbersome otherwise.\u003c/p\u003e","title":"Solving Advent of Code 2025"},{"content":"This is an introduction to NumPyro, using the Eight Schools model as example.\nHere we demonstrate the effects of model reparameterisation. Reparameterisation is especially important in hierarchical models, where the joint density tend to have high curvatures.\n1 2 3 4 5 6 7 8 import numpy as np import jax.numpy as jnp import numpyro import numpyro.distributions as dist from jax import random from numpyro.infer import MCMC, NUTS, Predictive rng_key = random.PRNGKey(0) Here we are using the classic eight schools dataset from Gelman et al. We have collected the test score statistics for eight schools, including the mean and standard error. The goal, is to determine whether some schools have done better than others. Note that since we are working with the mean and standard error of eight different schools, we are actually modeling the statistical analysis resutls of some other people: this is essentially a meta analysis problem.\n1 2 3 J = 8 y = np.array([28.0, 8.0, -3.0, 7.0, -1.0, 1.0, 18.0, 12.0]) sigma = np.array([15.0, 10.0, 16.0, 11.0, 9.0, 11.0, 10.0, 18.0]) Visualize the data.\n1 2 3 4 5 6 7 8 9 10 11 12 13 import matplotlib.pyplot as plt import seaborn as sns from scipy.stats import norm sns.set_theme() sns.set_palette(\u0026#34;Set2\u0026#34;) fig, ax = plt.subplots(figsize=(8, 4)) x = np.linspace(-50, 70, 1000) for mean, sd in zip(y, sigma): ax.plot(x, norm.pdf(x, mean, sd)) plt.title(\u0026#39;Test Score from Eight Schools\u0026#39;) The baseline model The model we are building is a standard hierarchical model. We assume that the observed school means represent their true mean \\(\\theta_n\\), but corrupted with some Normal noise, and the true means are themselves drawn from another district level distribution, with mean \\(\\mu\\) and standard deviation \\(\\tau\\), which are also modeled with suitable distributions. Essentially it\u0026rsquo;s Gaussian all the way up, and we have three different levels to consider: student, school, and the whole district level population.\n\\[ \\begin{align*} y_n \u0026\\sim \\text{N} (\\theta_n, \\sigma_n) \\\\ \\theta_n \u0026\\sim \\text{N} (\\mu, \\tau) \\\\ \\mu \u0026\\sim \\text{N} (0, 5) \\\\ \\tau \u0026\\sim \\text{HalfCauchy} (5). \\end{align*} \\]In NumPyro models are coded as functions.\n1 2 3 4 5 6 7 def es_0(J, sigma): mu = numpyro.sample(\u0026#39;mu\u0026#39;, dist.Normal(0, 5)) tau = numpyro.sample(\u0026#39;tau\u0026#39;, dist.HalfCauchy(5)) with numpyro.plate(\u0026#39;J\u0026#39;, J): theta = numpyro.sample(\u0026#39;theta\u0026#39;, dist.Normal(mu, tau)) numpyro.sample(\u0026#39;obs\u0026#39;, dist.Normal(theta, sigma)) Note that the code and the mathematical model are almost identical, except that they go in different directions. In the mathematical model, we start with the observed data, and reason backward to determine how they might be generated. In the code we start with the hyperparameters, and move forward to generate the observed data.\nJ and sigma are data we used to build our model, but they are not part of the model, in the sense that they are not assigned any probability distribution. y, on the other hand, is the central variable of the model, and is named obs in the model.\nnumpyro.plate is used to denote that the variables inside the plate are conditionally independent. Probability distributions are the building blocks of Bayesian models, and NumPyro has a lot of them. In NunPyro, probability distributions are wrappers of JAX random number generators, and they are translated into sampling statements using the numpyro.sample primitive.\nHowever numpyro.sample is used to define the model, not to draw samples from the distribution. To actually draw samples from a distribution, we use numpyro.infer.Predictive. In numpyro each model defines a joint distribution, and since it\u0026rsquo;s a probability distribution, we can draw samples from it. And since we haven\u0026rsquo;t conditioned on any data, the samples we draw are from the prior distribution.\nSampling directly from the prior distribution, and inspect the samples, is a good way to check if the model is correctly defined.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 es_prior_predictive_0 = Predictive(es_0, num_samples=1000) es_prior_samples_0 = es_prior_predictive_0(rng_key, J, sigma) def print_stats(name, samples): print(f\u0026#34;Variable: {name}\u0026#34;) print(f\u0026#34; Shape: {samples.shape}\u0026#34;) print(f\u0026#34; Mean: {jnp.mean(samples, axis=0)}\u0026#34;) print(f\u0026#34; Variance: {jnp.var(samples, axis=0)}\\n\u0026#34;) # Print statistics for each variable print_stats(\u0026#39;mu\u0026#39;, es_prior_samples_0[\u0026#39;mu\u0026#39;]) print_stats(\u0026#39;tau\u0026#39;, es_prior_samples_0[\u0026#39;tau\u0026#39;]) print_stats(\u0026#39;theta\u0026#39;, es_prior_samples_0[\u0026#39;theta\u0026#39;]) print_stats(\u0026#39;obs\u0026#39;, es_prior_samples_0[\u0026#39;obs\u0026#39;]) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 Variable: mu Shape: (1000,) Mean: 0.07919252663850784 Variance: 25.190040588378906 Variable: tau Shape: (1000,) Mean: 18.472793579101562 Variance: 4237.69921875 Variable: theta Shape: (1000, 8) Mean: [ 0.9881359 0.5466191 -3.216042 2.8044345 -1.2074522 2.4413567 4.238887 0.55094534] Variance: [ 5404.2183 2688.3088 4958.9756 2792.266 2839.972 6843.413 23103.627 1778.8481] Variable: obs Shape: (1000, 8) Mean: [ 0.3910051 0.649485 -3.0550027 3.0970583 -1.112252 2.3252409 4.4084034 1.3089797] Variance: [ 5565.5737 2790.3027 5192.405 2918.936 2898.9036 6917.7285 23114.23 2121.8855] When the samples of some variables are observed, we can condition on these observations, and infer the conditional distributions of the other variables. This process is called inference, and is commonly done using MCMC methods. The conditioning is done using the numpyro.handlers.condition primitive, by feeding it a data dict and the model.\nSince we have a GPU available, we will also configure the MCMC sampler to use vectorized chains.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from numpyro.handlers import condition mcmc_args = { \u0026#39;num_warmup\u0026#39;: 1000, \u0026#39;num_samples\u0026#39;: 5000, \u0026#39;num_chains\u0026#39;: 8, \u0026#39;progress_bar\u0026#39;: True, \u0026#39;chain_method\u0026#39;: \u0026#39;vectorized\u0026#39; } es_conditioned_0 = condition(es_0, data={\u0026#39;obs\u0026#39;: y}) es_nuts_0 = NUTS(es_conditioned_0) es_mcmc_0 = MCMC(es_nuts_0, **mcmc_args) es_mcmc_0.run(rng_key, J, sigma) 1 sample: 100% 6000/6000 [00:18\u0026lt;00:00, 328.12it/s] The NUTS sampler is a variant of the Hamiltonian Monte Carlo (HMC) sampler, which is a powerful tool for sampling from complex probability distributions. HMC also comes with its own set of diagnostics, which can be used to check the convergence of the Markov Chain. The most important ones are the effective sample size and the Gelman-Rubin statistic, which is a measure of the convergence of the Markov Chain.\n1 es_mcmc_0.print_summary() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 mean std median 5.0% 95.0% n_eff r_hat mu 4.46 3.30 4.55 -0.72 10.12 2801.92 1.00 tau 4.11 3.21 3.26 0.49 8.25 1534.54 1.01 theta[0] 6.61 5.83 6.05 -2.52 15.59 6268.80 1.00 theta[1] 5.04 4.86 5.00 -2.70 13.15 6283.30 1.00 theta[2] 3.90 5.57 4.24 -4.87 12.66 6632.62 1.00 theta[3] 4.90 4.98 4.91 -3.12 13.02 6666.61 1.00 theta[4] 3.53 4.85 3.89 -4.25 11.55 4699.78 1.00 theta[5] 4.04 5.04 4.38 -3.89 12.33 5403.89 1.00 theta[6] 6.63 5.25 6.09 -1.70 15.17 5692.19 1.00 theta[7] 4.98 5.59 4.99 -4.08 13.58 7813.93 1.00 Number of divergences: 1169 The effective sample size for \\(\\tau\\) is low, and judging from the r_hat value, the Markov Chain might not have converged. Also, the large number of divergences is a sign that the model might not be well specified. Here we are looking at a prominent problem in hierarchical modeling, known as Radford\u0026rsquo;s funnel, where the posterior distribution has a very sharp peak at the center, and a long tail, and the curvature of the distribution is very high. This seriously hinders the performance of HMC, and the divergences are a sign that the sampler is having trouble exploring the space.\nHowever, the issue can be readily rectified using a non-centered parameterization.\nManual reparameterisation The remedy we are proposing is quite simple: replacing\n$$ \\theta_n \\sim \\text{N} (\\mu, \\tau) $$with\n\\[ \\begin{align*} \\theta_n \u0026= \\mu + \\tau \\theta_0 \\\\ \\theta_0 \u0026\\sim \\text{N} (0, 1). \\end{align*} \\]In essence, instead of drawing from a Normal distribution whose parameters are themselves variables in the model, we draw from the unit Normal distribution, and transform it to get the variable we want. By doing so we untangled the sampling process of \\(\\theta\\) from that of \\(\\mu\\) and \\(\\tau\\).\n1 2 3 4 5 6 7 8 def es_1(J, sigma): mu = numpyro.sample(\u0026#39;mu\u0026#39;, dist.Normal(0, 5)) tau = numpyro.sample(\u0026#39;tau\u0026#39;, dist.HalfCauchy(5)) with numpyro.plate(\u0026#39;J\u0026#39;, J): theta_0 = numpyro.sample(\u0026#39;theta_0\u0026#39;, dist.Normal(0, 1)) theta = numpyro.deterministic(\u0026#39;theta\u0026#39;, mu + theta_0 * tau) numpyro.sample(\u0026#39;obs\u0026#39;, dist.Normal(theta, sigma)) Here we use another primitive, numpyro.deterministic, to register the transformed variable, so that its values can be stored and used later.\n1 2 3 4 es_prior_predictive_1 = Predictive(es_1, num_samples=1000) es_prior_samples_1 = es_prior_predictive_1(rng_key, J, sigma) print_stats(\u0026#39;theta_0\u0026#39;, es_prior_samples_1[\u0026#39;theta_0\u0026#39;]) 1 2 3 4 5 6 Variable: theta_0 Shape: (1000, 8) Mean: [ 0.03691418 0.02472821 0.01554041 0.03054582 -0.01188057 0.00954548 -0.01233995 0.03313057] Variance: [1.0163321 1.0030503 0.9524028 0.9750142 1.0098913 0.9325964 1.0606602 1.0366122] we can see the mean and variance are indeed quite close to that of the unit normal.\nCondition on the observed data and do inference.\n1 2 3 4 5 6 es_conditioned_1 = condition(es_1, data={\u0026#39;obs\u0026#39;: y}) es_nuts_1 = NUTS(es_conditioned_1) es_mcmc_1 = MCMC(es_nuts_1, **mcmc_args) es_mcmc_1.run(rng_key, J, sigma) es_mcmc_1.print_summary(exclude_deterministic=False) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 sample: 100% 6000/6000 [00:10\u0026lt;00:00, 546.18it/s] mean std median 5.0% 95.0% n_eff r_hat mu 4.37 3.32 4.39 -1.12 9.79 36259.41 1.00 tau 3.61 3.20 2.78 0.00 7.86 29127.09 1.00 theta[0] 6.21 5.57 5.66 -2.33 14.76 37206.26 1.00 theta[1] 4.93 4.69 4.85 -2.45 12.63 45631.36 1.00 theta[2] 3.89 5.30 4.13 -4.26 12.39 37622.30 1.00 theta[3] 4.77 4.79 4.72 -2.97 12.43 41713.54 1.00 theta[4] 3.60 4.68 3.85 -3.90 11.11 43218.27 1.00 theta[5] 4.01 4.85 4.20 -3.50 11.99 42052.70 1.00 theta[6] 6.29 5.07 5.78 -1.80 14.32 41045.35 1.00 theta[7] 4.87 5.34 4.76 -3.54 13.29 38723.13 1.00 theta_0[0] 0.32 0.99 0.34 -1.33 1.92 42168.81 1.00 theta_0[1] 0.10 0.93 0.11 -1.45 1.60 47772.91 1.00 theta_0[2] -0.08 0.97 -0.09 -1.67 1.50 46935.15 1.00 theta_0[3] 0.07 0.94 0.07 -1.49 1.61 46038.02 1.00 theta_0[4] -0.16 0.93 -0.16 -1.69 1.39 46670.00 1.00 theta_0[5] -0.07 0.94 -0.07 -1.56 1.55 44819.96 1.00 theta_0[6] 0.36 0.96 0.38 -1.30 1.88 41323.50 1.00 theta_0[7] 0.08 0.99 0.08 -1.59 1.64 44932.38 1.00 Number of divergences: 4 This looks much better, both the effective sample size and the r_hat have massively improved for the hyperparameter tau, and the number of divergences is also much lower. However, the fact that there are still divergences tells us that reparameterisation might improve the topology of the posterior parameter space, but there is no guarantee that it will completely eliminate the problem. When doing Bayesian inference, especially with models of complex dependency relationships as in hierarchical models, good techniques are never a sufficient replacement for good thinking.\nUsing numpyro\u0026rsquo;s reparameterisation handler Since this reparameterisation is so widely used, it has already been implemented in NumPyro. And since reparameterisation in general is so important in probabilistic modelling, NumPyro has implemented a wide suite of them.\nIn probabilistic modeling, although it\u0026rsquo;s always a good practice to separate modeling from inference, it\u0026rsquo;s not always easy to do so. As we have seen, how we formulate the model can have a significant impact on the inference performance. When building the model, not only do we need to configure the variable transformations, but we also need to inform the inference engine how to handle these transformed variables. This is where the numpyro.handlers.reparam handler comes in.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from numpyro.handlers import reparam from numpyro.infer.reparam import TransformReparam from numpyro.distributions.transforms import AffineTransform from numpyro.distributions import TransformedDistribution def es_2(J, sigma): mu = numpyro.sample(\u0026#39;mu\u0026#39;, dist.Normal(0, 5)) tau = numpyro.sample(\u0026#39;tau\u0026#39;, dist.HalfCauchy(5)) with numpyro.plate(\u0026#39;J\u0026#39;, J): with reparam(config={\u0026#39;theta\u0026#39;: TransformReparam()}): theta = numpyro.sample( \u0026#39;theta\u0026#39;, TransformedDistribution(dist.Normal(0., 1.), AffineTransform(mu, tau))) numpyro.sample(\u0026#39;obs\u0026#39;, dist.Normal(theta, sigma)) The process of reparameterisation goes as follows:\nStart with a standard Normal distribution dist.Normal(0., 1.), Transform it using the affine transformation AffineTransform(mu, tau), Denote the result as a TransformedDistribution, Register the transformed variable theta using numpyro.sample, Inform the inference engine of the reparameterisation using reparam. Proceed with prior predictive sampling.\n1 2 3 4 es_prior_predictive_2 = Predictive(es_2, num_samples=1000) es_prior_samples_2 = es_prior_predictive_2(rng_key, J, sigma) print_stats(\u0026#39;theta\u0026#39;, es_prior_samples_2[\u0026#39;theta\u0026#39;]) 1 2 3 4 5 6 Variable: theta Shape: (1000, 8) Mean: [ 0.9881359 0.5466191 -3.216042 2.8044345 -1.2074522 2.4413567 4.238887 0.55094534] Variance: [ 5404.2183 2688.3088 4958.9756 2792.266 2839.972 6843.413 23103.627 1778.8481] Condition on the observed data and do inference.\n1 2 3 4 5 6 es_conditioned_2 = condition(es_2, data={\u0026#39;obs\u0026#39;: y}) es_nuts_2 = NUTS(es_conditioned_2) es_mcmc_2 = MCMC(es_nuts_2, **mcmc_args) es_mcmc_2.run(rng_key, J, sigma) es_mcmc_2.print_summary() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 sample: 100% 6000/6000 [00:10\u0026lt;00:00, 550.07it/s] mean std median 5.0% 95.0% n_eff r_hat mu 4.37 3.32 4.39 -1.12 9.79 36259.41 1.00 tau 3.61 3.20 2.78 0.00 7.86 29127.09 1.00 theta_base[0] 0.32 0.99 0.34 -1.33 1.92 42168.81 1.00 theta_base[1] 0.10 0.93 0.11 -1.45 1.60 47772.91 1.00 theta_base[2] -0.08 0.97 -0.09 -1.67 1.50 46935.15 1.00 theta_base[3] 0.07 0.94 0.07 -1.49 1.61 46038.02 1.00 theta_base[4] -0.16 0.93 -0.16 -1.69 1.39 46670.00 1.00 theta_base[5] -0.07 0.94 -0.07 -1.56 1.55 44819.96 1.00 theta_base[6] 0.36 0.96 0.38 -1.30 1.88 41323.50 1.00 theta_base[7] 0.08 0.99 0.08 -1.59 1.64 44932.38 1.00 Number of divergences: 4 The model reparameterised using handlers, as we can see, performs just like the manually reparameterised one, with the same mean and variance estimation, same effective number of samples, and the same number of divergences.\nThe results are consistent with the manual reparameterisation. It might seem uncessarily complicated to use the reparameterisation handler in this simple example, but in more complex models, especially those with many layers of dependencies, the reparameterisation handler can greatly facilitate the model building process.\n","permalink":"https://riversdark.github.io/stats/eight/","summary":"\u003cp\u003eThis is an introduction to NumPyro, using the Eight Schools model as example.\u003c/p\u003e\n\u003cp\u003eHere we demonstrate the effects of model reparameterisation.\nReparameterisation is especially important in hierarchical models, where\nthe joint density tend to have high curvatures.\u003c/p\u003e\n\u003cp\u003e\u003c!-- raw HTML omitted --\u003e\u003c!-- raw HTML omitted --\u003e\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv class=\"chroma\"\u003e\n\u003ctable class=\"lntable\"\u003e\u003ctr\u003e\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode\u003e\u003cspan class=\"lnt\"\u003e1\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e2\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e3\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e4\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e5\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e6\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e7\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e8\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"nn\"\u003enumpy\u003c/span\u003e \u003cspan class=\"k\"\u003eas\u003c/span\u003e \u003cspan class=\"nn\"\u003enp\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"nn\"\u003ejax.numpy\u003c/span\u003e \u003cspan class=\"k\"\u003eas\u003c/span\u003e \u003cspan class=\"nn\"\u003ejnp\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"nn\"\u003enumpyro\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"nn\"\u003enumpyro.distributions\u003c/span\u003e \u003cspan class=\"k\"\u003eas\u003c/span\u003e \u003cspan class=\"nn\"\u003edist\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003ejax\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003erandom\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003enumpyro.infer\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eMCMC\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eNUTS\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ePredictive\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003erng_key\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003erandom\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ePRNGKey\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003eHere we are using the classic eight schools dataset from Gelman et\nal. We have collected the test score statistics for eight schools,\nincluding the mean and standard error. The goal, is to determine whether\nsome schools have done better than others. Note that since we are\nworking with the mean and standard error of eight different schools, we\nare actually modeling the statistical analysis resutls of some other\npeople: this is essentially a meta analysis problem.\u003c/p\u003e","title":"Eight Schools, or the importance of model reparameterisation"},{"content":"Un extrait de Des bribes du souvenir de l’époque de Quatre Mai, d’Eileen Chang.\nIls étaient toujours quatre personnes ; parfois un autre couple les rejoint et ils font six, mais ils n’étaient jamais deux. La situation a ainsi continué pendant presque un an. Luo et Guo étaient mariés - le malheur commun des hommes de cette époque. On a jamais entendu le mot l’amour et on est déjà marié ; et on a déjà plusieurs enfants. Entre Luo et Guo, et leurs deux petites amies, on laisse cours francs leurs sentiments, mais on reste toujours dans la lisière de la coutume sociale. Et ça ne gêne personne non plus.\nLes quatre se voyaient presque chaque jour, mais ils s’écrivaient toutefois aussi très souvent. Quand les deux filles n’étaient pas là, les deux hommes discutaient de leurs amies avec grand entrain. Ils partagent entre eux leurs lettres. Ils savourent chacun de leurs mots, de leurs gestes, de leurs sourires. Ils étudiaient ensemble méticuleusement leurs personnalités par leurs manières d’écrire, et par leurs choix des mots. Ils décident que mademoiselle Zhou est une fille de courage, et mademoiselle Fan une de quiétude. Ils analysent tout, de toute la profondeur, ils n’en ont trouvé aucune nouveauté depuis longtemps, mais ils continuent leur jeu avec enthousiasme. Chacun ne lésine sur ses compliments envers l’amie de l\u0026rsquo;autre, mais chacun est de l’avis que sa propre amie est la meilleure. Ils ne sont jamais rassasiés de leur jeu. En Chine, pendant ce temps-là, la relation romantique était une expérience toute nouvelle : on profite avec toute excitation d’une relation purement spirituelle.\n","permalink":"https://riversdark.github.io/culture/54/","summary":"\u003cp\u003e\u003cem\u003eUn extrait de \u003cstrong\u003eDes bribes du souvenir de l’époque de Quatre Mai\u003c/strong\u003e, d’Eileen Chang.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eIls étaient toujours quatre personnes ; parfois un autre couple les rejoint et ils font six, mais ils n’étaient jamais deux. La situation a ainsi continué pendant presque un an. Luo et Guo étaient mariés - le malheur commun des hommes de cette époque. On a jamais entendu le mot l’amour et on est déjà marié ; et on a déjà plusieurs enfants. Entre Luo et Guo, et leurs deux petites amies, on laisse cours francs leurs sentiments, mais on reste toujours dans la lisière de la coutume sociale. Et ça ne gêne personne non plus.\u003c/p\u003e","title":"54"}]