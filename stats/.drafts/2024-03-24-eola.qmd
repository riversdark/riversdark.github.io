---
title: "Elements of Learning Algorithms"
description: "A survey of machine learning algorithms applied to mixture modeling"
date: 2024-03-24
categories: [torch]
draft: true
---

In this post we will take a (not so brief, also not so comprehensive, albeit long) survey of some common learning algorithms in machine learning, using mixture models as the working example. The goal is to showcase how these methods works, and compare them with each other. We have chosen mixture models as the working example, because they are multi-modal models with both visible and hidden, and both discrete and continuous variables, so they are a good candidate to try out different learning algorithms. And also, of course, because they are ubiquitous in machine learning, and immensely useful in practice.

We will proceed following this roadmap:

- introduce and visualize the data
- K-means clustering
- EM (expectation maximization)
- SA (simulated annealing)
- VI (variational inference)
- MCMC (MH and Gibbs)
- normalizing flows 
- score matching
- flow matching
- Langevin sampling


We start with simple cases, to showcase the capabilities of each methods. Later we'll also show some more demanding examples to showcase the limitations, and why some methods are better suitable for some cases than others.

Of course, with a simple Gaussian mixture data, it's not possible to fully compare the advantages and limitations of each method.

## Data

We'll first write a data generating process to generate some data. This is a 2 component, 2 dimensional correlated bivariate Gaussian, with the first component having a larger variance and smaller correlation, and the second component having a smaller variance and larger correlation. We use synthetic data to better evaluate the performance of the various algorithms.

```{python}
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
sns.set_theme(palette='Set2')
colors = sns.color_palette()

np.random.seed(42)
n_points1 = 500
n_points2 = 1000

mean1 = [-1, -2]
cov1 = [[1, 0.2], [0.2, 1]]  

mean2 = [2, 1]
cov2 = [[0.5, 0.4], [0.4, 0.5]]

data1 = np.random.multivariate_normal(mean1, cov1, n_points1)
data2 = np.random.multivariate_normal(mean2, cov2, n_points2)
data = np.vstack((data1, data2))

df = pd.DataFrame(data, columns=['X', 'Y'])
df['Component'] = np.hstack((np.zeros(n_points1, dtype=int), np.ones(n_points2, dtype=int)))

plt.figure(figsize=(10, 8))
sns.scatterplot(data=df, x='X', y='Y', hue='Component')
plt.title("2-component 2D Gaussian Data")
plt.axis('equal')
plt.show()
```

As we can see, with varied variance and correlation, and some overlapping in the middle, it is not easy to see that these are generated by two separate Gaussian distributions. And since there are some overlap in the generated data, it won't be easy for any clustering algorithm to separate the two components. Realistically speaking, with only the observable data and without knowing the generative process, it's simply impossible to determine which points belong to which component.

## K-means

a self-contained, minimalistic implementation, using the scipy implementation as reference.

algorithm borrowed from Bishop DLFC 15.1.
## EM

## VI

## MH

## Gibbs

## Normalizing flows

## Flow matching

## Langevin sampling

## Score matching