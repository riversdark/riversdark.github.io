---
title: "Elements of machine learning algorithms"
description: "A survey of learning algorithms, with applications to mixture models"
date: 2024-07-01
categories: [torch]
draft: true
---

In this post we will take a (not so) brief survey of some learning algorithms in machine learning, using mixture models as the working example.  The goal is to showcase how these methods works, and compare their merits and limitations with each other.  Mixture models are multi-modal, have both visible and hidden variables, and have both discrete and continuous parameters, so they are a good example to showcase the various methods.


We will proceed following this roadmap:

- generate data as 2 component, 2 dimensional correlated bivariate Gaussian
- visualize the data
- use K-means to cluster the data
- use EM to fit a GMM model to the data
- use VI to fit a GMM model to the data
- use MCMC (MH and Gibbs) to fit a GMM model to the data
- use normalizing flows (coupling flow, autoregressive flow, continuous flow, flow matching) to fit a GMM model to the data
- use Langevin sampling to fit a GMM model to the data
- use score matching to fit a GMM model to the data

We start with simple cases, to showcase the capabilities of each methods. Later we'll also show some more demanding examples to showcase the limitations, and why some methods are better suitable for some cases than others.

Of course, with a simple Gaussian mixture data, it's not possible to fully compare the advantages and limitations of each method.

## Data

We'll first write a data generating process to generate some data. This is a 2 component, 2 dimensional correlated bivariate Gaussian, with the first component having a larger variance and smaller correlation, and the second component having a smaller variance and larger correlation. We use synthetic data to better evaluate the performance of the various algorithms.

```{python}
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
sns.set_theme(palette='Set2')
colors = sns.color_palette()

np.random.seed(42)
n_points1 = 500
n_points2 = 1000

mean1 = [-1, -2]
cov1 = [[1, 0.2], [0.2, 1]]  

mean2 = [2, 1]
cov2 = [[0.5, 0.4], [0.4, 0.5]]

data1 = np.random.multivariate_normal(mean1, cov1, n_points1)
data2 = np.random.multivariate_normal(mean2, cov2, n_points2)
data = np.vstack((data1, data2))

df = pd.DataFrame(data, columns=['X', 'Y'])
df['Component'] = np.hstack((np.zeros(n_points1, dtype=int), np.ones(n_points2, dtype=int)))

plt.figure(figsize=(10, 8))
sns.scatterplot(data=df, x='X', y='Y', hue='Component')
plt.title("2-component 2D Gaussian Data")
plt.axis('equal')
plt.show()
```

As we can see, with varied variance and correlation, and some overlapping in the middle, it is not easy to see that these are generated by two separate Gaussian distributions. And since there are some overlap in the generated data, it won't be easy for any clustering algorithm to separate the two components. Realistically speaking, with only the observable data and without knowing the generative process, it's simply impossible to determine which points belong to which component.

## K-means

a self-contained, minimalistic implementation, using the scipy implementation as reference.

algorithm borrowed from Bishop DLFC 15.1.
## EM

## VI

## MH

## Gibbs

## Normalizing flows

## Flow matching

## Langevin sampling

## Score matching