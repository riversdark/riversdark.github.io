---
title: flow matching for generative modeling
---

adapted from https://github.com/gle-bellier/flow-matching/blob/main/Flow_Matching.ipynb

## References:

This notebook centers around the **Flow Matching for Generative Modeling** article [1] and proposes an implementation of _Flow Matching_ in the case of _Optimal Transport conditional Vector Fields_. The implementation proposed in [2] was consulted and it inspired the use of the _Zuko_ [3] package for ODE solving and sampling. Moreover, this notebook adopts the notations of the original article and thus the numbers of the equations are the same as in the paper.

📄 [1] **Flow Matching for Generative Modeling** by Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, Matt Le - [Article](https://arxiv.org/abs/2210.02747)

🐍 [2] **Flow Matching in 100 LOC** by François Rozet - [Implementation](https://gist.github.com/francois-rozet/fd6a820e052157f8ac6e2aa39e16c1aa)

🐍 [3] **Zuko** package - [Website](https://zuko.readthedocs.io/en/stable/index.html)

📄 [4] **Score-Based Generative Modeling through Stochastic Differential Equations** by Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, Ben Poole - [Article](https://arxiv.org/abs/2011.13456)

## Introduction

The article [1] reminds the reader with the following notations:
- $\mathbb{R}^d$ denote the adata space with data points $x = (x^1, ..., x^d) \in \mathbb{R}^d$. 
- $v: [0, 1] \times \mathbb{R}^d \rightarrow \mathbb{R}^d$ is the _time-dependant vector field_. 
- $\phi: [0, 1] \times \mathbb{R}^d \rightarrow \mathbb{R}^d$ is a _flow_. A flow is associated to a vector field $v_t$ thanks to the following equations:
$$
\begin{cases}
  \frac{d}{dt}\phi_t(x) = v_t(\phi_t(x))\\ 
  \phi_0(x) = x \tag{1, 2}\\
\end{cases}
$$ 

The main idea is then to use a neural network to model the vector field $v_t$, which implies that $\phi_t$ will also be a parametric model and we call it a  _Continuous Normalizing Flow_ (CNF). In the context of generative modeling, the CNF allows us to transform a simple prior distribution (_e.g._ gaussian) $p_0$ into a more complex one $p_1$ (which we want close to the data distribution $q$ we need to model) by using the _push-forward equation_:
$$
 p_t = [\phi_t]_*p_0 \tag{3}
$$
Where:
$$
 \forall x \in \mathbb{R}^2, [\phi_t]_*p_0(x) = p_0(\phi^{-1}(x))\det\left[\frac{\partial\phi_t^{-1}}{\partial x}(x)\right]
$$




Thus the _Flow Matching_ aims at learning the true vector field $u_t$:

$$
\mathcal{L}_{FM}(\theta) = \mathbb{E}_{t, p_t(x)} \lVert v_t(x) - u_t(x) \rVert^2 \tag{5}
$$

However this objectif is intractable, so the article [1] introduces the _Conditional Flow Matching_ (CFM) objective where $x_1 \sim q(x_1) \approx p_1(x_1)$ :

$$
\mathcal{L}_{CFM}(\theta) = \mathbb{E}_{t, q(x_1), p_t(x|x_1)} \lVert v_t(x) - u_t(x|x_1) \rVert^2 \tag{9}
$$ 

This objective can be declined in several ways depending on the choice of the form of the conditional probability paths. In this notebook we will focus on the case of _Optimal Transport conditional Vector Fields_ and will leave the _Diffusion conditional Vector Fields_ for future works.




```{python}
#| cellView: form
#@title ⚠️ TODO: imports
%%capture 
!pip install zuko
!pip install torchdiffeq

import matplotlib.pyplot as plt
import torch
import torch.nn as nn

from sklearn.datasets import make_moons, make_swiss_roll
from sklearn.preprocessing import StandardScaler
from torch import Tensor
from torch.distributions import Normal
from torch.utils.data import TensorDataset, DataLoader
from zuko.utils import odeint
from tqdm import tqdm
from typing import *
import numpy as np
import pandas as pd


device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
```

## Optimal Transport conditional Vector Fields:

In this section, we will propose an implementation of the _Optimal Transport conditional Vector Fields_. Please refer to the article for more details about the motivations behind this concept.

```{python}
class OTFlowMatching:
  
  def __init__(self, sig_min: float = 0.001) -> None:
    super().__init__()
    self.sig_min = sig_min
```

Let $x_1 \sim q(x_1)$, _i.e._ a point from the dataset, and we can define the conditional flow $\psi_t$ (conditional version of $\phi_t$) to be the Optimal Transport _displacement map_ between the two Gaussians $p_0(x|x_1)$ and $p_1(x|x_1)$ (we can proove that $\psi_t$ satisfies Eq.3)
$$
\psi_t(x) = (1 - (1 - \sigma_{min})t)x + tx_1 \tag{22}
$$

```{python}
  def psi_t(self, x: torch.Tensor, x_1: torch.Tensor, t: torch.Tensor) -> torch.Tensor:
    """ Conditional Flow
    """
    t = t[:, None].expand(x.shape)
    return (1 - (1 - self.sig_min) * t) * x + t * x_1
```

In the case of _Optimal Transport conditional VFs_ the conditional flow matching objective loss can be written as follow:
$$
\mathcal{L}_{CFM}(\theta) = \mathbb{E}_{t, q(x_1), p(x_0)} \big\lVert v_t(\psi_t(x_0)) - \frac{d}{dt}\psi_t(x_0) \big\rVert^2 \tag{14}
$$
Given (22), we have:
$$
\frac{d}{dt}\psi_t(x_0) = (x_1 - (1 - \sigma_{min})x_0)
$$
Thus, we obtain the final form of the _Conditional Flow Matching_ loss:
$$
\mathcal{L}_{CFM}(\theta) = \mathbb{E}_{t, q(x_1), p(x_0)} \big\lVert v_t(\psi_t(x_0)) - (x_1 - (1 - \sigma_{min})x_0) \big\rVert^2 \tag{23}
$$
We sample $x_1 \sim q(x_1)$ (samples from the dataset), $x_0 \sim p(x_0)$ (samples from the _prior distribution_) and $t \sim \text{Unif}([0, 1])$. It is worth noticing that we apply a little trick for time sampling since we batch covers the range $[0, 1]$ uniformly with a random offset. Then we compute an approximation of the _CFM Loss_ on the batch: 
$$
\mathcal{L}_{\text{batch}}(\theta) = \frac{1}{N}\sum_{i=0}^{N-1} \big\lVert v_t\big(\psi_t\big(x_0^{(i)}\big)\big) - \big(x_{1}^{(i)} - \big(1 - \sigma_{min}\big)x_0^{(i)}\big) \big\rVert^2
$$

```{python}
  def loss(self, v_t: nn.Module, x_1: torch.Tensor) -> torch.Tensor:
    """ Compute loss
    """
    # t ~ Unif([0, 1])
    t = (torch.rand(1, device=x_1.device) + torch.arange(len(x_1), device=x_1.device) / len(x_1)) % (1 - self.eps)
    t = t[:, None].expand(x_1.shape)
    # x ~ p_t(x_0)
    x_0 = torch.randn_like(x_1) 
    v_psi = v_t(t[:,0], self.psi_t(x_0, x_1, t))
    d_psi = x_1 - (1 - self.sig_min) * x_0
    return torch.mean((v_psi - d_psi) ** 2)
```

```{python}
#| cellView: form
#@title ⏳ Summary: please run this cell which contains the ```OTFlowMatching``` class
class OTFlowMatching:
  
  def __init__(self, sig_min: float = 0.001) -> None:
    super().__init__()
    self.sig_min = sig_min
    self.eps = 1e-5

  def psi_t(self, x: torch.Tensor, x_1: torch.Tensor, t: torch.Tensor) -> torch.Tensor:
    """ Conditional Flow
    """
    return (1 - (1 - self.sig_min) * t) * x + t * x_1

  def loss(self, v_t: nn.Module, x_1: torch.Tensor) -> torch.Tensor:
    """ Compute loss
    """
    # t ~ Unif([0, 1])
    t = (torch.rand(1, device=x_1.device) + torch.arange(len(x_1), device=x_1.device) / len(x_1)) % (1 - self.eps)
    t = t[:, None].expand(x_1.shape)
    # x ~ p_t(x_0)
    x_0 = torch.randn_like(x_1) 
    v_psi = v_t(t[:,0], self.psi_t(x_0, x_1, t))
    d_psi = x_1 - (1 - self.sig_min) * x_0
    return torch.mean((v_psi - d_psi) ** 2)
```

## Diffusion conditional Vector Fields:


### Variance Preserving diffusion:
In this section, we propose an implementation of the _Conditional Vector Fields_ in the case of _Variance Preserving_ (VP) _Diffusion_.

```{python}
class VPDiffusionFlowMatching:

  def __init__(self) -> None:
    super().__init__()
    self.beta_min = 0.1
    self.beta_max = 20.0
    self.eps = 1e-5
```

We need to implement the $\mu_t$ and $\sigma_t$ functions which define the following flow:
$$
 \psi_t(x) = \sigma_t(x_1)x + \mu_t(x_1)
$$
These functions rely on the $\alpha_t$ and $\beta_t$ schedules [4].

```{python}
  def T(self, s: torch.Tensor) -> torch.Tensor:
    return self.beta_min * s + 0.5 * (s ** 2) * (self.beta_max - self.beta_min)

  def beta(self, t: torch.Tensor) -> torch.Tensor:
    return self.beta_min + t*(self.beta_max - self.beta_min)

  def alpha(self, t: torch.Tensor) -> torch.Tensor:
    return torch.exp(-0.5 * self.T(t))

  def mu_t(self, t: torch.Tensor, x_1: torch.Tensor) -> torch.Tensor:
    return self.alpha(1. - t) * x_1

  def sigma_t(self, t: torch.Tensor, x_1: torch.Tensor) -> torch.Tensor:
    return torch.sqrt(1. - self.alpha(1. - t) ** 2)
```

In the case of _Variance Preserving_ we have a closed-form equation for the _conditional Vector Field_:
$$
 u_t(x|x_1) = - \frac{T'(1-t)}{2}\left[ \frac{e^{-T(1-t)}x - e^{-\frac{1}{2}T(1-t)}x_1}{1 - e^{-T(1-t)}}\right] \tag{19}
$$

```{python}
  def u_t(self, t: torch.Tensor, x: torch.Tensor, x_1: torch.Tensor) -> torch.Tensor:
    num = torch.exp(-self.T(1. - t)) * x - torch.exp(-0.5 * self.T(1.-t))* x_1
    denum = 1. - torch.exp(- self.T(1. - t))
    return - 0.5 * self.beta(1. - t) * (num/denum)
```

Hence, we can compute the regression loss on the conditional vector fields.
$$
\mathcal{L}_{CFM}(\theta) = \mathbb{E}_{t, q(x_1), p_t(x|x_1)} \lVert v_t(x) - u_t(x|x_1) \rVert^2 \tag{9}
$$ 

```{python}
  def loss(self, v_t: nn.Module, x_1: torch.Tensor) -> torch.Tensor:
    """ Compute loss
    """ 
    # t ~ Unif([0, 1])
    t = (torch.rand(1, device=x_1.device) + torch.arange(len(x_1), device=x_1.device) / len(x_1)) % (1 - self.eps)
    t = t[:, None].expand(x_1.shape)
    # x ~ p_t(x|x_1)
    x = self.mu_t(t, x_1) + self.sigma_t(t, x_1) * torch.randn_like(x_1)

    return torch.mean((v_t(t[:,0], x) - self.u_t(t, x, x_1)) ** 2)
```

```{python}
#| cellView: form
#@title ⏳ Summary: please run this cell which contains the ```VPDiffusionFlowMatching``` class

class VPDiffusionFlowMatching:

  def __init__(self) -> None:
    super().__init__()
    self.beta_min = 0.1
    self.beta_max = 20.0
    self.eps = 1e-5

  def T(self, s: torch.Tensor) -> torch.Tensor:

    return self.beta_min * s + 0.5 * (s ** 2) * (self.beta_max - self.beta_min)

  def beta(self, t: torch.Tensor) -> torch.Tensor:
    
    return self.beta_min + t*(self.beta_max - self.beta_min)

  def alpha(self, t: torch.Tensor) -> torch.Tensor:

    return torch.exp(-0.5 * self.T(t))

  def mu_t(self, t: torch.Tensor, x_1: torch.Tensor) -> torch.Tensor:

    return self.alpha(1. - t) * x_1

  def sigma_t(self, t: torch.Tensor, x_1: torch.Tensor) -> torch.Tensor:

    return torch.sqrt(1. - self.alpha(1. - t) ** 2)

  def u_t(self, t: torch.Tensor, x: torch.Tensor, x_1: torch.Tensor) -> torch.Tensor:

    num = torch.exp(-self.T(1. - t)) * x - torch.exp(-0.5 * self.T(1.-t))* x_1
    denum = 1. - torch.exp(- self.T(1. - t))
    return - 0.5 * self.beta(1. - t) * (num/denum)


  def loss(self, v_t: nn.Module, x_1: torch.Tensor) -> torch.Tensor:
    """ Compute loss
    """ 
    # t ~ Unif([0, 1])
    t = (torch.rand(1, device=x_1.device) + torch.arange(len(x_1), device=x_1.device) / len(x_1)) % (1 - self.eps)
    t = t[:, None].expand(x_1.shape)
    # x ~ p_t(x|x_1)
    x = self.mu_t(t, x_1) + self.sigma_t(t, x_1) * torch.randn_like(x_1)

    return torch.mean((v_t(t[:,0], x) - self.u_t(t, x, x_1)) ** 2)

```

### Variance Exploding Diffusion:

In this section, we propose an implementation of the _Conditional Vector Fields_ in the case of _Variance Exploding_ (VE) _Diffusion_.

```{python}
class VEDiffusionFlowMatching:

  def __init__(self) -> None:
    super().__init__()
    self.sigma_min = 0.01
    self.sigma_max = 2. 
    self.eps = 1e-5
```

In the case of _Variance Exploding_ we have a closed-form equation for the _conditional Vector Field_ that rely on $\sigma_t$ and $\sigma_t'$:
$$
u(x| x_1) = - \frac{\sigma_{1-t}'}{\sigma_{1-t}}(x - x_1) \tag{17}
$$

```{python}
  def sigma_t(self, t: torch.Tensor) -> torch.Tensor:
    return self.sigma_min * (self.sigma_max / self.sigma_min) ** t

  def dsigma_dt(self, t: torch.Tensor) -> torch.Tensor:
    return self.sigma_t(t) * torch.log(torch.tensor(self.sigma_max/self.sigma_min))
```

```{python}
  def u_t(self, t: torch.Tensor, x: torch.Tensor, x_1: torch.Tensor) -> torch.Tensor:
    return -(self.dsigma_dt(1. - t) / self.sigma_t(1. - t)) * (x - x_1)
```

As for VP Diffusion, we can directly compute the regression loss on the conditional vector fields.
$$
\mathcal{L}_{CFM}(\theta) = \mathbb{E}_{t, q(x_1), p_t(x|x_1)} \lVert v_t(x) - u_t(x|x_1) \rVert^2 \tag{9}
$$ 

```{python}
def loss(self, v_t: nn.Module, x_1: torch.Tensor) -> torch.Tensor:
    """ Compute loss
    """ 
    # t ~ Unif([0, 1])
    t = (torch.rand(1, device=x_1.device) + torch.arange(len(x_1), device=x_1.device) / len(x_1)) % (1 - self.eps)
    t = t[:, None].expand(x_1.shape)
    # x ~ p_t(x|x_1)
    x = x_1 + self.sigma_t(1. - t) * torch.randn_like(x_1)

    return torch.mean((v_t(t[:,0], x) - self.u_t(t, x, x_1)) ** 2)
```

```{python}
#| cellView: form
#@title ⏳ Summary: please run this cell which contains the ```VEDiffusionFlowMatching``` class
class VEDiffusionFlowMatching:

  def __init__(self) -> None:
    super().__init__()
    self.sigma_min = 0.01
    self.sigma_max = 2.
    self.eps = 1e-5


  def sigma_t(self, t: torch.Tensor) -> torch.Tensor:

    return self.sigma_min * (self.sigma_max / self.sigma_min) ** t

  def dsigma_dt(self, t: torch.Tensor) -> torch.Tensor:

    return self.sigma_t(t) * torch.log(torch.tensor(self.sigma_max/self.sigma_min))

  def u_t(self, t: torch.Tensor, x: torch.Tensor, x_1: torch.Tensor) -> torch.Tensor:

    return -(self.dsigma_dt(1. - t) / self.sigma_t(1. - t)) * (x - x_1)

  def loss(self, v_t: nn.Module, x_1: torch.Tensor) -> torch.Tensor:
    """ Compute loss
    """ 
    # t ~ Unif([0, 1])
    t = (torch.rand(1, device=x_1.device) + torch.arange(len(x_1), device=x_1.device) / len(x_1)) % (1 - self.eps)
    t = t[:, None].expand(x_1.shape)
    # x ~ p_t(x|x_1)
    x = x_1 + self.sigma_t(1. - t) * torch.randn_like(x_1)

    return torch.mean((v_t(t[:,0], x) - self.u_t(t, x, x_1)) ** 2)
```

## Conditional Vector Field:

We add the ```CondVF```  class which aims to model the _Optimal Transport Conditional Vector Field_ $v_t$. This approximation $v_{t}(\;⋅\;;\theta)$ of the true $v_t$ will be optimized later in this notebook by tuning the parameters $\theta$ of ```CondVF.net```.

```{python}
class CondVF(nn.Module):
  def __init__(self, net: nn.Module) -> None:
    super().__init__()
    self.net = net
```

```{python}
  def forward(self, t: torch.Tensor, x: torch.Tensor) -> torch.Tensor:
    return self.net(t, x)
```

Furthermore, we define the ```encode``` and ```decode``` functions:
- ```encode``` maps the data distribtion $q(x_1)\approx p_1(x_1)$ to the prior distribution $p(x_0)$.
- ```decode``` maps the prior distribution $p(x_0)$ to the data distribtion $p_1(x_1) \approx q(x_1)$.

The article mentions that a flow $\phi_t$ and its corresponding vector field $v_t$ verify the following equations:
$$
\begin{cases}
  \frac{d}{dt}\phi_t(x) = v_t(\phi_t(x))\\ 
  \phi_0(x) = x \tag{1, 2}\\
\end{cases}
$$

Thus, to ```encode``` we integrate the first equation from $t=1$ to $t=0$ and the ```decode``` function is the same but going from $t=0$ to $t=1$. Following François Rozet's implementation and side notes [2], we use the function ```odeint``` from the _Zuko_ package [3]. We build a small ```wrapper``` which only aims at providing time tensor with the correct shapes to the neural network ```net``` when using ```odeint```.

```{python}
  def wrapper(self, t: torch.Tensor, x: torch.Tensor):
      t = t * torch.ones(len(x))
      return self(t, x)

  def encode(self, x_1: torch.Tensor) -> torch.Tensor:
    return odeint(self.wrapper, x_1, 1., 0., self.parameters())

  def decode(self, x_0: torch.Tensor) -> Tensor:
    return odeint(self.wrapper, x_0, 0., 1., self.parameters())
```

```{python}
#| cellView: form
#@title ⏳ Summary: please run this cell which contains the ```CondVF``` class.
class CondVF(nn.Module):
  def __init__(self, net: nn.Module, n_steps: int = 100) -> None:
    super().__init__()
    self.net = net

  def forward(self, t: torch.Tensor, x: torch.Tensor) -> torch.Tensor:
    return self.net(t, x)
    
  def wrapper(self, t: torch.Tensor, x: torch.Tensor) -> torch.Tensor:
      t = t * torch.ones(len(x), device=x.device)
      return self(t, x)

  def decode_t0_t1(self, x_0, t0, t1):
    return odeint(self.wrapper, x_0, t0, t1, self.parameters())


  def encode(self, x_1: torch.Tensor) -> torch.Tensor:
    return odeint(self.wrapper, x_1, 1., 0., self.parameters())

  def decode(self, x_0: torch.Tensor) -> torch.Tensor:
    return odeint(self.wrapper, x_0, 0., 1., self.parameters())
```

## Neural Network for VF approximation:

We want to approximate $v_t$ with a parametric model $v_{t}(\;\cdot\;;\theta)$. Since we deal with point coordinates and want to keep the neural network simple for this example, we use a simple _Multilayer Perceptron_ (MLP).

```{python}
class Net(nn.Module):
  def __init__(self, in_dim: int, out_dim: int, h_dims: List[int], n_frequencies: int) -> None:
    super().__init__()

    ins = [in_dim + 2 * n_frequencies] + h_dims
    outs = h_dims + [out_dim]
    self.n_frequencies = n_frequencies

    self.layers = nn.ModuleList([
        nn.Sequential(nn.Linear(in_d, out_d), nn.LeakyReLU()) for in_d, out_d in zip(ins, outs)
    ])
    self.top = nn.Sequential(nn.Linear(out_dim, out_dim))
```

 However the vector field $v$ is conditioned on time $t$. Thus, we create a _sinusoidal positional encoding_ of $t$ thanks to sine and cosine functions to have a time representation that make more sense than only its value between $0$ and $1$.

```{python}
  def time_encoder(self, t: torch.Tensor) -> torch.Tensor:
    freq = 2 * torch.arange(self.n_frequencies) * torch.pi
    t = freq * t[..., None]
    return torch.cat((t.cos(), t.sin()), dim=-1)
```

When computing $v_t(x)$ we simply concatenate the _sinusoidale positional encoding_ of $t$ with $x$ (input coordinates), and then process the obtained tensor with the MLP's layers.

```{python}
  def forward(self, t: torch.Tensor, x: torch.Tensor) -> torch.Tensor:
    t = self.time_encoder(t)
    x = torch.cat((x, t), dim=-1)
    for l in self.layers:
      x = l(x)
    return self.top(x) 
```

```{python}
#@title ⏳ Summary: please run this cell which contains the ```Net``` class.
class Net(nn.Module):
  def __init__(self, in_dim: int, out_dim: int, h_dims: List[int], n_frequencies:int) -> None:
    super().__init__()

    ins = [in_dim + 2 * n_frequencies] + h_dims
    outs = h_dims + [out_dim]
    self.n_frequencies = n_frequencies

    self.layers = nn.ModuleList([
        nn.Sequential(nn.Linear(in_d, out_d), nn.LeakyReLU()) for in_d, out_d in zip(ins, outs)
    ])
    self.top = nn.Sequential(nn.Linear(out_dim, out_dim))
  
  def time_encoder(self, t: torch.Tensor) -> torch.Tensor:
    freq = 2 * torch.arange(self.n_frequencies, device=t.device) * torch.pi
    t = freq * t[..., None]
    return torch.cat((t.cos(), t.sin()), dim=-1)
    
  def forward(self, t: torch.Tensor, x: torch.Tensor) -> torch.Tensor:
    t = self.time_encoder(t)
    x = torch.cat((x, t), dim=-1)
    
    for l in self.layers:
      x = l(x)
    return self.top(x)
```

## Create dataset

For this notebook we consider a 2D-dataset and we choose a standard dataset: the _half-moon dataset_. This defines the distribution $q(x_1)$ over the  $\mathbb{R}^2$ data space, _i.e._ the data points are described by two coordinates $x = (a, b)$. We sample a large number ```n_points``` of data points from $q(x_1)$ to build the dataset and we plot them. In addition, in this work we consider a simple gaussian _prior distribution_:
$$
p(x) = \mathcal{N}(x|0, I)
$$

```{python}
#| colab: {base_uri: 'https://localhost:8080/', height: 265}
def get_data(dataset: str, n_points: int) -> np.ndarray:
  if dataset == "moons":
    data, _ = make_moons(n_points, noise=0.15)
  elif dataset == "swiss":
    data, _ = make_swiss_roll(n_points, noise=0.25)
    data = data[:, [0, 2]] / 10.0
  return StandardScaler().fit_transform(data)


n_points = 10_000
DATASET = "swiss"
data = get_data(DATASET, n_points)

%matplotlib inline
plt.hist2d(data[:, 0], data[:, 1], bins=128)
plt.show()
```

We then transform the data points into a ```pytorch.utils.data.DataLoader``` to facilitate data handling. We set the ```batch_size``` to $512$.

```{python}
batch_size = 2048
dataset = torch.from_numpy(data).float()
dataset = dataset.to(device)
dataset = TensorDataset(dataset) 
dataloader = DataLoader(dataset, batch_size=batch_size)
```

## Training:

We train the neural network using the ```AdamW``` optimizer (Adam optimizer with _weight decay_) with a _learning rate_ $lr = 10^{-3}$ during $N = 500$ epochs:

```{python}
#| colab: {base_uri: 'https://localhost:8080/'}
def get_model(name: str):
    if name == "vp":
      return VPDiffusionFlowMatching()
    elif name == "ve":
      return VEDiffusionFlowMatching()
    if name == "ot":
      return OTFlowMatching()

MODEL = "ot"
model = get_model(MODEL)
net = Net(2, 2, [512]*5, 10).to(device)
v_t = CondVF(net)    

losses = [] 
# configure optimizer
optimizer = torch.optim.Adam(v_t.parameters(), lr=1e-3)
n_epochs = 5000
    
for epoch in tqdm(range(n_epochs), ncols=88):
    for batch in dataloader:
      x_1 = batch[0]
      # compute loss 
      loss = model.loss(v_t, x_1)
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()
      losses += [loss.detach()]
```

## Sampling:

To sample $\hat{x}_{1} \sim q(x_1)$ we first sample $x_0 \sim p(x)$, _i.e._ we draw samples from the _prior distribution_, and then apply the ```decode``` function from the ```CondVF``` class. This step may take several minutes depending on the ```n_samples``` chosen.

```{python}
# Sampling
n_samples = 10_000
with torch.no_grad():
    x_0 = torch.randn(n_samples, 2, device=device)
    x_1_hat = v_t.decode(x_0)
```

Last, we plot the samples $\hat{x}_1$ distribution:

```{python}
#| colab: {base_uri: 'https://localhost:8080/', height: 266}
%matplotlib inline
import matplotlib.pyplot as plt
x_1_hat = x_1_hat.cpu().numpy()
plt.hist2d(x_1_hat[:, 0], x_1_hat[:, 1], bins=164)
plt.show()
```

```{python}
# Sampling
N_SAMPLES = 10_000
N_STEPS = 100
t_steps = torch.linspace(0, 1, N_STEPS, device=device)
with torch.no_grad():
    x_t = [torch.randn(n_samples, 2, device=device)]
    for t in range(len(t_steps)-1):
      x_t += [v_t.decode_t0_t1(x_t[-1], t_steps[t], t_steps[t+1])]

# pad predictions
x_t = [x_t[0]]*10 + x_t + [x_t[-1]] * 10

x_t_numpy = np.array([x.detach().cpu().numpy() for x in x_t])
filename = f"{DATASET}_{MODEL}_{N_SAMPLES}_{N_STEPS}.npy"
np.save(filename, x_t_numpy)
```

