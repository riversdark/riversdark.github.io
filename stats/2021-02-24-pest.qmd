---
title: "The Pest: A primer on Bayesian workflow"
description: "A whirlwind tour on Bayesian model building, workflow, model criticism and robust inference."
date: 2021-02-24
categories: ["numpyro", "Bayesian"]
jupyter: jax
image: fig/pest.png
draft: true
---

This post is based on [the tutorial](https://github.com/jgabry/stancon2018helsinki_intro) given by Jonah Gabry at Stancon 2018, and that's where the data can be found. I ported the tutorial to `NumPyro` because I found it a great introduction to Bayesian modeling and workflow.

In this post we'll cover many different modeling techniques, including

-   Poisson model for discrete data
-   Negative Binomial model for overdispersed data
-   hierarchical modeling
-   additive multi-factor modeling
-   time series modeling with linear autoregressive process
-   time series modeling with non-linear non-parametric Gaussian process

The purpose of this post is to get familiar with the Bayesian model expansion process, and the use of data visualisation for model building.  There are many nuances in Bayesian modeling that we won't get into, especially the MCMC inference and its result examination. Essentially, we focus on the statistical modeling part while leaving out the details of statistical computation.

## The problem

New York, as its great reputation merits, has cockroaches everywhere, especially in the dark corners of the apartment compounds. Recently, some residents have been complaining about the omnipresence of the pest; the property manager intends to solve the problem efficiently and effectively, by deploying traps in the apartments. To establish the relationship between the number of traps to set up, and the number of complaints received, an experiment has been carried out:

-   At the beginning of each month, a pest inspector randomly places a number of bait stations throughout the building, without knowledge of the current cockroach levels in the building
-   At the end of the month, the manager records the total number of cockroach complaints in that building.

With this data, we'd like to model how complaints change as the number of traps changes. Since the number of traps installed and the expense for dealing with extra costs both contribute to the final maintenance cost, we'd like to choose the number of traps installed to minimise the cost.

Let's import the packages first.

``` {python}
%config InlineBackend.figure_format = 'retina'
import matplotlib.pyplot as plt
import arviz as az
import pandas as pd
import seaborn as sns
sns.set_theme(palette='Set2')
colors = sns.color_palette()

import numpyro
numpyro.set_host_device_count(4)

import jax.numpy as jnp
from jax import random
from jax.config import config; config.update("jax_enable_x64", True)
rng = random.PRNGKey(123)
```

## The data

The data provided to us is in a file called `pest.csv`.  Let's load the data and see what the structure is:

``` {python}
pest_link = 'data/pest.csv'
pest_data = pd.read_csv(pest_link, index_col=0)
pest_data.index = pest_data.index - 1
pest_data.columns
```

We have access to the following fields:

-   `complaints`: Number of complaints per building per month
-   `building_id`: The unique building identifier
-   `traps`: The number of traps used per month per building
-   `date`: The date at which the number of complaints are recorded
-   `live_in_super`: An indicator for whether the building has a live-in superintendent
-   `age_of_building`: The age of the building
-   `total_sq_foot`: The total square footage of the building
-   `average_tenant_age`: The average age of the tenants per building
-   `monthly_average_rent`: The average monthly rent per building
-   `floors`: The number of floors per building

First, let's see how many buildings we have data for:

``` {python}
print(pest_data.building_id.unique())
```

And make some plots of the raw data. First look at the variable we are trying to model, the `complaints`. We need to pass the `order` keyword to the count plot function because we want to plot [the zero counts](https://stackoverflow.com/questions/45352766/python-seaborn-plotting-frequencies-with-zero-values/45359713#45359713) in the data.

``` {python}
sns.countplot(data=pest_data,
              x='complaints',
              color=colors[0],
              order = range(pest_data.complaints.max()+1));
```

And a scatter plot between the predictor and the outcome. because both `traps` and `complaints` are integers, we need to jitter them a bit so that they don't overlap.

``` {python}
pest_data['traps_jitter'] = pest_data['traps'] + random.normal(rng, [len(pest_data)]) * 0.3
pest_data['complaints_jitter'] = pest_data['complaints'] + random.normal(rng, [len(pest_data)]) * 0.3
sns.scatterplot(data=pest_data, x='traps_jitter', y='complaints_jitter');
```

Just looking at the scatter plot, it's not immediately obvious how these two variables might be related, and there is certainly a large amount of variation. Without extra information, it might be difficult to do predictions of one variable using the other.

The first question we'll look at is just whether the number of `complaints` per building per month is associated with the number of `traps` per building per month, ignoring the temporal and across-building variation (we'll come back to those sources of variation later in the document). How should we model the number of complaints?

## Modeling count data: Poisson distribution

We only have some rudimentary information about what we should expect.  The number of complaints should be, naturally, natural numbers, and the Poisson distribution is the common distribution for modeling integer valued variables, so that's what we'll start with.

Having decided to use a Poisson model for the outcome variable, we'll next decide how to relate the predictors to the parameters of this distribution. The Poisson distribution has only one parameter, which decides both the mean and the variance of the outcome. More specifically, both the mean and variance are equal to the rate parameter. When doing statistical modeling, we choose a distribution for some of its properties, then we examine whether the data match the other properties of the distribution. If not, we then consider how to improve the fit by revising the model.

Because the rate parameter is always positive, We will build a linear model with support on $\mathbb{R}$, then use the exponential transformation to limit the support to $\mathbb{R}^{+}$. For each observation we have

$$
\begin{aligned}
\textrm{complaints}_{n} & \sim \textrm{Poisson}(\lambda_{n}) \\
\lambda_{n} & = \exp{(\eta_{n})} \\
\eta_{n} &= \alpha + \beta  \textrm{traps}_{n}
\end{aligned}
$$

One thing to notice is that although both the predictor, `traps`, and the outcome, `compalints`, can only be integers, this fact is not immediately obvious to our model. Here we are modeling `complaints` as a Poisson variable, so it's indeed interpreted as an integer value, but for `traps`, as far as our model is concerned, it's just a variable with any possible value on $\mathbb{R}$. For this reason we can transform `traps`, or any other variable we might include in our model later, so that the interpretation and computation are easier; but if we transform the outcome variable `complaints`, we need to consider whether the chosen probability distribution is still appropriate for the transformed variable.

We don't have much prior information about $\alpha$ and $\beta$, but we suspect that the intercept should be positive while the slope negative, so we give them some weakly information priors. To see if the priors actually make sense, we will have to see what kind of predictions the model makes, later, in the prior predictive checks.

Another thing worth noticing is that although to transform a variable $\eta$ on $\mathbb{R}$ to another variable $\lambda$ on $\mathbb{R}^+$, the [exponential transformation](https://en.wikipedia.org/wiki/Exponential_function#:~:text=In%20mathematics%2C%20an%20exponential%20function,x%20occurs%20as%20an%20exponent) is a natural choice, its not without its problems. We can take a look at the shape of the exponential function to notice that when $\eta$ is negative, a huge amount of change is $\eta$ leads to little change in $\lambda$ while when it's positive, a small change might leads to (exponentially, as the name indicates) huge jumps in $\lambda$ values.

![](https://upload.wikimedia.org/wikipedia/commons/thumb/c/c6/Exp.svg/320px-Exp.svg.png)

This can be a problem for us because it's very easy to have numerical overflows when using this transformation, when the rate parameter is too big. For this reason we'll try to transform the predictor variables to a reasonable range, and this also helps to ease the choice of priors.

### Model

We'll choose some vaguely reasonable priors for the parameters. Technically the priors are also part of the model but because this part of the model is closely related to how we implement the model and also how we transform the data, we keep them separated from the other part of the model.

$$
\begin{aligned}
\alpha & \sim \textrm{Normal}(1, 2) \\
\beta & \sim \textrm{Normal}(-1, 2) \\
\end{aligned}
$$

Is the prior flexible enough? We can check it through the prior predictive checks.

``` {python}
from numpyro import sample, plate, deterministic
from numpyro.distributions import Normal, Poisson

def sp(traps):
    """Simple Poisson Regression model"""

    α = sample('α', Normal(1, 2))
    β = sample('β', Normal(-1, 2))

    with plate("N", size=len(traps)):
        λ = deterministic('λ', jnp.exp(α + β * traps))
        return sample('y', Poisson(λ))
```

### Prepare data

we then collect the data used in modeling into separate dicts. Because the predictors `X`, and the outcomes, `y`, are treated fundamentally differently by our models, we also kept them separated. To avoid numerical overflow we have rescaled `traps`.

``` {python}
sp_X = {
    'traps': pest_data.traps.values / 10,
}
sp_y = {
    'y': pest_data.complaints.values
}
```

To test our model we'll also create a dict of fake predictors. The fake
predictors would have the same value range as the real data, but more
uniformly distributed to avoid complicate geometries in observed data.
Also the number of fake observations does not necessarily equal that of
the real data; as a matter of fact we have deliberately generated more
fake predictors than the real data so that we'll have enough data to
guarantee the model works well, and if it doesn't, we can rule out (to
a certain degree of course) the problem being with our data, and focus
our attention on debugging the model.

``` {python}
fobs_shape = [1000]
sp_fX = {
    'traps': random.randint(rng, fobs_shape, 0, 15) / 10,
}
```

And we'll also prepare a dict of fake model parameters for model
soundness check.

``` {python}
sp_params = {
    'α': jnp.ones(1),
    'β': -1 * jnp.ones(1)
}
```

### Prior predictive check

Because the model is generative, after specifying the model, we can immediately generate samples from it.

``` {python}
from numpyro.infer import Predictive

def get_prior_pred(model, X, n=1000):
    """Get prior predictive samples from a generative model.
    model: the unconditional model
    X: a dict of predictors
    n: the number of prior samples
    Returned: a dict of model prior samples
    """
    rng_key = random.PRNGKey(10)
    return Predictive(model, num_samples=n)(rng_key, **X)
```

``` {python}
sp_prior_pred = get_prior_pred(sp, sp_X)
sp_prior_pred.keys()
```

First let's see what the model predictions are like.

``` {python}
sns.kdeplot(x=sp_prior_pred['α'].flatten());
```

``` {python}
sns.kdeplot(x=sp_prior_pred['β'].flatten());
```

There is no surprise here, the KDE estimations of $\alpha$ and $\beta$
correspond well to their prior distributions. And the outcome

``` {python}
sp_prior_pred['y'].max()
```

Looks like we have some rare but very large predictions, and this is in
line with our earlier concern: when using the exponential
transformation, numerical stability can be a potential problem, which,
luckily, by rescaling the data we have been able to avoid.

To get a good sense of how the bulk of the data behaves, we'll clip the
visualisation of the prediction for the outcome variable.

``` {python}
def clipped_kde(d, cut=50, color=None):
    """KDE plot clipped between [0, cut].
    """
    d = d.flatten()
    sns.kdeplot(x=d[d <= cut], color=color)
    plt.xlim(left=0, right=cut);

clipped_kde(sp_prior_pred['y'])
```

We can see that most of the predictions are zero or close to zero, with
the counts grow exponentially smaller as the number gets larger. But in
general these predictions look reasonable, so we'll move on to model
inference.

### Fit model to fake data

However, before doing inference on real data, we should check that our
model works well with simulated data. We'll simulate data from the
model and then check that we can recover the parameter values used in
the simulation.

Here we'll fix the parameter values and draw one sample from the model,
then feed the sample back to the model, to see if we can recover the
fixed parameters that has been used to generate that exact data. I
collected the code for running inference and predictions into helper
functions so we can reuse them later.

``` {python}
from numpyro.handlers import condition
from numpyro.infer import MCMC, NUTS

def run_mcmc(model, X, Y, nwarmup=2000, nsample=2000, nchain=2, no_det=True):
    """Run MCMC inference with a NUTS kernel, and print summary statistics.
    model: a generative model, not conditioned on data.
    X: a dict of predictors.
    Y: a dict of outcome variables.
    no_det: Boolean, exclude deterministic variables in summary"""
    kernel = NUTS(condition(model, data=Y))
    rng_key = random.PRNGKey(10)
    mcmc = MCMC(kernel, num_warmup=nwarmup, num_samples=nsample, num_chains=nchain, progress_bar=False)
    mcmc.run(rng_key, **X, extra_fields=('potential_energy', 'diverging'))
    mcmc.print_summary(exclude_deterministic=no_det)
    return mcmc
```

``` {python}
def recover_params(model, params, X, sites=['y'], **mc_params):
    """Check parameter recovery of a generative model using MCMC.
    model: the model
    params: the fixed params
    X: a dict of predictors
    sites: the list of outcome variable names
    """
    rng_key = random.PRNGKey(10)
    pred = Predictive(
        model,
        posterior_samples=params,
        batch_ndims=0,
        return_sites=sites)(rng_key, **X)
    return run_mcmc(model, X, pred, **mc_params)
```

Now see how the inference worked

``` {python}
recover_params(sp, sp_params, sp_fX);
```

The results seem excellent. We can now move on to the real data.

### Fit model to real data

we now fit the model to the actual observed data.

``` {python}
sp_mcmc = run_mcmc(sp, sp_X, sp_y)
```

The inference result is not as accurate as with the fake data, but this
is to be expected, because with the fake data we have way more
observations. But as we suspected, the number of bait stations set in a
building is negatively associated with the number of complaints that
were made in the following month.

### Posterior predictive check

After fitting the model, we can make some predictions. we can make
predictions for the original data, or for some new data; but since here
we are trying to compare the model prediction with observed data, we'll
make predictions using the original predictors. Later when we want to
make predictions for the future, we can also use new predictors.

``` {python}
def get_posterior_pred(model, X, mcmc, sites=None):
    """Get posterior predictive samples from a generative model.
    model: the unconditioned model
    X: a dict of predictors
    mcmc: the inferred MCMC object
    sites: the list of outcome variables names
    """
    rng_key = random.PRNGKey(10)
    return Predictive(model, posterior_samples=mcmc.get_samples(), return_sites=sites)(rng_key, **X)

sp_posterior_pred = get_posterior_pred(sp, sp_X, sp_mcmc, sites=['y'])
```

### Density overlay

Plot the observations against the predictions

``` {python}
def overlay_kde(y, y_pred, n=200):
    """
    Overlay KDE plots. observed vs. prediction
    """
    for i in range(n):
        sns.kdeplot(x=y_pred[i], color=colors[0])
        sns.kdeplot(x=y, color=colors[1])
        plt.xlim(left=0);

overlay_kde(sp_y['y'], sp_posterior_pred['y'])
```

Comparing the densities of the observed and predicted data is the most
basic posterior predictive check. If the overall shape of the
distributions does not match, it makes no sense to go into more details
and into the subgroups and special characteristics of the data. Here we
can see that the model under predicts smaller values, over-predict
middle range values, and again under-predict large values. How can we
improve on this? The most obvious choice, is to add more predictors, so
as to increase the variation in the data.

### Specific statistics

Apart from looking at the overall distribution of the prediction, we can
also look at specific statistics **of the overall distribution**. For
example, the proportion of zeros in the data

``` {python}
prop_zero = lambda t: (t == 0).sum(-1) / t.shape[-1]
prop_large = lambda t: (t > 10).sum(-1) / t.shape[-1]
```

``` {python}
def ppc_stat(fn, true, pred):
    """
    Plot certain statistic, true value vs. model prediction
    fn: function to calculate the statistic
    """
    plt.hist(x=fn(pred), color=colors[0])
    plt.axvline(fn(true), color=colors[1], lw=4)
    plt.xlim(left=0)
    plt.title("True vs Predicted");
```

``` {python}
ppc_stat(prop_zero, sp_y['y'], sp_posterior_pred['y'])
```

The plot above shows the observed proportion of zeros (thick vertical
line) and a histogram of the proportion of zeros in each of the
simulated data sets. It is clear that the model does not capture this
feature of the data well at all.

Or we can look at the proportion of large values

``` {python}
ppc_stat(prop_large, sp_y['y'], sp_posterior_pred['y'])
```

As we saw from the density overlay, the proportion of large values is
also underestimated. So our model is underestimating the tails of the
distribution. Helpfully further modeling improvements might help on
this.

## Expanding the model: multiple predictors

The simple Poisson model does not work very well so as an instinctive follow up, we might consider adding more predictor variables. Moreover, the manager told us that they expect there be a number of other reasons that one building might have more roach complaints than another. We'll incorporate this information into our model.

Currently, our model's mean parameter is a rate of complaints per 30 days, but we're modeling a process that occurs over an area as well as over time. We have the square footage of each building, so if we add that information into the model, we can interpret our parameters as a rate of complaints per square foot per 30 days. A quick test shows us that there appears to be a relationship between the square footage of the building and the number of complaints received:

``` {python}
sns.lmplot(x='total_sq_foot', y='complaints', data=pest_data);
```

Using the property manager's intuition, we also include an extra pieces of information we know about the buildings - whether there is a live in super or not - into the model.

``` {python}
sns.violinplot(x='live_in_super', y='complaints', data=pest_data);
```

``` {python}
sns.scatterplot(x='traps_jitter', y='complaints_jitter', hue='live_in_super', data=pest_data);
```

### Model

And the model now becomes

$$
\begin{aligned}
\textrm{complaints}_{n} & \sim \textrm{Poisson}(\textrm{sq\_foot}_b\lambda_{n}) \\
\lambda_{n} & = \exp{(\eta_{n} )} \\
\eta_{n} &= \alpha + \beta  \textrm{traps}_{n}  + \beta_{super}  \textrm{live_in_super}_{b}
\end{aligned}
$$

The term $\text{sq_foot}$ is called an exposure term. If we log the
term, we can put it in $\eta_{n}$:

$$
\begin{aligned}
\textrm{complaints}_{n} & \sim \textrm{Poisson}(\lambda_{n}) \\
\lambda_{n} & = \exp{(\eta_{n} )} \\
\eta_{n} &= \alpha + \beta  \textrm{traps}_{n}  + \beta_{super}  \textrm{live_in_super}_{b} + \textrm{log_sq_foot}_b
\end{aligned}
$$

We'll use the same priors for $\alpha$ and $\beta$ as before, as for
$\beta_{\text{super}}$, we suspect that having a super might reduce that
amount of complaints so

$$
\begin{aligned}
\alpha & \sim \textrm{Normal}(1, 2) \\
\beta & \sim \textrm{Normal}(-1, 2) \\
\beta_{\text{super}} & \sim \textrm{Normal}(-0.5, 2) \\
\end{aligned}
$$

Now we can write the model

``` {python}
def mp(traps, live_in_super, log_sq_foot):
    """Multiple Poisson Regression"""

    α = sample('α', Normal(1, 2))
    β = sample('β', Normal(-1, 2))
    β_super = sample('β_super', Normal(-0.5, 2))

    with plate("N", size=len(traps)):
        λ = deterministic('λ', jnp.exp(α + β * traps + β_super * live_in_super + log_sq_foot))
        return sample('y', Poisson(λ))
```

### prepare the data

Notice that the predictors have changed but we're still using the same
outcome variable. As a matter of fact, we'll be using the same outcome
variable throughout this study.

``` {python}
pest_data['log_sq_foot'] = jnp.log(pest_data.total_sq_foot.values / 1e4)
```

We also demean the data for computational stability.

``` {python}
mp_X = {
    'traps': pest_data.traps.values / 10,
    'live_in_super': pest_data.live_in_super.values - pest_data.live_in_super.mean(),
    'log_sq_foot': pest_data.log_sq_foot.values - pest_data.log_sq_foot.mean(),
}
```

and some fake data

``` {python}
mp_fX = {
    'traps': random.randint(rng, fobs_shape, 0, 15) / 10,
    'live_in_super': random.bernoulli(rng, 0.5, fobs_shape) - 0.5,
    'log_sq_foot': random.normal(rng, fobs_shape) * 0.4
}
```

and fake parameters

``` {python}
mp_params = {
    'α': jnp.ones(1),
    'β': -1 * jnp.ones(1),
    'β_super': jnp.array([-0.5])
}
```

### Prior predictive check

Like before, we can generate samples from the prior joint distribution.

``` {python}
mp_prior_pred = get_prior_pred(mp, mp_X)
```

``` {python}
mp_prior_pred['y'].max()
```

``` {python}
clipped_kde(mp_prior_pred['y'])
```

Judging by the prior prediction, this model does not seem to vary much
from the previous one, but the predictions are still reasonable, though
with a much wider range than the observed data (and this is what want).

### Fit model to fake data

Now see how the inference goes

``` {python}
recover_params(mp, mp_params, mp_fX);
```

We've recovered the parameters sufficiently well, so we are now
confident to fit the real data.

### Fit model to real data

Now let's use the real data and explore the fit.

``` {python}
mp_mcmc = run_mcmc(mp, mp_X, sp_y)
```

### Posterior predictive check

With the simple Poisson model, the predictions don't match the observed
data. Let's see if we have made any improvement.

``` {python}
mp_posterior_pred = get_posterior_pred(mp, mp_X, mp_mcmc, sites=['y'])
```

### Density overlay

``` {python}
overlay_kde(sp_y['y'], mp_posterior_pred['y'])
```

Looks much better than before because the real data lay somewhat in
between the predictions. Still, the disparity is clear.

### Specific statistics

And prediction for small and large values

``` {python}
ppc_stat(prop_zero, sp_y['y'], mp_posterior_pred['y'])
```

``` {python}
ppc_stat(prop_large, sp_y['y'], mp_posterior_pred['y'])
```

Compared with the simple Poisson model, we have definitely made some
progress, both with the overall distribution and the specific
statistics; but it looks like we are still under-estimating the variance
in the data. We can keep adding new predictors to increase variation in
the data(if we can find more predictors), but at this stage we might
just consider choosing another distribution that allows more variance
than Poisson. We turn next to the negative binomial distribution.

## Modeling overdispersed data: Negative Binomial distribution

When modeling the data using a Poisson distribution, we saw that the
model didn't fit as well to the data as we would like. In particular
the model underpredicted low and high numbers of complaints, and
overpredicted the medium number of complaints. This is one indication of
over-dispersion, that is, the observed data have more variance than the
distribution used to model them. A Poisson model doesn't fit
over-dispersed count data very well because the same parameter $\lambda$
controls both the expected counts and the variance of these counts. The
natural alternative to this is the negative binomial model.

NumPyro doesn't have a native implementation of the Negative Binomial
distribution, but we have two alternatives: the Tensorflow
implementation that can be imported into NumPyro, or another
parameterisation known as the Gamma-Poisson distribution.

``` {python}
from numpyro.contrib.tfp.distributions import NegativeBinomial
```

The Poisson distribution can be interpreted as the number of successes
happening at a certain rate, across certain time period and certain
space, as we are using here; it can also be interpreted as the sum of
Bernoulli trials with a constant probability of success. Similarly, the
Negative Binomial distribution (in NumPyro, because there are many
different variations) can be interpreted as the sum of Bernoulli trials,
but as the sum of successes we have before a certain number of failures
is reached. Like the Poisson, it also has support on all the natural
numbers. In this interpretation, the Negative Binomial distribution has
two parameters, the failure count $k$, and the probability of success,
$p$. And for $\text{NegativeBinomial}(k, p)$, the mean and variance are

$$
\begin{aligned}
\mathbb{m} &= \frac{k}{1-p} - k = \frac{pk}{1-p} \\
\mathbb{v} &=  \frac{p}{(1-p)}  \frac{k}{(1-p)} = \frac{pk}{(1-p)^2}
\end{aligned}
$$

Although this parameterisation is easy to understand, it's not very
interpretable for us when using it to model count data, and with a
linear model for the rate parameter, like we did for the Poisson model.
One parameterisation that meets such demands is to augment the Poisson
distribution with a **concentration** parameter, $\phi$, as
$\text{Neg-Binomial}(y; \lambda, \phi)$, so that we can separate the
modeling of mean and variance. Using this parameterisation, the mean and
variance are

$$
\begin{aligned}
\mathbb{m}  &= \lambda \\
\mathbb{v}  &= \lambda + \lambda^2 / \phi .
\end{aligned}
$$

As we can see, the mean is the same as before, but now the variance is
always bigger than the mean: this is how we meet the over-dispersion
demand. As $\phi$ gets larger, the distribution becomes more
concentrated, the term $\lambda^2 / \phi$ approaches zero, the variance
of the negative-binomial approaches $\lambda$, and the Negative Binomial
gets closer and closer to the Poisson.

Combining the two representation, we can model the intensity parameter
as before, and along with the new concentration parameter, we can obtain
the parameterisation used in the TFP implementation

$$
\begin{aligned}
k  &= \phi \\
p  &= \frac{\lambda }{  \lambda + \phi} \\
\end{aligned}
$$

The Tensorflow implementation of Negative Binomial also accepts the log
odds parameterisation, which is more convenient for us, and also
numerically more stable:

$$
\log \frac{p}{1-p} = \log \lambda - \log \phi
$$

``` {python}
k, p = 3.5, 0.3
nb = NegativeBinomial(total_count=k, probs=p)

print(nb.mean(), k*p / (1-p), nb.variance(), k*p / (1-p)**2)
```

There is another implementation of the Negative Binomial distribution,
natively implemented in NumPyro, that directly augments the Poisson
distribution, and is called the Gamma-Poisson distribution. Our goal is
to use a distribution to model the count data, with over-dispersion, the
natural choice, if one Poisson distribution with constant $\lambda$ is
too limited, is to use a family of Poissons with varying $\lambda$.
Since $\lambda$ is a positive real number, we may consider putting a
Gamma distribution on it, and this leads to the compound Gamma-Poisson
distribution. It turns out that this is another parameterisation of the
Negative Binomial distribution.

The Gamma-Poisson distribution implementation in NumPyro is
parameterised with a concentration parameter `c` and a rate
parameter `r`, but have nothing to do with the concentration
and rate parameters before. With this parameterisation the parameters
are defined as

$$
\begin{aligned}
c &= k \\
r &= \frac{1-p}{p}\\
\end{aligned}
$$

Or, using the rate-concentration parameterisation

$$
\begin{aligned}
c  &= \phi \\
r  &= \frac{\phi}{\lambda} \\
\end{aligned}
$$

and the mean and the variance can aslo be calculated

$$
\begin{aligned}
\mathbb{m} &=  \frac{c}{r} \\
\mathbb{v} &=  \frac{ c (1+r) }{r^2}
\end{aligned}
$$

``` {python}
from numpyro.distributions import GammaPoisson

c, r = 3.5, 0.7/0.3
gap = GammaPoisson(c,r)
print(gap.mean, c/r, gap.variance, c*(1+r)/r**2)
```

Another difference with the native NumPyro implementation, is that the
mean and variance are attributes while with the Tensorflow one they are
methods. Because this natively implemented version works better with
other NumPyro functionalities, we'll use this distribution to model our
data.

### Model

using the mathematical manipulation above, we can write the Gamma
Poisson model as:

$$
\begin{aligned}
\text{complaints}_{n} & \sim \text{Gamma-Poisson}(c, r_{n}) \\
c  &= \phi \\
r_{n}  &= \frac{\phi}{\lambda_n} \\
\log \lambda_{n} &= \alpha + \beta  {\rm traps}_{n} + \beta_{\rm super}  {\rm super}_{b} + \text{log\_sq\_foot}_{b}
\end{aligned}
$$

We don't have much information on what $\phi$ should be, apart from
it's positive, we'll put a Log-normal distribution on it.

$$
\begin{aligned}
\alpha & \sim \textrm{Normal}(1, 2) \\
\beta & \sim \textrm{Normal}(-1, 2) \\
\beta_{\text{super}} & \sim \textrm{Normal}(-0.5, 1) \\
\phi & \sim \textrm{LogNormal}(0, 1) \\
\end{aligned}
$$

now translate the mathematical model into code

``` {python}
from numpyro.distributions import LogNormal

def gap(traps, live_in_super, log_sq_foot):
    """Multiple Gamma-Poisson Regression"""

    α = sample('α', Normal(1, 2))
    β = sample('β', Normal(-1, 2))
    β_super = sample('β_super', Normal(-0.5, 1))
    φ = sample('φ', LogNormal(0, 1))

    with plate("N", size=len(traps)):
        λ = deterministic('λ', jnp.exp(α + β * traps + β_super * live_in_super + log_sq_foot))
        return sample('y', GammaPoisson(φ, φ / λ))
```

### Prepare the data

Both the predictors and the outcome are the same as before, we only have
to prepare the fake parameters

``` {python}
gap_params = {
    'α': jnp.ones(1),
    'β': -1 * jnp.ones(1),
    'β_super': jnp.array([-0.5]),
    'φ': jnp.ones(1)
}
```

### Prior predictive check

Like before, we can generate samples from the prior joint distribution.
The predictors used are the same as the previous multiple Poisson model.

``` {python}
gap_prior_pred = get_prior_pred(gap, mp_X)
```

The Log-Normal prior for $\phi$

``` {python}
sns.histplot(gap_prior_pred['φ']);
```

``` {python}
gap_prior_pred['y'].max()
```

The prediction now has a much wider range.

``` {python}
clipped_kde(gap_prior_pred['y']);
```

### Fit the model to fake data

``` {python}
recover_params(gap, gap_params, mp_fX);
```

The results seems okay. Move on.

### Fit model the real data

Now let's use the real data and explore the fit.

``` {python}
gap_mcmc = run_mcmc(gap, mp_X, sp_y)
```

### Posterior predictive check

``` {python}
gap_posterior_pred = get_posterior_pred(gap, mp_X, gap_mcmc, sites=['y'])
```

### Density overlay

``` {python}
overlay_kde(sp_y['y'], gap_posterior_pred['y'])
```

The prediction looks much better now, the real data is well in the range
of model predictions. We have captured the overall distribution of the
data quite well.

### Specific statistics

And let's also look at the proportion of small and large values.

``` {python}
ppc_stat(prop_zero, sp_y['y'], gap_posterior_pred['y'])
```

Now our model captures the proportion of zeros almost perfectly. It's
true that there is still large uncertainty, the predicted proportion of
zeros has a wide range, but we have limited data and there is only that
much we can learn from it. To improve our model prediction, we can
either find more observations, or find more useful predictors for our
existing observations.

``` {python}
ppc_stat(prop_large, sp_y['y'], gap_posterior_pred['y'])
```

The proportion of larger predicted values also seems reasonable. So
using the Gamma-Poisson distribution with increased dispersion indeed
fits the data much better.

Now that we have captured the overall distribution of the observations,
what next? We should look closer to the predictions to see if the model
prediction has other problems.

One general way to do this is using the residual plots.

### posterior prediction by groups

We haven't used the fact that the data are clustered by building and
time yet. A posterior predictive check might elucidate whether it would
be a good idea to add these information into the model. Here we are
using information that is known to us but unknown to the model, to see
if the model prediction generalises well.

``` {python}
def plot_grouped_mean(obs, pred, group, nrow, ncol, figsize):
    """Plot the grouped prediction against observation"""
    fig, axes = plt.subplots(nrow, ncol, sharex=False, sharey=True, figsize=figsize)
    ids = group.unique()
    ids.sort()

    for i, id in enumerate(ids):
        ax = axes.flatten()[i]
        mask = group == id
        y = obs[mask]
        p = pred.T[mask.values]
        assert len(y) == p.shape[0]
        y_mean = y.mean()
        p_mean = p.mean(0)
        sns.histplot(x=p_mean, color=colors[0], ax=ax)
        ax.axvline(y_mean, color=colors[1], lw=2)
        ax.set_title(id)
        plt.tight_layout()
```

Let's look at the model prediction, grouped by the buildings and by the
time.

``` {python}
plot_grouped_mean(pest_data['complaints'], gap_posterior_pred['y'], pest_data['building_id'], 2, 5, (12,4))
```

``` {python}
plot_grouped_mean(pest_data['complaints'], gap_posterior_pred['y'], pest_data['month'], 3, 4, (12,6))
```

We're getting plausible predictions for most building means but some
are estimated better than others and some have larger uncertainties than
we might expect. And most of the predictions grouped by time are a bit
off. If we explicitly model the variation across buildings we may be
able to get much better estimates.

## Going hierarchical: Modeling varying intercepts for each building

We have been able to fit the overall distribution of the observed data
quite well, but as the model criticism in the previous part has shown,
there is a large amount of uncertainty around our predictions, and some
per group predictions are a bit off. Because we know that our data have
some natural groupings, for example, some observations are from the same
building, and some observations are from the same time period; these
grouping information can help us reduce the variance, and make better
predictions, so we'd also like to incorporate these information into
our model.

First let's focus on the grouping by buildings. Since the complaints
made by the inhabitants are closely related to the conditions of each
building, we'd expect there be some differences among the buildings
that are contributing to the complaints we received. As such, it's
natural to expect the intercepts of the linear model for each building
be different, because the intercept captures what are not modeled by the
other included variables; we'll start by adding a different intercept
parameter, $\pi$, to each building, and since we expect these intercepts
be different but nevertheless related (they are all tenant buildings
after all), we'll also put a common prior on these intercepts. This
leads us to the hierarchical intercept model.

Because of this formulation, during the inference process we will not
only learn about the parameters for each building, but the
hyperparameters that regulate the parameters. These formulation also
helps us to make predictions for new buildings, if we later have
information about these new buildings.

Since we have already a common intercept $\alpha$, the second intercept
will be modeled as a **variation** to the first and thus has zero mean.

$$
\begin{aligned}
\log \lambda_{n} &= \alpha + \pi_b + \beta  {\rm traps}_{n} + \beta_{\rm super}  {\rm super}_{b} + \text{log_sq_foot}_{b} \\
\pi &\sim \text{Normal}(0, \sigma) \\
\end{aligned}
$$

The subscription in $\pi_b$ is the index. Some of our predictors vary
only by building, so we can rewrite the above model more efficiently
like so:

$$
\begin{aligned}
\log \lambda_{n}  &= \alpha + \pi_b + \beta  {\rm traps}_{n} + \text{log_sq_foot}_b\\
\pi &\sim \text{Normal}(\beta_{\text{super}}   \text{super}_b , \sigma) \\
\end{aligned}
$$

We have more information at the building level as well, like the average
age of the residents, the average age of the buildings, and the average
per-apartment monthly rent so we can add that data into a matrix called
`building_data`, which will have one row per building and
four columns:

-   `live_in_super`
-   `age_of_building`
-   `average_tentant_age`
-   `monthly_average_rent`

### Model

And the final model is

$$
\begin{aligned}
\text{complaints}_{n} & \sim \text{Gamma-Poisson}(c, r_{n}) \\
c  &= \phi \\
r_{n}  &= \frac{\phi}{\lambda_n} \\
\log \lambda_{n}  &= \alpha + \pi_b + \beta  {\rm traps}_{n} + \text{log_sq_foot}_b\\
\pi &\sim \text{Normal}(\texttt{building_data}  \zeta, \sigma) \\
\end{aligned}
$$

Now we have to choose a prior for $\sigma$. Because $\pi$ is the
deviation of each building from the common intercept, and from the
earlier models we know that the intercept seems to be between 2-3, which
prior should we give $\sigma$? Looking at the shape of the Log-Normal
distribution

![](https://upload.wikimedia.org/wikipedia/commons/thumb/a/ae/PDF-log_normal_distributions.svg/240px-PDF-log_normal_distributions.svg.png)

Because we are only modeling the deviation, and each building shouldn't
deviate from the common mean very much, we can choose a prior with large
mass on small values but also a heavy tail for safety sake. Both
$\sigma=0.5$ and $\sigma=1$ have heavy tails but let's go with
$\sigma=1$ because it has heavier tail.

$$
\begin{aligned}
\alpha & \sim \textrm{Normal}(1, 2) \\
\beta & \sim \textrm{Normal}(-1, 2) \\
\phi & \sim \textrm{LogNormal}(0, 1) \\
\zeta & \sim \textrm{Normal}(0, 1) \\
\sigma & \sim \textrm{LogNormal}(0, 1) \\
\end{aligned}
$$

``` {python}
def vi(traps, log_sq_foot, b, building_data):
    """Multiple Gamma-Poisson Regression
    With Varying Interceps, centred parameterisation
    """

    α = sample('α', Normal(1, 2))
    β = sample('β', Normal(-1, 2))
    φ = sample('φ', LogNormal(0, 1))
    σ = sample('σ', LogNormal(0, 1))

    with plate('N_predictors', size=building_data.shape[1]):
        ζ = sample('ζ', Normal(0, 1))

    with plate('N_buildings', size=building_data.shape[0]):
        π = sample('π', Normal(building_data @ ζ, σ))

    with plate("N", size=len(traps)):
        λ = deterministic('λ', jnp.exp(α + π[b] + β * traps + log_sq_foot))
        return sample('y', GammaPoisson(φ, φ / λ))
```

### Prepare the data

We need the building index for each observation

``` {python}
pest_data['b'] = pest_data.building_id.astype('category').cat.codes
```

The building level predictors

``` {python}
bd = pest_data[['b', 'live_in_super', 'age_of_building', 'average_tenant_age', 'monthly_average_rent']]
bd = bd.drop_duplicates().sort_values(by='b').set_index(keys='b')

bd = (bd - bd.mean())
bd['age_of_building'] = bd['age_of_building'] / 10
bd['average_tenant_age'] = bd['average_tenant_age'] / 10
bd['monthly_average_rent'] = bd['monthly_average_rent'] / 1000

print(bd)
```

All the predictors

``` {python}
vi_X = {
    'traps': pest_data.traps.values / 10,
    'log_sq_foot': pest_data.log_sq_foot.values - pest_data.log_sq_foot.mean(),
    'b': pest_data.b.values,
    'building_data': bd.values
}
```

Fake predictors

``` {python}
vi_fX = dict(
    traps = random.randint(rng, fobs_shape, 0, 15) / 10,
    log_sq_foot = random.normal(rng, fobs_shape) * 0.4,
    b = random.randint(rng, fobs_shape, 0, 10),
    building_data = random.normal(rng, [10, 4])
)
```

Fake parameters

``` {python}
vi_params = {
    'α': jnp.ones(1),
    'β': -1 * jnp.ones(1),
    'φ': jnp.ones(1),
    'σ': jnp.ones(1),
    'ζ': jnp.ones(4)
}
```

### prior predictive check

Like before, we can generate samples from the prior joint distribution.
But we can also check the prediction for the hierarchical groups to
better understand the model.

``` {python}
vi_prior_pred = get_prior_pred(vi, vi_X)
vi_prior_pred.keys()
```

``` {python}
vi_prior_pred['λ'].max()
```

This doesn't look very good: there are extremely big and thus
unreasonable rate parameters. With values this big, it's way beyond the
region of reasonableness, and we are likely to have numerical problems.
let's look at the prior distribution.

``` {python}
clipped_kde(vi_prior_pred['λ']);
```

We can see that most of the $\lambda$s are small, but we have a very
long tail.

``` {python}
print(vi_prior_pred['y'].min(), vi_prior_pred['y'].max())
```

``` {python}
print(jnp.sum(vi_prior_pred['y'] < 0))
```

As we can see here, we're having numerical overflow. Let's see where
the problem is happening

``` {python}
print(jnp.where(jnp.any(vi_prior_pred['y'] < 0, axis=1)))
```

we have
`jnp.where(jnp.any(vi_prior_pred['y'] < 0, axis=1))[0].shape[0]`{.python}
{{{results(`4`)}}} samples having overflow

``` {python}
for id in jnp.where(jnp.any(vi_prior_pred['y'] < 0, axis=1))[0]:
    print('sample ', id)
    for k, v in vi_prior_pred.items():
        if k != 'y' and k != 'λ':
            print(k, v[id])
```

As we can see, either we are having too big $\sigma$ which in turn leads
to extreme values in $\pi$, or we are having too small $\phi$. We should
probably revise the model to restraint them a little bit. But for the
moment we'll just carry on. But still, we can already see that as our
model gets more complex, it's important to check prior predictive
distributions to catch problems early on.

### Fit the model to fake data

Now we fix the parameters

``` {python}
recover_params(vi, vi_params, vi_fX);
```

We are not recovering $\zeta$ very well, but this is to be expected,
because at the building level we only have 10 data points, yet we have 4
$\zeta$s, 10 $\pi$s, and one $\sigma$. We simply don't have enough data
to make good inference.

### Fit the real data

Now let's use the real data and explore the fit.

``` {python}
vi_mcmc = run_mcmc(vi, vi_X, sp_y)
```

HMC is having divergent transitions, we might need to have a closer look
at the inference result. Having divergent transitions (even only one
divergence) means that the posterior hasn't been properly explored, the
posterior might be biased; and thus in consequence everything we do with
the inference result could be put into doubt!

``` {python}
vi_post_samples = vi_mcmc.get_samples()
for k, v in vi_post_samples.items():
    print(k, v.shape)
```

``` {python}
fig, axes = plt.subplots(1, 2, figsize=(8, 4))
sns.histplot(vi_prior_pred['σ'], ax=axes[0])
sns.histplot(vi_post_samples['σ'], ax=axes[1]);
```

we have learned a lot from the model, but we don't know if the result
can be trusted.

### Diagnostics

We get a bunch of divergent transitions, which is an indication that there may be regions of the posterior that have not been explored by the Markov chains. Divergences are discussed in more detail in the original Stancon tutorial, and the [bayesplot](https://mc-stan.org/bayesplot/index.html) package in R, the [ArviZ](https://arviz-devs.github.io/arviz/) package in Python all have many functions to visualise and analyse the divergences, and Michael Betancourt's [paper on Hamiltonian Monte Carlo in general](https://arxiv.org/abs/1701.02434) and [diagnosing biased inference case study in specific](https://betanalpha.github.io/assets/case_studies/divergences_and_bias.html) contain many great insights on this topic.

Unfortunate for the didactic purpose and fortunate from a modeling point
of view, we only have two divergent cases so the plots are not very
helpful here, we'll just print out the divergent transitions and see if
we can spot any problem.

``` {python}
print(jnp.where(vi_mcmc.get_extra_fields()['diverging'] == True))
```

``` {python}
for id in jnp.where(vi_mcmc.get_extra_fields()['diverging'] == True)[0]:
    print('sample ', id)
    for k, v in vi_post_samples.items():
        if k in ['σ']:
            print(k, v[id])
```

Again, we are seeing similar problems as encountered earlier, in prior
predictive check. Here we are having extremely small $\sigma$ which is
problematic.

### Model, revised

We have located two problems with our model:

1.  the prior for $\sigma$ is too diffuse and we are having unreasonably large predictions
2.  when the sampled value for $\phi$ is too small, the posterior tends to degenerate.

To solve these problems, we can put some more restrictive priors on
these parameters, or we can reparameterise the model so that even when
the parameters take extreme values, the posterior geometry is still
smooth enough to not cause trouble for the HMC algorithm. We first try
the first solution.

change to the model prior

$$
\begin{aligned}
\phi & \sim \textrm{LogNormal}(0, 0.5) \\
\sigma & \sim \textrm{LogNormal}(0, 0.5) \\
\end{aligned}
$$

``` {python}
def vi2(traps, log_sq_foot, b, building_data):
    """Multiple Gamma-Poisson Regression
    With Varying Interceps, more restrictive prior
    """

    α = sample('α', Normal(1, 2))
    β = sample('β', Normal(-1, 2))
    φ = sample('φ', LogNormal(0, 0.5))
    σ = sample('σ', LogNormal(0, 0.5))

    with plate('N_predictors', size=building_data.shape[1]):
        ζ = sample('ζ', Normal(0, 1))

    with plate('N_buildings', size=building_data.shape[0]):
        π = sample('π', Normal(building_data @ ζ, σ))

    with plate("N", size=len(traps)):
        λ = deterministic('λ', jnp.exp(α + π[b] + β * traps + log_sq_foot))
        return sample('y', GammaPoisson(φ, φ / λ))
```

### Prepare the data

There is no change in the data.

### Prior predictive check

``` {python}
vi2_prior_pred = get_prior_pred(vi2, vi_X)
print(vi2_prior_pred['y'].min(), vi2_prior_pred['y'].max())
```

Great, now there is no longer any numerical overflow

``` {python}
clipped_kde(vi2_prior_pred['y']);
```

### Model, reparameterised

The second solution is to sample an auxiliary variable $\pi_0$ from the
standard normal first, then transform it to $\pi$, so that the sampling
of $\sigma$ doesn't affect the sampling of $\pi$

$$
\begin{aligned}
\text{complaints}_{n} & \sim \text{Gamma-Poisson}(c, r_{n}) \\
c  &= \phi \\
r_{n}  &= \frac{\phi}{\lambda_n} \\
\log \lambda_{n}  &= \alpha + \pi_b + \beta  {\rm traps}_{n} + \text{log\_sq\_foot}_b\\
\pi &= \texttt{building\_data}  \zeta + \sigma \pi_0 \\
\end{aligned}
$$

and the priors

$$
\begin{aligned}
\alpha & \sim \textrm{Normal}(1, 2) \\
\beta & \sim \textrm{Normal}(-1, 2) \\
\phi & \sim \textrm{LogNormal}(0, 0.5) \\
\zeta & \sim \textrm{Normal}(0, 1) \\
\pi_0 & \sim \textrm{Normal}(0, 1) \\
\sigma & \sim \textrm{LogNormal}(0, 0.5) \\
\end{aligned}
$$

``` {python}
def vincp(traps, log_sq_foot, b, building_data):
    """Multiple Gamma-Poisson Regression
    With Varying Interceps, noncentred parameterisation
    """

    α = sample('α', Normal(1, 2))
    β = sample('β', Normal(-1, 2))
    φ = sample('φ', LogNormal(0, 0.5))
    σ = sample('σ', LogNormal(0, 0.5))

    with plate('N_predictors', size=building_data.shape[1]):
        ζ = sample('ζ', Normal(0, 1))

    with plate('N_buildings', size=building_data.shape[0]):
        π0 = sample('π0', Normal(0, 1))

    π = deterministic('π', building_data @ ζ + σ * π0)

    with plate("N", size=len(traps)):
        λ = deterministic('λ', jnp.exp(α + π[b] + β * traps + log_sq_foot))
        return sample('y', GammaPoisson(φ, φ / λ))
```

### Prepare the data

We have to change the fixed parameters used for model checking. With our
models getting more complex, even assigning fake values to parameters
gets more and more difficult because the fixed parameters can take us to
some strange regions of the parameter space and beats the purpose of
model checking

``` {python}
vincp_params = {
    'α': jnp.ones(1),
    'β': -1 * jnp.ones(1),
    'φ': jnp.ones(1),
    'σ': jnp.ones(1),
    'ζ': 0.5 * jnp.ones(4),
    'π0': 0.2 * jnp.ones(10),
}
```

### Prior predictive check

``` {python}
vincp_prior_pred = get_prior_pred(vincp, vi_X)
print(vincp_prior_pred['y'].min(), vincp_prior_pred['y'].max())
```

``` {python}
clipped_kde(vincp_prior_pred['y']);
```

### Fit the model to fake data

``` {python}
recover_params(vincp, vincp_params, vi_fX);
```

Now the model is taking significantly longer to fit.

### Fit model to the real data

``` {python}
vincp_mcmc = run_mcmc(vincp, vi_X, sp_y)
```

Great, no more divergence!

### Posterior predictive check

``` {python}
vincp_posterior_pred = get_posterior_pred(vincp, vi_X, vincp_mcmc, sites=['y'])
```

### Density overlay

``` {python}
overlay_kde(sp_y['y'], vincp_posterior_pred['y'])
```

The prediction looks much better now, the real data is well in the range
of model predictions. We have captured the overall distribution of the
data quite well.

### Specific statistics

``` {python}
ppc_stat(prop_zero, sp_y['y'], vincp_posterior_pred['y'])
```

``` {python}
ppc_stat(prop_large, sp_y['y'], vincp_posterior_pred['y'])
```

the upper tail seem to be a little overestimated, but not much.

### posterior prediction by groups

``` {python}
plot_grouped_mean(pest_data['complaints'], vincp_posterior_pred['y'], pest_data['building_id'], 2, 5, (12,4))
```

We weren't doing terribly bad with the building-specific means before,
but now they are all well-captured by our model. The model is also able
to do a decent job estimating within-building variability.

## Extending the hierarchy: Modeling varying slopes for each building

Perhaps if the building information affects the model intercept, it
would also affect the coefficient on `traps`. We can add this
to our model and observe the fit. Like the varying intercepts, the
varying slopes will also use a non-centered parameterisation, and we are
giving the corresponding variables the same prior.

### Model

The observational model

$$
\begin{aligned}
\text{complaints}_{n} & \sim \text{Gamma-Poisson}(c, r_{n}) \\
c  &= \phi \\
r_{n}  &= \frac{\phi}{\lambda_n} \\
\log \lambda_{n}  &= \alpha + \pi_b + (\beta + \kappa_b)  {\rm traps}_{n} + \text{log\_sq\_foot}_b\\
\pi &= \texttt{building\_data}  \zeta + \sigma \pi_0 \\
\kappa &= \texttt{building\_data}  \gamma + \tau \kappa_0 \\
\end{aligned}
$$
the priors

$$
\begin{aligned}
\alpha & \sim \textrm{Normal}(1, 2) \\
\beta & \sim \textrm{Normal}(-1, 2) \\
\phi & \sim \textrm{LogNormal}(0, 0.5) \\
\zeta & \sim \textrm{Normal}(0, 1) \\
\gamma & \sim \textrm{Normal}(0, 1) \\
\pi_0 & \sim \textrm{Normal}(0, 1) \\
\kappa_0 & \sim \textrm{Normal}(0, 1) \\
\sigma & \sim \textrm{LogNormal}(0, 0.5) \\
\tau & \sim \textrm{LogNormal}(0, 0.5) \\
\end{aligned}
$$

``` {python}
def vs(traps, log_sq_foot, b, building_data):
    """Multiple Gamma-Poisson Regression
    With group level Varying Interceps and Varying Slopes, Non-centered parameterisation.
    """

    α = sample('α', Normal(1, 2))
    β = sample('β', Normal(-1, 2))
    φ = sample('φ', LogNormal(0, 0.5))
    σ = sample('σ', LogNormal(0, 0.5))
    τ = sample('τ', LogNormal(0, 0.5))

    with plate('N_predictors', size=building_data.shape[1]):
        ζ = sample('ζ', Normal(0, 1))
        γ = sample('γ', Normal(0, 1))

    with plate('N_buildings', size=building_data.shape[0]):
        π0 = sample('π0', Normal(0, 1))
        κ0 = sample('κ0', Normal(0, 1))

    π = deterministic('π', building_data @ ζ + σ * π0)
    κ = deterministic('κ', building_data @ γ + τ * κ0)

    with plate("N", size=len(traps)):
        λ = deterministic('λ', jnp.exp(α + π[b] + (β + κ[b]) * traps + log_sq_foot))
        return sample('y', GammaPoisson(φ, φ / λ))
```

### Prepare the data

As the model gets more and more complex, we are having more and more
model parameters. As such, with limited data, it will be more and more
difficult to get accurate inference. It's always good to think harder
about the model assumptions, use better priors, but sometimes we just
need more data to make more reliable inference. Lucky for us, we've
gotten some new data that extends the number of time points for which we
have observations for each building. So from now on we'll use the
extended data set.

``` {python}
pest_more_link = 'data/more_pest.csv'
```

``` {python}
pest_more = pd.read_csv(pest_more_link, index_col=0)
pest_more['m'] = pest_more.m - 1
pest_more['b'] = pest_more.b - 1
pest_more.index = pest_more.index - 1
```

Like before, we extract the building level data and put them in a
separate data set.

``` {python}
bd_more = (pest_more[['live_in_super', 'age_of_building', 'average_tenant_age', 'monthly_average_rent', 'b']]
           .drop_duplicates()
           .sort_values(by='b')
           .set_index('b', drop=True))
bd_more = bd_more - bd_more.mean()

print(bd_more)
```

Again collect the predictor variables into a dict.

``` {python}
vs_X = {
    'traps': pest_more.traps.values / 10,
    'log_sq_foot': pest_more.log_sq_foot.values - pest_more.log_sq_foot.mean(),
    'b': pest_more.b.values,
    'building_data': bd_more.values
}
```

We can use the previous fake dataset, because the model inputs are the
same. And the fixed parameters

``` {python}
vs_params = {
    'α': jnp.ones(1),
    'β': -1 * jnp.ones(1),
    'φ': jnp.ones(1),
    'σ': jnp.ones(1),
    'τ': jnp.ones(1),
    'ζ': 0.5 * jnp.ones(4),
    'γ': 0.5 * jnp.ones(4),
    'π0': 0.2 * jnp.ones(10),
    'κ0': 0.2 * jnp.ones(10),
}
```

Finally, our outcome variable now has new data

``` {python}
vs_y = {
    'y': pest_more.complaints.values
}
```

### prior predictive check

``` {python}
vs_prior_pred = get_prior_pred(vs, vi_X)
print(vs_prior_pred['y'].min(), vs_prior_pred['y'].max())
```

seems reasonable.

``` {python}
clipped_kde(vs_prior_pred['y']);
```

### Fit the model to fake data

``` {python}
recover_params(vs, vs_params, vi_fX);
```

### Fit model to the real data

``` {python}
vs_mcmc = run_mcmc(vs, vs_X, vs_y)
```

### Posterior predictive check

``` {python}
vs_posterior_pred = get_posterior_pred(vs, vs_X, vs_mcmc, sites=['y'])
print(vs_posterior_pred['y'].min(), vs_posterior_pred['y'].max())
```

### Density overlay

``` {python}
overlay_kde(vs_y['y'], vs_posterior_pred['y'])
```

At this stage, it's difficult to look at the overall distribution and
see if our model has performed better. We just use the density overlay
to check that that our model is working as expected.

Nevertheless, we can see that compared to earlier models, the range of
outcome values is growing bigger.

### Specific statistics

``` {python}
ppc_stat(prop_zero, vs_y['y'], vs_posterior_pred['y'])
```

The model seems to be underestimating zeros a little bit, but the
variance has reduced significantly.

``` {python}
ppc_stat(prop_large, vs_y['y'], vs_posterior_pred['y'])
```

### posterior prediction by groups

``` {python}
plot_grouped_mean(pest_more['complaints'], vs_posterior_pred['y'], pest_more['b'], 2, 5, (12,4))
```

compared to the early model, now the prediction variance is further more
reduced. But since we are also using more data, it's not sure which
part has contributed more, the data or the model.

We haven't model the monthly effect yet, but let's see how the monthly
prediction looks like

``` {python}
plot_grouped_mean(pest_more['complaints'], vs_posterior_pred['y'], pest_more['m'], 6, 6, (12,12))
```

This doesn't look good, some of the predictions are quite far off. This
implies that by adding the time information, we might be able to further
improve our model.

## Adding new hierarchies: Modeling varying intercepts for each month

After looking at the grouping among buildings, and observing that this
grouping information indeed helps to improve the model fit, we move on
to another grouping variable, the different months at which we observed
the data. Specifically, we'll add a **second variation to the
intercept** to the linear model

$$
\log \lambda_{n}  = \alpha + \pi_b + \theta_m + (\beta + \kappa_b)  {\rm traps}_{n} + \text{log_sq_foot}_b\\
$$

Previously, when modeling the varying intercepts (and also the varying
slopes) for each building, we have assumed that all the varying
intercepts are drawn independently from the same prior distribution

$$ 
\pi \sim \text{Normal}(\texttt{building_data}  \zeta, \sigma)
$$

We made this independence assumption because we have no knowledge on how
the buildings might be related to each other; however, when grouping the
observations by when they are observed, we know immediately that there
are some relationships among these groups, because these groups follow a
specific order: the observations from February follows those from
January, and thos from March follow those from February, etc. As such,
we have some certain structure among the variables, and to make use of
this structure, we'll model them jointly with some **structured
prior**.

In this section we'll model the varying monthly effects on the
intercept with an AR(1) model, in which the conditional mean of each
variable depends on the previous variable. Considering the months are
sequentially ordered, this seems like a natural choice:

$$
\theta_m \sim \text{Normal}(\rho  \theta_{m-1}, \upsilon)
$$

Using the error term representation, which represents the conditional
distribution as a rescaling and translation of the standard normal
distribution (the error term), and which is more commonly used and more
convenient for mathematical manipulation in time series models, the
AR(1) model can be represented as

$$
\begin{aligned}
\theta_m &= \rho  \theta_{m-1} +\upsilon \epsilon_m \\
\epsilon_m &\sim \text{Normal}(0, 1) \\
\end{aligned}
$$

In the AR(1) model we have two (hyper)parameters, the correlation
coefficient $\rho$ and the conditional standard deviation $\upsilon$.
And we also need to assign a prior for $m_1$ since there is no previous
variable to condition on. Generally speaking we can assign any prior to
these variables, but in practice with AR process, we often assume that
the process is stationary, and this assumption applies extra constraints
on the variables.

By stationarity, all variables in the AR process have the same marginal
distribution. For normally distributed variables this means

$$
\begin{aligned}
\text{Var}(\theta_m) &= \text{Var}(\theta_{m-1}) = \dots = \text{Var}(\theta_{1}) \\
\mathbb{E}(\theta_m) &= \mathbb{E}(\theta_{m-1}) = \dots = \mathbb{E}(\theta_{1}) \\
\end{aligned}
$$
so

$$
\begin{aligned}
\text{Var}(\theta_m) &= \text{Var}(\rho \theta_{m-1} + \upsilon \epsilon_m)  \\
&= \text{Var}(\rho \theta_{m-1}) + \text{Var}(\upsilon \epsilon_m) \\
&= \rho^2 \text{Var}( \theta_{m})  + \upsilon^2 \\
\end{aligned}
$$

Clearly we need $\rho \in (-1, 1)$ for this to make sense. We then
obtain the marginal variance:

$$
\text{Var}(\theta_m) = \frac{\upsilon^2}{1 - \rho^2}
$$

For the mean of $\theta_m$ things are a bit simpler:

$$
\begin{aligned}
\mathbb{E}(\theta_m) &= \mathbb{E}(\rho  \theta_{m-1} + \upsilon \epsilon_t) \\
&= \mathbb{E}(\rho  \theta_{m-1}) + \mathbb{E}(\upsilon \epsilon_t) \\
&= \mathbb{E}(\rho  \theta_{m-1})  + \upsilon 0\\
&= \rho \mathbb{E}(\theta_m)
\end{aligned}
$$

which for $\rho \neq 1$ yields $\mathbb{E}(\theta_{m}) = 0$.

We now have the marginal distribution for $\theta_{m}$, which, in our
case, we will use for $\theta_1$:

$$
\theta_1 \sim \text{Normal}\left(0, \frac{\upsilon}{\sqrt{1 - \rho^2}}\right) \\
$$

We need $\rho \in (-1, 1)$, but there are no obvious distributions with
support on $(-1,1)$, we can define a raw variable on $[0,1]$ and then
transform it to $[-1,1]$. Specifically,

$$
\rho_0 \in [0, 1] \\
\rho = 2 \times \rho_0 - 1
$$

Given the description of the process above, it seems like there could be
either positive or negative associations between the months, but when
observing more complaints this month, we probably would also expect more
complaints next month, so there should be a bit more weight placed on
positive $\rho$s: we'll put an informative prior that pushes the
parameter $\rho$ towards 0.5.

Since the noise terms for all time periods are independent and normally
distributed, we'll also model the standard deviation $\upsilon$
separately. Apart from being positive, we don't have any information on
where it should be, so we'll put a general prior on it.

### Model

Now the complete model. Note that because Python uses zero based
indexing, the months begin at zero and ends at 11.

$$
\begin{aligned}
\text{complaints}_{n} & \sim \text{Gamma-Poisson}(c, r_{n}) \\
c  &= \phi \\
r_{n}  &= \frac{\phi}{\lambda_n} \\
\log \lambda_{n}  &= \alpha + \pi_b + \theta_m + (\beta + \kappa_b)  {\rm traps}_{n} + \text{log_sq_foot}_b\\
\pi &= \texttt{building_data}  \zeta + \sigma \pi_0 \\
\kappa &= \texttt{building_data}  \gamma + \tau \kappa_0 \\
\theta_m &= \rho  \theta_{m-1} + \upsilon \epsilon_m,  m = 1, ..., 11 \\
\theta_0 &\sim \left( \upsilon / \sqrt{1 - \rho^2} \right) \epsilon_0 \\
\rho &= 2 \times \rho_0 - 1 \\
\end{aligned}
$$

The priors

$$
\begin{aligned}
\alpha & \sim \textrm{Normal}(1, 2) \\
\beta & \sim \textrm{Normal}(-1, 2) \\
\phi & \sim \textrm{LogNormal}(0, 0.5) \\
\zeta & \sim \textrm{Normal}(0, 1) \\
\gamma & \sim \textrm{Normal}(0, 1) \\
\sigma & \sim \textrm{LogNormal}(0, 0.5) \\
\tau & \sim \textrm{LogNormal}(0, 0.5) \\
\pi_0 & \sim \textrm{Normal}(0, 1) \\
\kappa_0 & \sim \textrm{Normal}(0, 1) \\
\epsilon &\sim \text{Normal}(0, 1) \\
\rho_0 &\sim \text{Beta}(3, 1)\\
\upsilon & \sim \textrm{LogNormal}(0, 0.5) \\
\end{aligned}
$$

``` {python}
from numpyro.distributions import Beta, HalfNormal
from numpyro.contrib.control_flow import scan

def ar(traps, log_sq_foot, b, building_data, m, M):
    """Hierarchical Gamma-Poisson Regression
    Building level: Varying intercepts and Varying Slopes, Non-centered parameterisation.
    Monthly level: varying intercepts. AR(1), Non-centered parameterisation.
    """

    α = sample('α', Normal(1, 2))
    β = sample('β', Normal(-1, 2))
    φ = sample('φ', LogNormal(0, 0.5))
    σ = sample('σ', LogNormal(0, 0.5))
    τ = sample('τ', LogNormal(0, 0.5))

    with plate('N_predictors', size=building_data.shape[1]):
        ζ = sample('ζ', Normal(0, 1))
        γ = sample('γ', Normal(0, 1))

    with plate('N_buildings', size=building_data.shape[0]):
        π0 = sample('π0', Normal(0, 1))
        κ0 = sample('κ0', Normal(0, 1))

    π = deterministic('π', building_data @ ζ + σ * π0)
    κ = deterministic('κ', building_data @ γ + τ * κ0)

    # AR(1) process, non-centered
    ν = sample('ν', LogNormal(0, 0.5))
    ρ0 = sample('ρ0', Beta(10, 5))
    ρ = deterministic('ρ', 2 * ρ0 - 1)

    with plate('N_months', size=M):
        ε = sample('ε', Normal(0, 1))

    θ0 = ν * ε[0] / jnp.sqrt(1 - ρ**2)

    def f(p, e):
        c = ρ * p + ν * e
        return c, c

    _, θ_rest = scan(f, θ0, ε[1:], length=M-1)
    θ = deterministic('θ', jnp.squeeze(jnp.concatenate((θ0[None], θ_rest))))

    # observations
    with plate("N", size=len(traps)):
        λ = deterministic('λ', jnp.exp(
            α + π[b] + θ[m] + (β + κ[b]) * traps + log_sq_foot))
        return sample('y', GammaPoisson(φ, φ / λ))
```

### Prepare data

Compared to the previous model, we now also have the month indicator. We
only have data for 36 months, but we can feed model to 42 months (6
months more) so we can also look at the future prediction of the time
series model.

``` {python}
ar_X = {
    'traps': pest_more.traps.values / 10,
    'log_sq_foot': pest_more.log_sq_foot.values - pest_more.log_sq_foot.mean(),
    'b': pest_more.b.values,
    'm': pest_more.m.values,
    'building_data': bd_more.values,
    'M': 42
}
```

and update the fake data accordingly

``` {python}
ar_fX = dict(
    traps=random.randint(rng, fobs_shape, 0, 15) / 10,
    log_sq_foot=random.normal(rng, fobs_shape) * 0.4,
    b=random.randint(rng, fobs_shape, 0, 10),
    m=random.randint(rng, fobs_shape, 0, 12),
    building_data=random.normal(rng, [10, 4]),
    M=42
)
```

and the fixed parameters. With the model getting more and more complex,
it's more and more difficult to have reasonable fake parameters in the
reasonable region.

``` {python}
ar_params = {
    'α': 2 * jnp.ones(1),
    'β': -1 * jnp.ones(1),
    'φ': jnp.ones(1),
    'σ': 0.5 * jnp.ones(1),
    'τ': 0.5 * jnp.ones(1),
    'ζ': 0.2 * random.normal(rng, [4]),
    'γ': 0.2 * random.normal(rng, [4]),
    'π0': 0.2 * random.normal(rng, [10]),
    'κ0': 0.2 * random.normal(rng, [10]),
    'ε': 0.5 * random.normal(rng, [42]),
    'ρ': 0.5 * jnp.ones(1),
    'ν': 0.5 * jnp.ones(1),
}
```

The outcome is the same as before.

### prior predictive check

``` {python}
ar_prior_pred = get_prior_pred(ar, ar_X)
print(ar_prior_pred['λ'].min(), ar_prior_pred['λ'].max())
print(ar_prior_pred['y'].min(), ar_prior_pred['y'].max())
```

we can see that as the model grows more and more complex, so is the
corresponding parameter space; and consequentially, the outcome range
has also significantly grown.

``` {python}
clipped_kde(ar_prior_pred['y']);
```

``` {python}
sns.histplot(ar_prior_pred['ν']);
```

we can also look at the prior for the AR(1) model

``` {python}
fig, ax = plt.subplots(figsize=(16, 8))
for i in random.randint(rng, [10], 0, 1000):
    ax.plot(ar_prior_pred['θ'][i])
```

we can see that a priori $\theta$ is centered at zero and their is no
strong correlation between them. Also the plots are quite wiggly.

### Fit the model to fake data

``` {python}
recover_params(ar, ar_params, ar_fX);
```

### Fit model to the real data {#fit-model-to-the-real-data}

``` {python}
ar_mcmc = run_mcmc(ar, ar_X, vs_y)
```

The inference result is quite different from earlier models

### Posterior predictive check

``` {python}
ar_posterior_pred = get_posterior_pred(ar, ar_X, ar_mcmc)
```

### Density overlay

``` {python}
overlay_kde(vs_y['y'], ar_posterior_pred['y'])
```

### Specific statistics

``` {python}
ppc_stat(prop_zero, vs_y['y'], ar_posterior_pred['y'])
```

``` {python}
ppc_stat(prop_large, vs_y['y'], ar_posterior_pred['y'])
```

### AR(1) model

``` {python}
fig, ax = plt.subplots(figsize=(16, 8))
plt.axvline(35, color=colors[1], alpha=0.5, lw=4)
for i in random.randint(rng, [200], 0, 1000):
    ax.plot(ar_posterior_pred['θ'][i], color=colors[0], alpha=0.2)
```

Now we can clearly see the monthly trend, although there are many abrupt
changes in the trend. We can also see that the periods where we don't
have data, after 36th month, the prediction has much bigger uncertainty.
This isn't very realistic, we'd imagine that if something changes over
time, it should more realistically change gradually. This is why we
might consider modeling the time series with a Gaussian process.

### posterior prediction by groups

``` {python}
plot_grouped_mean(pest_more['complaints'], ar_posterior_pred['y'], pest_more['b'], 2, 5, (12,4))
```

We have been able to further reduce the prediction variance.

``` {python}
plot_grouped_mean(pest_more['complaints'], ar_posterior_pred['y'], pest_more['m'], 6, 6, (12,12))
```

and the monthly predictions are also quite good. Except for the first
month, because with an AR(1) model, there is no previous period to
condition on, the first period is not really modeled in the AR(1)
process, only as an independent normal distribution.

Now our model is also doing quite well with the prediction among
different groups.

## Going nonparametric: Modeling monthly effects with Gaussian process

We have been using the AR(1) process to model the relationship between
the varying monthly effect. The AR(1) process, like other Markov models,
using a number of parameters to directly model the local relationship
between neighbouring variables. But we can also use a non-parametric
method, the Gaussian process, to jointly model the relationship between
all the variables. In fact, as shown below, the AR(1) process can be
seen to be implicitly defining a specific Gaussian Process. Using a
Gaussian Process, we can model the structured prior with more
flexibility.

### Joint density for AR(1) process

With Gaussian process, any finite number of variables are modeled as a
multivariate normal distribution. To see its relation with the AR(1)
process, we first derive the joint distribution of the variables defined
by the AR(1) process:

From before we know that the marginal distributions for the monthly
effects are

$$
\begin{aligned}
\theta_1 & \sim \text{Normal}\left(0, \frac{\upsilon}{\sqrt{1 - \rho^2}}\right) \\
\theta_m & \sim \text{Normal}\left(\rho  \theta_{m-1}, \upsilon\right) \; \forall t > 1
\end{aligned}
$$

Rewriting our process in terms of the errors will make the derivation of
the joint distribution clearer

$$
\begin{aligned}
\theta_m & = \rho  \theta_{m-1} + \upsilon\epsilon_t  \\
\theta_1 & \sim \frac{\upsilon}{\sqrt{1 - \rho^2}} \epsilon_1 \\
\epsilon_t & \sim \text{Normal}\left(0, 1\right) \; t = 1, 2, \dots,
\end{aligned}
$$

We can see that our first term $\theta_1$ is normally distributed, and
the subsequent terms are sums of normal random variables, and thus are
also normally distributed; we can further show that jointly the vector,
$\theta$, the $t$-th element being the scalar variable $\theta_m$, is
multivariate normal distributed:

Take $t=3$ as an example. The first element is fairly straightforward,
because it mirrors our earlier parameterization of the AR(1) prior. The
only difference is that we're explicitly adding the last two terms of
$\epsilon$ into the equation so we can use matrix algebra for our
transformation.

$$ 
\theta_1 = \frac{\nu}{\sqrt{1 - \rho^2}} \times \epsilon_1 + 0 \times \epsilon_2 + 0 \times \epsilon_3\\ 
$$

The second element:

$$
\begin{aligned}
\theta_2 & = \rho \theta_1 + \nu\epsilon_2 + 0 \times \epsilon_3  \\
 & = \rho \left(\frac{\nu}{\sqrt{1 - \rho^2}} \times \epsilon_1\right) + \nu\epsilon_2 + 0 \times \epsilon_3  \\
 & = \frac{\rho \nu}{\sqrt{1 - \rho^2}} \times \epsilon_1 + \nu\epsilon_2 + 0 \times \epsilon_3  \\
\end{aligned}
$$

While the third element

$$
\begin{aligned}
\theta_3 & = \rho  \theta_2 + \nu\epsilon_3 \\
 & = \rho \left(\frac{\rho \nu}{\sqrt{1 - \rho^2}} \times \epsilon_1 + \nu\epsilon_2\right) + \nu \epsilon_3  \\
& = \frac{\rho^2 \nu}{\sqrt{1 - \rho^2}} \times \epsilon_1 + \rho  \nu\epsilon_2 +  \nu\epsilon_3  \\
\end{aligned}
$$

Writing this all together:

$$
\begin{aligned}
\theta_1 & = \frac{\nu}{\sqrt{1 - \rho^2}} \times \epsilon_1 + 0 \times \epsilon_2 + 0 \times \epsilon_3\\
\theta_2 & = \frac{\rho \nu}{\sqrt{1 - \rho^2}} \times \epsilon_1 + \nu\epsilon_2 + 0 \times \epsilon_3  \\
\theta_3 & = \frac{\rho^2 \nu}{\sqrt{1 - \rho^2}} \times \epsilon_1 + \rho  \nu\epsilon_2 +  \nu\epsilon_3  \\
\end{aligned}
$$

Now we have been able to represent all the variables, instead of using
the variable from the **previous one period**, but error terms from
**all the previous periods**. A model that model the current variable
with previous error terms is known as a MA (moving average) model, and
from the previous manipulation we can see that an AR(1) model is
equivalent to a MA(∞) model.

In matrix form:

$$
\theta = \begin{bmatrix} \upsilon / \sqrt{1 - \rho^2} & 0 & 0 \\
\rho \upsilon / \sqrt{1 - \rho^2} & \upsilon & 0 \\
\rho^2 \upsilon / \sqrt{1 - \rho^2} & \rho \upsilon  & \upsilon
\end{bmatrix} \times \epsilon
$$

Because the elements in $\epsilon$ are all independent and standard
normal distributed, we know that $\epsilon$ is multivariate-normal
distributed with independent components,
$\epsilon \sim \text{MultiNormal}(0, I_M)$, $I_M$ being the identity
matrix. And let

$$
L = \begin{bmatrix} \upsilon / \sqrt{1 - \rho^2} & 0 & 0 \\
\rho \upsilon / \sqrt{1 - \rho^2} & \upsilon & 0 \\
\rho^2 \upsilon / \sqrt{1 - \rho^2} & \rho \upsilon  & \upsilon
\end{bmatrix}
$$

$\theta = L\times\epsilon$ is also multivariate normal distributed with
$\theta \sim \text{MultiNormal}(0, L L^T)$. So instead of defining the
conditional distributions of each variable separately, we can also
define them like this, as a multivariate normal distribution.

The covariance

$$
\begin{aligned}
L L^T &=
\begin{bmatrix} \upsilon / \sqrt{1 - \rho^2} & 0 & 0 \\
\rho \upsilon / \sqrt{1 - \rho^2} & \upsilon & 0 \\
\rho^2 \upsilon / \sqrt{1 - \rho^2} & \rho \upsilon  & \upsilon
\end{bmatrix} \times
\begin{bmatrix} \upsilon / \sqrt{1 - \rho^2} & \rho \upsilon / \sqrt{1 - \rho^2} & \rho^2 \upsilon / \sqrt{1 - \rho^2} \\
0 & \upsilon & \rho \upsilon \\
0 & 0  & \upsilon
\end{bmatrix} \\
&=
 \begin{bmatrix} \nu / (1 - \rho^2) & \rho  \nu / (1 - \rho^2) &  \rho^2  \nu / (1 - \rho^2)\\
\rho  \nu / (1 - \rho^2)  & \nu / (1 - \rho^2)  & \rho  \nu / (1 - \rho^2) \\
\rho^2 \nu / (1 - \rho^2) & \rho  \nu / (1 - \rho^2) & \nu / (1 - \rho^2)
\end{bmatrix} \\
&=
\left( \frac{\nu}{1 - \rho^2} \right)
 \begin{bmatrix} 1 & \rho  &  \rho^2 \\
\rho  & 1  & \rho  \\
\rho^2 & \rho & 1
\end{bmatrix}
\end{aligned}
$$

This of course generalises to any number of higher dimensions. As before
we can see that all the variables have the same marginal distribution,
and the correlation between neighbouring variables are $\rho$, and
decreases as the distance gets further.

### Cholesky decomposition

Now that we have $\theta \sim \text{MultiNormal}(0, L L^T)$, we can
directly sample from it. But as it turned out, it's more efficient to
sample first $\epsilon$, and then transform $\theta = L\times\epsilon$.
In the above AR(1) process we have already explicitly constructed $L$ so
this is easy, but in a general case when a covariance matrix is given,
we need to decompose $\Sigma$ into $L L^T$ first, this process is called
the **Cholesky decomposition** .

### Extension to Gaussian processes

The prior we defined for $\theta$ is strictly speaking a Gaussian
process. Gaussian process is a stochastic process that is distributed as
jointly multivariate normal for any finite value of $M$ and as we have
seen, the AR(1) process is exactly that. In Gaussian process notation,
we could write the above prior for $\theta$ as:

$$ 
\theta_m \sim \text{GP}\left( 0, K(t | \upsilon,\rho) \right) 
$$

In which $K(t | \upsilon,\rho)$ is called a kernel function and it
defines the covariance matrix of the process over the domain $t$,

$$ 
\text{Cov}(\theta_m,\theta_{m+h}) = k(m, m+h | \upsilon, \rho). 
$$

And now we have a Gaussian process, with its covariance matrix
parameterised by two hyperparameters, $\upsilon$ and $\rho$. What if we
want to use a different covariance? It turns out there are many
elementwise kernel functions

$$
K_{[m,m+h]} = k(m, m+h | \theta)
$$

where $\theta$ are the hyperparameters, that can be applied to construct
covariance matrices, a very popular one, and one we are going to use
here, is called the **exponentiated quadratic function**:

$$
\begin{aligned}
k(m, m+h | \theta) & = \chi^2  \exp \left( - \dfrac{1}{2\ell^2} ((m+h) - m)^2 \right) \\
& = \chi^2  \exp \left( - \dfrac{h^2}{2\ell^2} \right).
\end{aligned}
$$

The exponentiated quadratic kernel has two hyperparameters, $\chi$, the
marginal standard deviation of the stochastic process, which determines
the scale of the outputand and length-scale $\ell$, which determines the
smoothness of the process. The larger $\chi$, the larger the output of
the process; the larger $\ell$, the smoother the process.

To implement the Gaussian process in practice we'll make use of the
Cholsky decomposition we have introduced above. We first draw

$$
\epsilon \sim \text{MultiNormal}(0, I_M)
$$

and because the elements are independent we can simply draw multiple
samples from one dimensional standard normal; then we can construct the
covariance matrix $\Sigma$ using the squared exponential kernel,
decomposed it to get $L$, finally we transform it to get

$$
\theta = L\times\epsilon \sim \text{MultiNormal}(0, \Sigma)
$$

As we can see here, although Gaussian process is a non-parametric
process defined on infinite dimensions but in practice, we are always
dealing with finite dimensions and all we need to do is some linear
algebra.

### Model

Now we can write out the complete model using GP for the time series
effects.

$$
\begin{aligned}
\text{complaints}_{n} & \sim \text{Gamma-Poisson}(c, r_{n}) \\
c  &= \phi \\
r_{n}  &= \frac{\phi}{\lambda_n} \\
\log \lambda_{n}  &= \alpha + \pi_b + \theta_m + (\beta + \kappa_b)  {\rm traps}_{n} + \text{log\_sq\_foot}_b\\
\pi &= \texttt{building\_data}  \zeta + \sigma \pi_0 \\
\kappa &= \texttt{building\_data}  \gamma + \tau \kappa_0 \\
\theta &= L \times \epsilon \\
L &= \text{Cholsky}(\Sigma) \\
\Sigma &= \text{K}({\rm m}, \chi, \ell) \\
\end{aligned}
$$

$\text{K}$ is the exponentiated square kernel function

``` {python}
# exponentiated square kernel with diagonal noise term
def K(X, Z, χ, ell, jitter=1.0e-8):
    deltaXsq = jnp.power((X[:, None] - Z) / ell, 2.0)
    k = jnp.power(χ, 2.0) * jnp.exp(-0.5 * deltaXsq)
    k += jitter * jnp.eye(X.shape[0])
    return k
```

We are still faced with the problem of choosing priors for $\chi$ and
$\ell$. The prior for $\chi$ should not be too difficult, because it
controls the output scale, and since we have been modeling the intercept
quite extensively, we have a good idea of what scale it should take, so
we'll put a weakly informative prior on it. As for $\ell$, the sensible
prior well depend upon the distance between the neighbouring points, and
more specifically, how do we determine the numerical distance between
the months? we can simply use the indicators from 0 to 35, but this is a
relatively large range, and besides, if the minimal distance in our data
is 1, there is no way our model can learn any length scale that's less
than one because we don't have information over that limit.

Now the prior model

$$
\begin{aligned}
\alpha & \sim \textrm{Normal}(1, 2) \\
\beta & \sim \textrm{Normal}(-1, 2) \\
\phi & \sim \textrm{LogNormal}(0, 0.5) \\
\zeta & \sim \textrm{Normal}(0, 1) \\
\gamma & \sim \textrm{Normal}(0, 1) \\
\sigma & \sim \textrm{LogNormal}(0, 0.5) \\
\tau & \sim \textrm{LogNormal}(0, 0.5) \\
\pi_0 & \sim \textrm{Normal}(0, 1) \\
\kappa_0 & \sim \textrm{Normal}(0, 1) \\
\epsilon &\sim \text{MultiNormal}(0, I_M) \\
\chi &\sim \textrm{LogNormal}(0, 0.5)\\
\ell & \sim \textrm{Gamma}(10, 2) \\
\end{aligned}
$$

``` {python}
from numpyro.distributions import Gamma
from jax.scipy.linalg import cholesky

def gp(traps, log_sq_foot, b, building_data, m, M):
    """Hierarchical Gamma-Poisson Regression
    Building level: Varying intercepts and Varying Slopes, Non-centered parameterisation.
    Monthly level: varying intercepts. Gaussian process, Non-centered parameterisation.
    """

    α = sample('α', Normal(1, 2))
    β = sample('β', Normal(-1, 2))
    φ = sample('φ', LogNormal(0, 0.5))
    σ = sample('σ', LogNormal(0, 0.5))
    τ = sample('τ', LogNormal(0, 0.5))

    with plate('N_predictors', size=building_data.shape[1]):
        ζ = sample('ζ', Normal(0, 1))
        γ = sample('γ', Normal(0, 1))

    with plate('N_buildings', size=building_data.shape[0]):
        π0 = sample('π0', Normal(0, 1))
        κ0 = sample('κ0', Normal(0, 1))

    π = deterministic('π', building_data @ ζ + σ * π0)
    κ = deterministic('κ', building_data @ γ + τ * κ0)

    # GP, non-centered
    χ = sample('χ', LogNormal(0, 0.5))
    ell = sample('ell', Gamma(10, 2))

    with plate('N_months', size=M):
        ε = sample('ε', Normal(0, 1))

    ms = jnp.arange(0, M)
    Σ = K(ms, ms, χ, ell)
    L = cholesky(Σ, lower=True)

    θ = deterministic('θ', jnp.matmul(L, ε))

    # observations
    with plate("N", size=len(traps)):
        λ = deterministic('λ', jnp.exp(α + π[b] + θ[m] + (β + κ[b]) * traps + log_sq_foot))
        return sample('y', GammaPoisson(φ, φ / λ))
```

### Prepare data

The predictors, the fake predictors, and the observations are all the
same as before, but we have to prepare the fake parameters.

``` {python}
gp_params = {
    'α': 2 * jnp.ones(1),
    'β': -1 * jnp.ones(1),
    'φ': jnp.ones(1),
    'σ': 0.5 * jnp.ones(1),
    'τ': 0.5 * jnp.ones(1),
    'ζ': 0.2 * random.normal(rng, [4]),
    'γ': 0.2 * random.normal(rng, [4]),
    'π0': 0.2 * random.normal(rng, [10]),
    'κ0': 0.2 * random.normal(rng, [10]),
    'ε': 0.5 * random.normal(rng, [42]),
    'χ': 0.5 * jnp.ones(1),
    'ell': 0.5 * jnp.ones(1),
}
```

### prior predictive check

``` {python}
gp_prior_pred = get_prior_pred(gp, ar_X)
print(gp_prior_pred['y'].min(), gp_prior_pred['y'].max())
for k, v in gp_prior_pred.items():
    print(k, v.shape)
```

``` {python}
sns.kdeplot(gp_prior_pred['ell']);
```

let's also look at the prior for the monthly effects. First the
individual functions

``` {python}
fig, ax = plt.subplots(figsize=(16, 8))
for i in random.randint(rng, [10], 0, 1000):
    ax.plot(gp_prior_pred['θ'][i], alpha=0.5)
```

Compared with before, the prior for the monthly effects is much
smoother. And the overall shape

``` {python}
fig, ax = plt.subplots(figsize=(16, 8))
for i in random.randint(rng, [400], 0, 1000):
    ax.plot(gp_prior_pred['θ'][i], color=colors[0], alpha=0.1)
```

### Fit the model to fake data

``` {python}
recover_params(gp, gp_params, ar_fX);
```

there are a few divergent transitions.

### Fit model to the real data 

``` {python}
gp_mcmc = run_mcmc(gp, ar_X, vs_y)
```

``` {python}
gp_posterior_samples = gp_mcmc.get_samples()

for k, v in gp_posterior_samples.items():
    print(k, v.shape)
```

``` {python}
fig, axes = plt.subplots(2, 1, sharex=True, figsize=(8, 6))
sns.histplot(gp_prior_pred['ell'], ax=axes[0])
sns.histplot(gp_posterior_samples['ell'], ax=axes[1]);
plt.xticks(jnp.arange(10));
```

notice that the posterior is essentially cut off at 1 because there is
no way we can infer any value less than 1 from the data, since the
smallest distance in the data (the months) is already 1; we see here
that the scaling of the data is essential to the inference we can do.

``` {python}
fig, axes = plt.subplots(2, 1, sharex=True, figsize=(8, 6))
sns.histplot(gp_prior_pred['χ'], ax=axes[0])
sns.histplot(gp_posterior_samples['χ'], ax=axes[1]);
```

``` {python}
fig, axes = plt.subplots(2, 1, sharex=True, figsize=(8, 6))
sns.histplot(gp_prior_pred['χ'][:1000] / gp_prior_pred['ell'], ax=axes[0])
sns.histplot(gp_posterior_samples['χ'][:1000] / gp_prior_pred['ell'], ax=axes[1]);
```

### Posterior predictive check

``` {python}
gp_posterior_pred = get_posterior_pred(gp, ar_X, gp_mcmc)
print(gp_posterior_pred['y'].min(), gp_posterior_pred['y'].max())

for k, v in gp_posterior_pred.items():
    print(k, v.shape)
```

### Density overlay

``` {python}
overlay_kde(vs_y['y'], gp_posterior_pred['y'])
```

### Specific statistics

``` {python}
ppc_stat(prop_zero, vs_y['y'], gp_posterior_pred['y'])
```

``` {python}
ppc_stat(prop_large, vs_y['y'], gp_posterior_pred['y'])
```

### GP model

Let's look at the Gaussian process part of the model, and compare it
with the AR(1) model

``` {python}
ar_mean = ar_posterior_pred['θ'].mean(axis=0)
ar_std = ar_posterior_pred['θ'].std(axis=0)
gp_mean = gp_posterior_pred['θ'].mean(axis=0)
gp_std = gp_posterior_pred['θ'].std(axis=0)
x = jnp.arange(42)

fig, ax = plt.subplots(figsize=(16, 8))
plt.axvline(35, color=colors[2], alpha=0.5, lw=4)
ax.plot(x, ar_mean, 'o-', label='AR')
ax.fill_between(x, ar_mean-ar_std, ar_mean+ar_std, alpha=0.3)
ax.plot(x, gp_mean, 'o-', label='GP', color=colors[1])
ax.fill_between(x, gp_mean-gp_std, gp_mean+gp_std, color=colors[1], alpha=0.3)
plt.legend();
```

Although the overall predictions are similar, we see that the specific
predictions of the Gaussian process is quite different from the AR(1)
process. GP avoids the sharp spikes and sudden jumps that's evident in
the AR(1) process by enforcing a global stationarity and local
smoothness, the result is more regularised. We are much less likely to
see extreme predictions.

### posterior prediction by groups

``` {python}
plot_grouped_mean(pest_more['complaints'], gp_posterior_pred['y'], pest_more['b'], 2, 5, (12,4))
```

We have been able to further reduce the prediction variance.

``` {python}
plot_grouped_mean(pest_more['complaints'], gp_posterior_pred['y'], pest_more['m'], 6, 6, (12,12))
```

generally speaking, the monthly prediction looks good.

## Ending thoughts

We won't go on expanding the model, because it looks like our model is
already good enough. But good enough for what? Each model building
practice should have the specific model, and the specific decisions to
make about the model in mind. With observed social observations, we can
never build a model good enough to capture all the aspects of the data,
or how the data is really generated. All that we can do, when a problem
is at hand, is to construct a model that solves or approximately solves
the problems. Since we didn't touch on the decision making part in this
post, we'll stop the model building here because the model outcome data
seems to resemble the observed data enough.

Also the structure and substructure of the post might look quite repetitive, that's because we are aiming to enforce a certain procedure to build and validate our models.