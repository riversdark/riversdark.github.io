---
title: "Understanding the attention mechanism"
description: "It's all about (left and right) matrix multiplications."
date: 2023-06-19
categories: ["transformer", "LLM"]
jupyter: torch
image: fig/attention.png
draft: true
---

This post explains how the attention mechanism in modern language models works, and how it is accomplished through matrix multiplication. 

## Token interaction as matrix multiplication

In language modeling, a chuck of text can be tokenised and embedded into a data matrix.  In the following `(2, 3)` data matrix, each row represent the embedding of one token

$$
X = \begin{pmatrix}
x_{11} & x_{12} & x_{13} \\
x_{21} & x_{22} & x_{23}
\end{pmatrix}
$$

However, we know that to model the human natural language, it's not enough to have each token independently embedded; the tokens are also semantically related to each other. For example in the sentence "the art of writting is the art of discovering what you believe", the meaning of the sentence is not only in the individual words, but also in how they are methodically arranged together. With the words and sentences tokenized as the above data matrix, interactions can easily be realized by left and right multiplying the data matrix with interaction matrices. 

For example left multiplication it with an `(2, 2)` interaction matrix 
$$
L = \begin{pmatrix}
l_{11} & l_{12} \\
l_{21} & l_{22}
\end{pmatrix}
$$

will give us 

$$
LX = \begin{pmatrix}
l_{11} & l_{12} \\
l_{21} & l_{22}
\end{pmatrix}
\begin{pmatrix}
x_{11} & x_{12} & x_{13} \\
x_{21} & x_{22} & x_{23}
\end{pmatrix}
= \begin{pmatrix}
l_{11}x_{11} + l_{12}x_{21} & l_{11}x_{12} + l_{12}x_{22} & l_{11}x_{13} + l_{12}x_{23} \\
l_{21}x_{11} + l_{22}x_{21} & l_{21}x_{12} + l_{22}x_{22} & l_{21}x_{13} + l_{22}x_{23}
\end{pmatrix},
$$

which is again a `(2, 3)` matrix, the same as the original data matrix. We notice that each entry in the new data matrix is the weighted sum of the corresponding entries in the original matrix, frome different rows, but from the same column. In this way we achieved the goal of **interacting the different tokens with each other**. However we should also note that there is no interaction between the different columns of the original matrix. 

Similarly, right multiplication it with an `(3, 2)` interaction matrix

$$
R = \begin{pmatrix}
r_{11} & r_{12} \\
r_{21} & r_{22} \\
r_{31} & r_{32}
\end{pmatrix}
$$

will give us 

$$
XR = \begin{pmatrix}
x_{11} & x_{12} & x_{13} \\
x_{21} & x_{22} & x_{23}
\end{pmatrix}
\begin{pmatrix}
r_{11} & r_{12} \\
r_{21} & r_{22} \\
r_{31} & r_{32}
\end{pmatrix}
= \begin{pmatrix}
x_{11}r_{11} + x_{12}r_{21} + x_{13}r_{31} & x_{11}r_{12} + x_{12}r_{22} + x_{13}r_{32} \\
x_{21}r_{11} + x_{22}r_{21} + x_{23}r_{31} & x_{21}r_{12} + x_{22}r_{22} + x_{23}r_{32}
\end{pmatrix},
$$

which is a `(2, 2)` matrix. It has the same number of rows as the original data matrix, but the number of columns is determined by the interaction matrix. We notice that each entry in the new data matrix is the weighted sum of the corresponding entries in the original matrix, from different columns, but from the same row. And there is no interaction between the different rows of the original matrix. In this way we have updated the embedding of each token, but without interacting the tokens with each other.

Combining left and right multiplication, we can thus both make the tokens interact with each other, and update the embedding of each token. In transformer language models, it's the repeated application of this operation that allows the model to "understand" the context of the sentence, and represent it in the most meaningful way. This is the backbone of all the large language models.

## Interaction matrices in practice

To see this in action let's first define a data matrix.

``` {python}
import numpy as np
np.random.seed(0)

T, E = 4, 5  # sequence length, embedding dimension
x = np.random.randn(T, E)  # input tensor
print(x)
```

Here each row is an embedding of one token as a 5 dimensional vector (E=5), and 4 tokens (T=4) are stack together, from top to bottom. This way we end up with a `(4, 5)` data matrix.

Next, let's start with left multiplication interactions.  A diagonal interaction matrix, which means that each token only interacts with itself (i.e. no interaction at all), is the simplest of all the interaction matrices.

``` {python}
L = np.eye(T)
print(L, L @ x)
```

As we expected, the embedding matrix is the same as the original x. Next we can make all the tokens equally affecting all others by setting the interaction matrix to be a matrix of all ones.

``` {python}
L = np.ones((T, T)) 
print(L, L @ x)
```

Now all the rows of the embedding matrix are the same, since we effectively summed up all the rows in the original matrix to create the new ones. A slightly more interesting interaction matrix is the lower triangular matrix:

``` {python}
L = np.tril(np.ones((T, T))) 
print(L, L @ x)
```

After left multiplying L, the first row (embeddings for the first word) is left as is, the new second row is the sum of the first two rows, the new third row is the sum of the first three, etc.

Of course the interaction matrix can be any matrix, not just the ones we've seen above.

``` {python}
L = np.random.randn(T, T)
print(L, L @ x)
```

We can no longer spot the relationship between the tokens since, as the name implied, we are now randomly mixing up the tokens. 

One extra thing to note is that it's generally not a good idea to simply summing vectors up: when the vectors get extremely long, as is often the case in modern deep learning, the total sum can often explode, which affects the stability and efficiency of the model training. So we'll use weights the instead, and make sure each row of L sum to one. Things are easy if all the values are positive, say 1, 2, 2, 3, then the sum is 8, and the weights are simply 1/8, 2/8, 2/8, and 3/8. But, surprisingly, it's not always easy to get meaningful weights, if the entries in L are not always positive. What if the values are 1, -2, 2, 3? And they may even sum to zero, like -1, 2, 2, -3, leaving us with a devided by zero issue. The solution we use is simple: first exponentiate all the entries to make sure they are all positive, and then compute the weights using these positive numbers.

``` {python}
L = np.random.randn(T, T)
L = np.exp(L)
L = L / L.sum(axis=1, keepdims=True)
print(L, L @ x)
```

Similarly we can create a right interaction matrix R to make the columns interact, and thus update the embedding of each token. We can of course apply the same softmax treatment to each column of R so that each column sums to one, but for some reason (which is still beyond my understanding) this has never been done in practice. So here we'll stick to the common wisdom. When applying the right interaction matrix, we need to decide on the new embedding dimension `H1`. (It's named `H1` because we are going to need another `H2`, as we'll see shortly.)

``` {python}
H1 = 6
R = np.random.randn(E, H1)
print(R, x @ R)
```

We end up with a new embedding matrix of dimension `(T, H1)`.

## Parameterizing query, key, and value

We have seen how the left and right interaction matrices can be used to update the embedding of each token, now is the time to determine how to populate them. Since we are talking about neural networks, we can just treat L and R as parameters of the network, and learn them from the data. However, once learned, the same parameter matrices will be applied to all sentences, which effectively means that all the sentences will be forced to interact in the same way, which is clearly not what we want. On the contrary the interaction matrices should be bespoke for each sentence; for this reason the interaction matrices not only need to have learnable parameters, but also functions of the input sentence.

We finally settle on the following formula to calculate the left and right interaction matrices:
$$
\begin{align*}
\mathbf{Y} &= \mathbf{L} \mathbf{X} \mathbf{R} \\
\mathbf{L} &= \text{softmax} \left( \mathbf{Q} \mathbf{K}^\top \right) \\
\mathbf{Q} &= \mathbf{X} \mathbf{W}^Q \\
\mathbf{K} &= \mathbf{X} \mathbf{W}^K \\
\mathbf{V} &= \mathbf{X} \mathbf{W}^V = \mathbf{X} \mathbf{R}
\end{align*}
$$

The left interaction matrix L is the product of two matrices, the query matrix Q and the key matrix K (Ignore softmax, which is for turning random matrices into weights). Since Q and K are calculated exactly the same way, their difference in naming is only an convention. Note that since Q and K (and V) are all calculated by right multiplying a parameter matrix, they are all new embeddings for each token, with no interactions between them. However by multiplying them together

$$
\mathbf{Q} \mathbf{K}^\top = \left( \mathbf{X} \mathbf{W}^Q \right) \left( \mathbf{X} \mathbf{W}^K \right)^\top = \mathbf{X} \mathbf{W}^Q {\mathbf{W}^K}^\top \mathbf{X}^\top,
$$ 

We see that the weight matrices are both appearing on the left (for the second X) and right (for the first X), the resulting matrix thus captures all possible information flows among the tokens.

The value matrix $\mathbf{V}$ is just a rewrite of $\mathbf{X} \mathbf{R}$.

We can now implement the whole process 

``` {python}
T, E, H1, H2 = 4, 5, 6, 7

X = np.random.randn(T, E)

W_Q = np.random.randn(E, H2)
W_K = np.random.randn(E, H2)
W_V = np.random.randn(E, H1)

Q = X @ W_Q
K = X @ W_K
V = X @ W_V

L = np.exp(Q @ K.T)
L = L / L.sum(axis=1, keepdims=True)

print("L shape:", L.shape)
print("V shape:", V.shape)
print("L @ V shape:", (L @ V).shape)
print(L, V, L @ V)
```

## Masked attention

Modern Large language models are mostly next-word-prediction machines.  That is, given a word, we want the model to predict what the next word should be. Actually we mean tokens, but tokens are abstract and unfamiliar, so we'll use them interchangeably to represent "the units of meaning". In this problem setup, with "the art of writting is the art of discovering what you believe", when we see "the art of writting", we don't know that "the art of discovering what you believe" is going to follow; but when we see "the art of discovering what you believe", we have already known that "the art of writting" have come before it. The information flows only one way, like the rivers flow down the Himalayas, but they never flow back to the Himalayas.

For this reason we need the information to only flow forward and never backward. This can be easily achieved by making L a lower triangle matrix:
There are some serious consequences in the approach, but we'll talk
about them another day. (Softmax has a tendency to make the bigger
weights even bigger, which is why we need the normalisation layers.)

## Conclusion

Two outcomes are achieved after all this manipulation:

1.  the tokens are further embedded with the neural network,
2.  and the tokens are communicating with each other. Only one
    directional, of course.

And voilà, we now have gone through the single most import component is
modern large language models.

However, our explanation of the attention mechanism overlooks (or even
intentionally obfuscates) one of the most essential features of
attention, and we will come back to it in the next post on transformer
layers.


todo:
Here we decomposed the task into several subtasks to make it more manageable and extensible:

1.  a lower triangular mask matrix is defined, with the lower left part being ones and upper right part being zeros.
2.  an all-connected-to-all weight matrix is obtained, this captures all possible information flows among the tokens, forward and also backward. We are still using ones here, meaning that all tokens are equally contributing to all other tokens, but this will be modified later.
3.  the mask is applied to the weight matrix to suppress all backward information flow. This is done by keeping the lower left part of the weight matrix as is, but replacing the upper right part as negative infinites.
4.  the weights are exponentiated and normalised so that each row sum to one (using the softmax function). When exponentiated the masked entries, i.e. the negative infinites become zeros, and thus contributing zero weight to the information flow.

Now, what next? We have successfully set up the one direction
information flow, but the left interaction weights are preset equal
weights, and the right (column) interaction weights are just random. The
next step is to make them all neural network parameters and learn from
the data. By tuning the parameters we can decide how much information
the earlier words should pass to the later words, i.e. how much the
later words should "attend" to the earlier words: there goes the
"attention" mechanism. We can also try to find the best embeddings for
the tokens.

In generative pretraining of large language models, we have no other
information than the training input data; understandably the weights
will come from a certain transformation of the input. But what
transformation? The way attention models work, is to further embed the
token embeddings with two extra embedding matrix, and then generate the
weights matrix by multiplying them together.

`key` and `query` are the two embedding matrices
of dimension `(E, H)`, and after the attention embedding the
token embedding will be of dimension (T, E) @ (E, H) = (T, H). Here we
implemented the attention embedding with `nn.linear`, because
when defining a layer using `nn.Linear`, under the hood,
PyTorch automatically initialises a weight matrix of dimensions
`(E, H)` with random values, so `query(x)` and
`key(x)` will do matric multiplication to transforme the
input embeddings into attention heads.

Similiarly we will apply a comparable attention transformation to the
token embedding x, so that the token embeddings are not only upgraded on
the interaction matrix side, but also on the token embedding side.

