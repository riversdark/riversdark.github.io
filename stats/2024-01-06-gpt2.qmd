---
title: "GPT-2 inference with PyTorch"
date: 2024-01-06
draft: true
---

The model is built up step by step, so is the inference.

## encoder

Byte pair encoding utilities.

Copied from:
<https://github.com/openai/gpt-2/blob/master/src/encoder.py>.

## model

The code is adapted from Karpathy's nanoGPT repo.

The transformers are a class of very simple models. It\'s important to
remember this point. They are powerful, of course; the transformer is an
innovative architecture, of course; but most of their power comes from
the fact that they have a HUGE amount of parameters, and this allows the
simple units be adapted and combined in many different ways. Just like
the simple neurons in our brain.

The GPT-2 model, like its predecessor GPT, is based on the transformer
architecture, which primarily uses self-attention mechanisms. The
architecture of GPT-2 can be broken down into several key components,
each with its own weight matrices.

1.  Input Embedding Layer

-   Function: Converts input tokens into vectors.
-   Weight Matrix:
    -   Shape: $[V, E]$, where $V$ is the vocabulary size and $E$ is the
        embedding size.
    -   The same embedding matrix is used for both the input tokens and
        position encodings.
    -   The output of this layer is a sequence of vectors, one for each
        input token.
    -   The total number of parameters in this layer is $V \times E$.

1.  Positional Encoding

-   Function: Adds position information to the input embeddings.
-   Weight Matrix:
    -   Shape: $[N, E]$, where $N$ is the maximum sequence length.
    -   This is not a learned matrix but a fixed one based on sinusoidal
        functions.
    -   The output of this layer is a sequence of vectors, one for each
        input token.
    -   The total number of parameters in this layer is 0.

1.  Transformer Blocks

GPT-2 consists of several transformer blocks (layers) stacked on top of
each other. Each block has two main components:

a\. Multi-Head Self-Attention

-   Function: Allows the model to weigh the importance of different
    words in the input sequence.
-   Weight Matrices:
    -   Each head has three matrices for Query (Q), Key (K), and Value
        (V).
    -   Shape of each matrix: $[E, E/H]$, where $H$ is the number of
        heads.
    -   Additionally, there\'s an output matrix with shape $[E, E]$.
    -   The total number of parameters in this layer is
        $3 \times E^2 + E^2$.

b\. Feed-Forward Neural Network

-   Function: Processes the output from the attention mechanism.
-   Weight Matrices:
    -   Two linear layers, each with a weight matrix.
    -   First layer: $[F, E]$, where $F$ is typically 4 times $E$.
    -   Second layer: $[E, F]$.
    -   The total number of parameters in this layer is
        $F \times E + E \times F$.

1.  Normalization and Residual Connections

-   Function: Used after each sub-layer (self-attention and
    feed-forward) for stabilization and training efficiency.
-   Weight Matrices:
    -   Layer normalization does not significantly change the
        dimensionality but has a small set of parameters for scaling and
        shifting, typically two vectors of size $E$.
    -   The total number of parameters in this layer is $2 \times E$.

1.  Output Layer

-   Function: Converts the final transformer block\'s output into
    probabilities over the vocabulary.
-   Weight Matrix:
    -   Typically tied with the input embedding matrix, hence no
        additional parameters.
    -   The output is processed through a softmax layer to produce
        probabilities.

Summary of Weight Matrix Shapes

-   The exact shapes and sizes of these matrices depend on the specific
    GPT-2 variant (e.g., Small, Medium, Large, XL).
-   Larger variants have more layers and larger embedding sizes, leading
    to a greater number of parameters.

This architecture enables GPT-2 to effectively generate coherent and
contextually relevant language by capturing complex patterns in the data
it\'s trained on. The self-attention mechanism, in particular, allows it
to focus on different parts of the input sequence, which is crucial for
understanding and generating natural language.

``` python
import pandas as pd

# Defining the parameters for each GPT-2 model size
# The structure of GPT-2 models is similar across different sizes,
# but the number of layers and the size of the layers differ.

models = ["GPT-2 Small", "GPT-2 Medium", "GPT-2 Large", "GPT-2 XL"]
layers = [12, 24, 36, 48]  # Number of layers
embedding_sizes = [768, 1024, 1280, 1600]  # Size of embeddings and hidden layers
num_heads = [12, 16, 20, 25]  # Number of attention heads
vocab_size = 50257  # Vocabulary size is the same for all models

# calculate parameters and disk usage
def calculate_parameters_and_storage(layers, embedding_size, num_heads, vocab_size):
    # Each parameter is a 32-bit float, which takes 4 bytes of storage
    bytes_per_parameter = 4

    # Input Embedding Layer Parameters
    input_embedding_params = embedding_size * vocab_size

    # Parameters per Transformer Block
    # Multi-Head Self-Attention Parameters (per head)
    dim_per_head = embedding_size // num_heads
    attention_params_per_head = 3 * dim_per_head * embedding_size
    total_attention_params = attention_params_per_head * num_heads

    # Feed-Forward Network Parameters
    feed_forward_size = embedding_size * 4
    feed_forward_params = feed_forward_size * embedding_size + embedding_size * feed_forward_size

    # Total Parameters per Block
    params_per_block = total_attention_params + feed_forward_params

    # Total Parameters for all Blocks
    total_params_blocks = params_per_block * layers

    # Total Parameters (including input embedding layer)
    total_params = total_params_blocks + input_embedding_params

    # Calculating Disk Usage in Gigabytes
    total_storage_bytes = total_params * bytes_per_parameter
    total_storage_gb = total_storage_bytes / (1024**3)  # Convert bytes to gigabytes

    return total_params, total_storage_gb

# Creating a DataFrame for comparison
data = {
    "Model": models,
    "Layers": layers,
    "Embedding Size": embedding_sizes,
    "Attention Heads": num_heads,
}

df = pd.DataFrame(data)
df.set_index("Model", inplace=True)

# calculating parameters and storage for each model
df["Total Parameters"], df["Disk Usage (GB)"] = zip(*[calculate_parameters_and_storage(l, e, h, vocab_size) for l, e, h in zip(layers, embedding_sizes, num_heads)])
df["Total Parameters"] = df["Total Parameters"].apply(lambda x: "{:.1f}B".format(x / 1e9) if x >= 1e9 else "{:.1f}M".format(x / 1e6))
df["Disk Usage (GB)"] = df["Disk Usage (GB)"].apply(lambda x: "{:.2f}".format(x))

print(df)
```

## reference

LLM Visualization <https://bbycroft.net/llm>
## utils

### download GPT2 files

### load GPT2 parameters from check point

### set in nested dict

### load encoder hyperparameters and parameters

## inference
