---
title: "Surveying the landscape of learning algorithms"
date: 2024-07-01
categories: [torch]
draft: true
---

In this post we will take a (not so) brief survey of some learning algorithms in machine learning, using mixture models as the working example.
The goal is to show how these methods works, and compare their merits and limitations with each other.
Mixture models are multi-modal, have both visible and hidden variables, and have both discrete and continuous parameters, so they are a good example to showcase the various methods.


We will proceed following this roadmap:

- generate data as 2 component, 2 dimensional correlated bivariate Gaussian
- visualize the data
- use K-means to cluster the data
- use EM to fit a GMM model to the data
- use VI to fit a GMM model to the data
- use MCMC (MH and Gibbs) to fit a GMM model to the data
- use normalizing flows (coupling flow, autoregressive flow, continuous flow, flow matching) to fit a GMM model to the data
- use Langevin sampling to fit a GMM model to the data
- use score matching to fit a GMM model to the data

We start with simple cases, to showcase the capabilities of each methods. Later we'll also show some more demanding examples to showcase the limitations, and why some methods are better suitable for some cases than others.

## Data

In this inaugural post, we'll write a data generating process to generate some data, visualize the data, and then build a GMM model to capture the data generating process. Then we'll run a Bayesian inference on the data, using existing Hamiltonian Monte Carlo implementation from NumPyro. This result will be used as the baseline, and later when we implement the various inference algorithms, we'll compare the results to this baseline.

## Visualization

## K-means

## EM

## VI

## MH

## Gibbs

## Normalizing flows

## Flow matching

## Langevin sampling

## Score matching