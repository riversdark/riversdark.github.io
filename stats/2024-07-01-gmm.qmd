---
title: "Mixture models with EM, MCMC (Gibbs), Langevin sampling and score matching"
date: 2024-07-01
draft: true
---

In this series of posts, we will use Gaussian mixture models (GMM) as an example, to illustrate how various inference methods works.

The goal is to show how these methods works, and how they compare to each other.

We will also use pytorch to implement these methods, but the ideas are of course language agnostic.

- generate data from 2-component mixture model
- visualize the data
- model building
- K-means
- EM 
- MCMC (MH and Gibbs)
- normalizing flows (coupling flow, autoregressive flow, continuous flow, flow matching)
- Langevin sampling
- score matching

We start with simple cases, to showcase the capabilities of each methods. Later we'll also show some more demanding examples to showcase the limitations, and why some methods are better suitable for some cases than others.

In this inaugural post, we'll write a data generating process to generate some data, visualize the data, and then build a GMM model to capture the data generating process. Then we'll run a Bayesian inference on the data, using existing Hamiltonian Monte Carlo implementation from NumPyro. This result will be used as the baseline, and later when we implement the various inference algorithms, we'll compare the results to this baseline.