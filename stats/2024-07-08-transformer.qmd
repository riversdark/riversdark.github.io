---
title: "Transformer"
date: 2024-07-08
categories: [LLM]
tags: [transformer]
draft: true
---

## Scaled self-attention

## Multi-head attention

## Residual connection

## Causal attention

## Positional encoding

## Transformer layer

## Ending remarks
