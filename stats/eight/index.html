<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Eight Schools, or the importance of model reparameterisation | Lottery of Babylonian Variations</title><meta name=keywords content="reparam"><meta name=description content="This is an introduction to NumPyro, using the Eight Schools model as example.
Here we demonstrate the effects of model reparameterisation.
Reparameterisation is especially important in hierarchical models, where
the joint density tend to have high curvatures.



1
2
3
4
5
6
7
8


import numpy as np
import jax.numpy as jnp
import numpyro
import numpyro.distributions as dist
from jax import random
from numpyro.infer import MCMC, NUTS, Predictive

rng_key = random.PRNGKey(0)


Here we are using the classic eight schools dataset from Gelman et
al. We have collected the test score statistics for eight schools,
including the mean and standard error. The goal, is to determine whether
some schools have done better than others. Note that since we are
working with the mean and standard error of eight different schools, we
are actually modeling the statistical analysis resutls of some other
people: this is essentially a meta analysis problem."><meta name=author content="olivier"><link rel=canonical href=https://riversdark.github.io/stats/eight/><link crossorigin=anonymous href=/assets/css/stylesheet.870d54df3a830fe0b647f01b40db4202038c3519b977caf3fa4263ddb5a64f79.css integrity="sha256-hw1U3zqDD+C2R/AbQNtCAgOMNRm5d8rz+kJj3bWmT3k=" rel="preload stylesheet" as=style><link rel=icon href=https://riversdark.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://riversdark.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://riversdark.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://riversdark.github.io/apple-touch-icon.png><link rel=mask-icon href=https://riversdark.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://riversdark.github.io/stats/eight/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js></script><script defer>window.addEventListener("DOMContentLoaded",function(){if(typeof renderMathInElement!="function")return;renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1},{left:"$",right:"$",display:!1}],throwOnError:!1})})</script><meta property="og:url" content="https://riversdark.github.io/stats/eight/"><meta property="og:site_name" content="Lottery of Babylonian Variations"><meta property="og:title" content="Eight Schools, or the importance of model reparameterisation"><meta property="og:description" content="This is an introduction to NumPyro, using the Eight Schools model as example.
Here we demonstrate the effects of model reparameterisation. Reparameterisation is especially important in hierarchical models, where the joint density tend to have high curvatures.
1 2 3 4 5 6 7 8 import numpy as np import jax.numpy as jnp import numpyro import numpyro.distributions as dist from jax import random from numpyro.infer import MCMC, NUTS, Predictive rng_key = random.PRNGKey(0) Here we are using the classic eight schools dataset from Gelman et al. We have collected the test score statistics for eight schools, including the mean and standard error. The goal, is to determine whether some schools have done better than others. Note that since we are working with the mean and standard error of eight different schools, we are actually modeling the statistical analysis resutls of some other people: this is essentially a meta analysis problem."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="stats"><meta property="article:published_time" content="2022-06-22T00:00:00+00:00"><meta property="article:modified_time" content="2022-06-22T00:00:00+00:00"><meta property="article:tag" content="Reparam"><meta name=twitter:card content="summary"><meta name=twitter:title content="Eight Schools, or the importance of model reparameterisation"><meta name=twitter:description content="This is an introduction to NumPyro, using the Eight Schools model as example.
Here we demonstrate the effects of model reparameterisation.
Reparameterisation is especially important in hierarchical models, where
the joint density tend to have high curvatures.



1
2
3
4
5
6
7
8


import numpy as np
import jax.numpy as jnp
import numpyro
import numpyro.distributions as dist
from jax import random
from numpyro.infer import MCMC, NUTS, Predictive

rng_key = random.PRNGKey(0)


Here we are using the classic eight schools dataset from Gelman et
al. We have collected the test score statistics for eight schools,
including the mean and standard error. The goal, is to determine whether
some schools have done better than others. Note that since we are
working with the mean and standard error of eight different schools, we
are actually modeling the statistical analysis resutls of some other
people: this is essentially a meta analysis problem."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Stats","item":"https://riversdark.github.io/stats/"},{"@type":"ListItem","position":2,"name":"Eight Schools, or the importance of model reparameterisation","item":"https://riversdark.github.io/stats/eight/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Eight Schools, or the importance of model reparameterisation","name":"Eight Schools, or the importance of model reparameterisation","description":"This is an introduction to NumPyro, using the Eight Schools model as example.\nHere we demonstrate the effects of model reparameterisation. Reparameterisation is especially important in hierarchical models, where the joint density tend to have high curvatures.\n1 2 3 4 5 6 7 8 import numpy as np import jax.numpy as jnp import numpyro import numpyro.distributions as dist from jax import random from numpyro.infer import MCMC, NUTS, Predictive rng_key = random.PRNGKey(0) Here we are using the classic eight schools dataset from Gelman et al. We have collected the test score statistics for eight schools, including the mean and standard error. The goal, is to determine whether some schools have done better than others. Note that since we are working with the mean and standard error of eight different schools, we are actually modeling the statistical analysis resutls of some other people: this is essentially a meta analysis problem.\n","keywords":["reparam"],"articleBody":"This is an introduction to NumPyro, using the Eight Schools model as example.\nHere we demonstrate the effects of model reparameterisation. Reparameterisation is especially important in hierarchical models, where the joint density tend to have high curvatures.\n1 2 3 4 5 6 7 8 import numpy as np import jax.numpy as jnp import numpyro import numpyro.distributions as dist from jax import random from numpyro.infer import MCMC, NUTS, Predictive rng_key = random.PRNGKey(0) Here we are using the classic eight schools dataset from Gelman et al. We have collected the test score statistics for eight schools, including the mean and standard error. The goal, is to determine whether some schools have done better than others. Note that since we are working with the mean and standard error of eight different schools, we are actually modeling the statistical analysis resutls of some other people: this is essentially a meta analysis problem.\n1 2 3 J = 8 y = np.array([28.0, 8.0, -3.0, 7.0, -1.0, 1.0, 18.0, 12.0]) sigma = np.array([15.0, 10.0, 16.0, 11.0, 9.0, 11.0, 10.0, 18.0]) Visualize the data.\n1 2 3 4 5 6 7 8 9 10 11 12 13 import matplotlib.pyplot as plt import seaborn as sns from scipy.stats import norm sns.set_theme() sns.set_palette(\"Set2\") fig, ax = plt.subplots(figsize=(8, 4)) x = np.linspace(-50, 70, 1000) for mean, sd in zip(y, sigma): ax.plot(x, norm.pdf(x, mean, sd)) plt.title('Test Score from Eight Schools') The baseline model The model we are building is a standard hierarchical model. We assume that the observed school means represent their true mean \\(\\theta_n\\), but corrupted with some Normal noise, and the true means are themselves drawn from another district level distribution, with mean \\(\\mu\\) and standard deviation \\(\\tau\\), which are also modeled with suitable distributions. Essentially it’s Gaussian all the way up, and we have three different levels to consider: student, school, and the whole district level population.\n\\[ \\begin{align*} y_n \u0026\\sim \\text{N} (\\theta_n, \\sigma_n) \\\\ \\theta_n \u0026\\sim \\text{N} (\\mu, \\tau) \\\\ \\mu \u0026\\sim \\text{N} (0, 5) \\\\ \\tau \u0026\\sim \\text{HalfCauchy} (5). \\end{align*} \\]In NumPyro models are coded as functions.\n1 2 3 4 5 6 7 def es_0(J, sigma): mu = numpyro.sample('mu', dist.Normal(0, 5)) tau = numpyro.sample('tau', dist.HalfCauchy(5)) with numpyro.plate('J', J): theta = numpyro.sample('theta', dist.Normal(mu, tau)) numpyro.sample('obs', dist.Normal(theta, sigma)) Note that the code and the mathematical model are almost identical, except that they go in different directions. In the mathematical model, we start with the observed data, and reason backward to determine how they might be generated. In the code we start with the hyperparameters, and move forward to generate the observed data.\nJ and sigma are data we used to build our model, but they are not part of the model, in the sense that they are not assigned any probability distribution. y, on the other hand, is the central variable of the model, and is named obs in the model.\nnumpyro.plate is used to denote that the variables inside the plate are conditionally independent. Probability distributions are the building blocks of Bayesian models, and NumPyro has a lot of them. In NunPyro, probability distributions are wrappers of JAX random number generators, and they are translated into sampling statements using the numpyro.sample primitive.\nHowever numpyro.sample is used to define the model, not to draw samples from the distribution. To actually draw samples from a distribution, we use numpyro.infer.Predictive. In numpyro each model defines a joint distribution, and since it’s a probability distribution, we can draw samples from it. And since we haven’t conditioned on any data, the samples we draw are from the prior distribution.\nSampling directly from the prior distribution, and inspect the samples, is a good way to check if the model is correctly defined.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 es_prior_predictive_0 = Predictive(es_0, num_samples=1000) es_prior_samples_0 = es_prior_predictive_0(rng_key, J, sigma) def print_stats(name, samples): print(f\"Variable: {name}\") print(f\" Shape: {samples.shape}\") print(f\" Mean: {jnp.mean(samples, axis=0)}\") print(f\" Variance: {jnp.var(samples, axis=0)}\\n\") # Print statistics for each variable print_stats('mu', es_prior_samples_0['mu']) print_stats('tau', es_prior_samples_0['tau']) print_stats('theta', es_prior_samples_0['theta']) print_stats('obs', es_prior_samples_0['obs']) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 Variable: mu Shape: (1000,) Mean: 0.07919252663850784 Variance: 25.190040588378906 Variable: tau Shape: (1000,) Mean: 18.472793579101562 Variance: 4237.69921875 Variable: theta Shape: (1000, 8) Mean: [ 0.9881359 0.5466191 -3.216042 2.8044345 -1.2074522 2.4413567 4.238887 0.55094534] Variance: [ 5404.2183 2688.3088 4958.9756 2792.266 2839.972 6843.413 23103.627 1778.8481] Variable: obs Shape: (1000, 8) Mean: [ 0.3910051 0.649485 -3.0550027 3.0970583 -1.112252 2.3252409 4.4084034 1.3089797] Variance: [ 5565.5737 2790.3027 5192.405 2918.936 2898.9036 6917.7285 23114.23 2121.8855] When the samples of some variables are observed, we can condition on these observations, and infer the conditional distributions of the other variables. This process is called inference, and is commonly done using MCMC methods. The conditioning is done using the numpyro.handlers.condition primitive, by feeding it a data dict and the model.\nSince we have a GPU available, we will also configure the MCMC sampler to use vectorized chains.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from numpyro.handlers import condition mcmc_args = { 'num_warmup': 1000, 'num_samples': 5000, 'num_chains': 8, 'progress_bar': True, 'chain_method': 'vectorized' } es_conditioned_0 = condition(es_0, data={'obs': y}) es_nuts_0 = NUTS(es_conditioned_0) es_mcmc_0 = MCMC(es_nuts_0, **mcmc_args) es_mcmc_0.run(rng_key, J, sigma) 1 sample: 100% 6000/6000 [00:18\u003c00:00, 328.12it/s] The NUTS sampler is a variant of the Hamiltonian Monte Carlo (HMC) sampler, which is a powerful tool for sampling from complex probability distributions. HMC also comes with its own set of diagnostics, which can be used to check the convergence of the Markov Chain. The most important ones are the effective sample size and the Gelman-Rubin statistic, which is a measure of the convergence of the Markov Chain.\n1 es_mcmc_0.print_summary() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 mean std median 5.0% 95.0% n_eff r_hat mu 4.46 3.30 4.55 -0.72 10.12 2801.92 1.00 tau 4.11 3.21 3.26 0.49 8.25 1534.54 1.01 theta[0] 6.61 5.83 6.05 -2.52 15.59 6268.80 1.00 theta[1] 5.04 4.86 5.00 -2.70 13.15 6283.30 1.00 theta[2] 3.90 5.57 4.24 -4.87 12.66 6632.62 1.00 theta[3] 4.90 4.98 4.91 -3.12 13.02 6666.61 1.00 theta[4] 3.53 4.85 3.89 -4.25 11.55 4699.78 1.00 theta[5] 4.04 5.04 4.38 -3.89 12.33 5403.89 1.00 theta[6] 6.63 5.25 6.09 -1.70 15.17 5692.19 1.00 theta[7] 4.98 5.59 4.99 -4.08 13.58 7813.93 1.00 Number of divergences: 1169 The effective sample size for \\(\\tau\\) is low, and judging from the r_hat value, the Markov Chain might not have converged. Also, the large number of divergences is a sign that the model might not be well specified. Here we are looking at a prominent problem in hierarchical modeling, known as Radford’s funnel, where the posterior distribution has a very sharp peak at the center, and a long tail, and the curvature of the distribution is very high. This seriously hinders the performance of HMC, and the divergences are a sign that the sampler is having trouble exploring the space.\nHowever, the issue can be readily rectified using a non-centered parameterization.\nManual reparameterisation The remedy we are proposing is quite simple: replacing\n$$ \\theta_n \\sim \\text{N} (\\mu, \\tau) $$with\n\\[ \\begin{align*} \\theta_n \u0026= \\mu + \\tau \\theta_0 \\\\ \\theta_0 \u0026\\sim \\text{N} (0, 1). \\end{align*} \\]In essence, instead of drawing from a Normal distribution whose parameters are themselves variables in the model, we draw from the unit Normal distribution, and transform it to get the variable we want. By doing so we untangled the sampling process of \\(\\theta\\) from that of \\(\\mu\\) and \\(\\tau\\).\n1 2 3 4 5 6 7 8 def es_1(J, sigma): mu = numpyro.sample('mu', dist.Normal(0, 5)) tau = numpyro.sample('tau', dist.HalfCauchy(5)) with numpyro.plate('J', J): theta_0 = numpyro.sample('theta_0', dist.Normal(0, 1)) theta = numpyro.deterministic('theta', mu + theta_0 * tau) numpyro.sample('obs', dist.Normal(theta, sigma)) Here we use another primitive, numpyro.deterministic, to register the transformed variable, so that its values can be stored and used later.\n1 2 3 4 es_prior_predictive_1 = Predictive(es_1, num_samples=1000) es_prior_samples_1 = es_prior_predictive_1(rng_key, J, sigma) print_stats('theta_0', es_prior_samples_1['theta_0']) 1 2 3 4 5 6 Variable: theta_0 Shape: (1000, 8) Mean: [ 0.03691418 0.02472821 0.01554041 0.03054582 -0.01188057 0.00954548 -0.01233995 0.03313057] Variance: [1.0163321 1.0030503 0.9524028 0.9750142 1.0098913 0.9325964 1.0606602 1.0366122] we can see the mean and variance are indeed quite close to that of the unit normal.\nCondition on the observed data and do inference.\n1 2 3 4 5 6 es_conditioned_1 = condition(es_1, data={'obs': y}) es_nuts_1 = NUTS(es_conditioned_1) es_mcmc_1 = MCMC(es_nuts_1, **mcmc_args) es_mcmc_1.run(rng_key, J, sigma) es_mcmc_1.print_summary(exclude_deterministic=False) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 sample: 100% 6000/6000 [00:10\u003c00:00, 546.18it/s] mean std median 5.0% 95.0% n_eff r_hat mu 4.37 3.32 4.39 -1.12 9.79 36259.41 1.00 tau 3.61 3.20 2.78 0.00 7.86 29127.09 1.00 theta[0] 6.21 5.57 5.66 -2.33 14.76 37206.26 1.00 theta[1] 4.93 4.69 4.85 -2.45 12.63 45631.36 1.00 theta[2] 3.89 5.30 4.13 -4.26 12.39 37622.30 1.00 theta[3] 4.77 4.79 4.72 -2.97 12.43 41713.54 1.00 theta[4] 3.60 4.68 3.85 -3.90 11.11 43218.27 1.00 theta[5] 4.01 4.85 4.20 -3.50 11.99 42052.70 1.00 theta[6] 6.29 5.07 5.78 -1.80 14.32 41045.35 1.00 theta[7] 4.87 5.34 4.76 -3.54 13.29 38723.13 1.00 theta_0[0] 0.32 0.99 0.34 -1.33 1.92 42168.81 1.00 theta_0[1] 0.10 0.93 0.11 -1.45 1.60 47772.91 1.00 theta_0[2] -0.08 0.97 -0.09 -1.67 1.50 46935.15 1.00 theta_0[3] 0.07 0.94 0.07 -1.49 1.61 46038.02 1.00 theta_0[4] -0.16 0.93 -0.16 -1.69 1.39 46670.00 1.00 theta_0[5] -0.07 0.94 -0.07 -1.56 1.55 44819.96 1.00 theta_0[6] 0.36 0.96 0.38 -1.30 1.88 41323.50 1.00 theta_0[7] 0.08 0.99 0.08 -1.59 1.64 44932.38 1.00 Number of divergences: 4 This looks much better, both the effective sample size and the r_hat have massively improved for the hyperparameter tau, and the number of divergences is also much lower. However, the fact that there are still divergences tells us that reparameterisation might improve the topology of the posterior parameter space, but there is no guarantee that it will completely eliminate the problem. When doing Bayesian inference, especially with models of complex dependency relationships as in hierarchical models, good techniques are never a sufficient replacement for good thinking.\nUsing numpyro’s reparameterisation handler Since this reparameterisation is so widely used, it has already been implemented in NumPyro. And since reparameterisation in general is so important in probabilistic modelling, NumPyro has implemented a wide suite of them.\nIn probabilistic modeling, although it’s always a good practice to separate modeling from inference, it’s not always easy to do so. As we have seen, how we formulate the model can have a significant impact on the inference performance. When building the model, not only do we need to configure the variable transformations, but we also need to inform the inference engine how to handle these transformed variables. This is where the numpyro.handlers.reparam handler comes in.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from numpyro.handlers import reparam from numpyro.infer.reparam import TransformReparam from numpyro.distributions.transforms import AffineTransform from numpyro.distributions import TransformedDistribution def es_2(J, sigma): mu = numpyro.sample('mu', dist.Normal(0, 5)) tau = numpyro.sample('tau', dist.HalfCauchy(5)) with numpyro.plate('J', J): with reparam(config={'theta': TransformReparam()}): theta = numpyro.sample( 'theta', TransformedDistribution(dist.Normal(0., 1.), AffineTransform(mu, tau))) numpyro.sample('obs', dist.Normal(theta, sigma)) The process of reparameterisation goes as follows:\nStart with a standard Normal distribution dist.Normal(0., 1.), Transform it using the affine transformation AffineTransform(mu, tau), Denote the result as a TransformedDistribution, Register the transformed variable theta using numpyro.sample, Inform the inference engine of the reparameterisation using reparam. Proceed with prior predictive sampling.\n1 2 3 4 es_prior_predictive_2 = Predictive(es_2, num_samples=1000) es_prior_samples_2 = es_prior_predictive_2(rng_key, J, sigma) print_stats('theta', es_prior_samples_2['theta']) 1 2 3 4 5 6 Variable: theta Shape: (1000, 8) Mean: [ 0.9881359 0.5466191 -3.216042 2.8044345 -1.2074522 2.4413567 4.238887 0.55094534] Variance: [ 5404.2183 2688.3088 4958.9756 2792.266 2839.972 6843.413 23103.627 1778.8481] Condition on the observed data and do inference.\n1 2 3 4 5 6 es_conditioned_2 = condition(es_2, data={'obs': y}) es_nuts_2 = NUTS(es_conditioned_2) es_mcmc_2 = MCMC(es_nuts_2, **mcmc_args) es_mcmc_2.run(rng_key, J, sigma) es_mcmc_2.print_summary() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 sample: 100% 6000/6000 [00:10\u003c00:00, 550.07it/s] mean std median 5.0% 95.0% n_eff r_hat mu 4.37 3.32 4.39 -1.12 9.79 36259.41 1.00 tau 3.61 3.20 2.78 0.00 7.86 29127.09 1.00 theta_base[0] 0.32 0.99 0.34 -1.33 1.92 42168.81 1.00 theta_base[1] 0.10 0.93 0.11 -1.45 1.60 47772.91 1.00 theta_base[2] -0.08 0.97 -0.09 -1.67 1.50 46935.15 1.00 theta_base[3] 0.07 0.94 0.07 -1.49 1.61 46038.02 1.00 theta_base[4] -0.16 0.93 -0.16 -1.69 1.39 46670.00 1.00 theta_base[5] -0.07 0.94 -0.07 -1.56 1.55 44819.96 1.00 theta_base[6] 0.36 0.96 0.38 -1.30 1.88 41323.50 1.00 theta_base[7] 0.08 0.99 0.08 -1.59 1.64 44932.38 1.00 Number of divergences: 4 The model reparameterised using handlers, as we can see, performs just like the manually reparameterised one, with the same mean and variance estimation, same effective number of samples, and the same number of divergences.\nThe results are consistent with the manual reparameterisation. It might seem uncessarily complicated to use the reparameterisation handler in this simple example, but in more complex models, especially those with many layers of dependencies, the reparameterisation handler can greatly facilitate the model building process.\n","wordCount":"2153","inLanguage":"en","datePublished":"2022-06-22T00:00:00Z","dateModified":"2022-06-22T00:00:00Z","author":[{"@type":"Person","name":"olivier"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://riversdark.github.io/stats/eight/"},"publisher":{"@type":"Organization","name":"Lottery of Babylonian Variations","logo":{"@type":"ImageObject","url":"https://riversdark.github.io/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://riversdark.github.io/ accesskey=h title="Lottery of Babylonian Variations (Alt + H)">Lottery of Babylonian Variations</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://riversdark.github.io/ title=Home><span>Home</span></a></li><li><a href=https://riversdark.github.io/stats/ title=Stats><span>Stats</span></a></li><li><a href=https://riversdark.github.io/culture/ title=Culture><span>Culture</span></a></li><li><a href=https://riversdark.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://riversdark.github.io/categories/ title=Categories><span>Categories</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Eight Schools, or the importance of model reparameterisation</h1><div class=post-meta><span title='2022-06-22 00:00:00 +0000 UTC'>June 22, 2022</span>&nbsp;·&nbsp;<span>2153 words</span>&nbsp;·&nbsp;<span>olivier</span></div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#the-baseline-model>The baseline model</a></li><li><a href=#manual-reparameterisation>Manual reparameterisation</a></li><li><a href=#using-numpyro-s-reparameterisation-handler>Using numpyro&rsquo;s reparameterisation handler</a></li></ul></nav></div></details></div><div class=post-content><p>This is an introduction to NumPyro, using the Eight Schools model as example.</p><p>Here we demonstrate the effects of model reparameterisation.
Reparameterisation is especially important in hierarchical models, where
the joint density tend to have high curvatures.</p><p><a id=code-snippet--import></a></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>jax.numpy</span> <span class=k>as</span> <span class=nn>jnp</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpyro</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpyro.distributions</span> <span class=k>as</span> <span class=nn>dist</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>jax</span> <span class=kn>import</span> <span class=n>random</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>numpyro.infer</span> <span class=kn>import</span> <span class=n>MCMC</span><span class=p>,</span> <span class=n>NUTS</span><span class=p>,</span> <span class=n>Predictive</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>rng_key</span> <span class=o>=</span> <span class=n>random</span><span class=o>.</span><span class=n>PRNGKey</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>Here we are using the classic eight schools dataset from Gelman et
al. We have collected the test score statistics for eight schools,
including the mean and standard error. The goal, is to determine whether
some schools have done better than others. Note that since we are
working with the mean and standard error of eight different schools, we
are actually modeling the statistical analysis resutls of some other
people: this is essentially a meta analysis problem.</p><p><a id=code-snippet--data></a></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>J</span> <span class=o>=</span> <span class=mi>8</span>
</span></span><span class=line><span class=cl><span class=n>y</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=mf>28.0</span><span class=p>,</span> <span class=mf>8.0</span><span class=p>,</span> <span class=o>-</span><span class=mf>3.0</span><span class=p>,</span> <span class=mf>7.0</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.0</span><span class=p>,</span> <span class=mf>1.0</span><span class=p>,</span> <span class=mf>18.0</span><span class=p>,</span> <span class=mf>12.0</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>sigma</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=mf>15.0</span><span class=p>,</span> <span class=mf>10.0</span><span class=p>,</span> <span class=mf>16.0</span><span class=p>,</span> <span class=mf>11.0</span><span class=p>,</span> <span class=mf>9.0</span><span class=p>,</span> <span class=mf>11.0</span><span class=p>,</span> <span class=mf>10.0</span><span class=p>,</span> <span class=mf>18.0</span><span class=p>])</span>
</span></span></code></pre></td></tr></table></div></div><p>Visualize the data.</p><p><a id=code-snippet--data-vis></a></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=k>as</span> <span class=nn>plt</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>seaborn</span> <span class=k>as</span> <span class=nn>sns</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>scipy.stats</span> <span class=kn>import</span> <span class=n>norm</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>sns</span><span class=o>.</span><span class=n>set_theme</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>sns</span><span class=o>.</span><span class=n>set_palette</span><span class=p>(</span><span class=s2>&#34;Set2&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>fig</span><span class=p>,</span> <span class=n>ax</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>subplots</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>8</span><span class=p>,</span> <span class=mi>4</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>linspace</span><span class=p>(</span><span class=o>-</span><span class=mi>50</span><span class=p>,</span> <span class=mi>70</span><span class=p>,</span> <span class=mi>1000</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>mean</span><span class=p>,</span> <span class=n>sd</span> <span class=ow>in</span> <span class=nb>zip</span><span class=p>(</span><span class=n>y</span><span class=p>,</span> <span class=n>sigma</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>ax</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>norm</span><span class=o>.</span><span class=n>pdf</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>mean</span><span class=p>,</span> <span class=n>sd</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s1>&#39;Test Score from Eight Schools&#39;</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><figure><img loading=lazy src=/ox-hugo/e9d3417662a95b7aac6ceef284da58183a443c88.png></figure><h2 id=the-baseline-model>The baseline model<a hidden class=anchor aria-hidden=true href=#the-baseline-model>#</a></h2><p>The model we are building is a standard hierarchical model. We assume
that the observed school means represent their true mean \(\theta_n\), but
corrupted with some Normal noise, and the true means are themselves
drawn from another district level distribution, with mean \(\mu\) and
standard deviation \(\tau\), which are also modeled with suitable
distributions. Essentially it&rsquo;s Gaussian all the way up, and we have
three different levels to consider: student, school, and the whole
district level population.</p>\[
\begin{align*}
y_n &\sim \text{N} (\theta_n, \sigma_n) \\
\theta_n &\sim \text{N} (\mu, \tau) \\
\mu &\sim \text{N} (0, 5) \\
\tau &\sim \text{HalfCauchy} (5).
\end{align*}
\]<p>In NumPyro models are coded as functions.</p><p><a id=code-snippet--baseline-model></a></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>es_0</span><span class=p>(</span><span class=n>J</span><span class=p>,</span> <span class=n>sigma</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>mu</span> <span class=o>=</span> <span class=n>numpyro</span><span class=o>.</span><span class=n>sample</span><span class=p>(</span><span class=s1>&#39;mu&#39;</span><span class=p>,</span> <span class=n>dist</span><span class=o>.</span><span class=n>Normal</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>5</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>tau</span> <span class=o>=</span> <span class=n>numpyro</span><span class=o>.</span><span class=n>sample</span><span class=p>(</span><span class=s1>&#39;tau&#39;</span><span class=p>,</span> <span class=n>dist</span><span class=o>.</span><span class=n>HalfCauchy</span><span class=p>(</span><span class=mi>5</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>with</span> <span class=n>numpyro</span><span class=o>.</span><span class=n>plate</span><span class=p>(</span><span class=s1>&#39;J&#39;</span><span class=p>,</span> <span class=n>J</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>theta</span> <span class=o>=</span> <span class=n>numpyro</span><span class=o>.</span><span class=n>sample</span><span class=p>(</span><span class=s1>&#39;theta&#39;</span><span class=p>,</span> <span class=n>dist</span><span class=o>.</span><span class=n>Normal</span><span class=p>(</span><span class=n>mu</span><span class=p>,</span> <span class=n>tau</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>numpyro</span><span class=o>.</span><span class=n>sample</span><span class=p>(</span><span class=s1>&#39;obs&#39;</span><span class=p>,</span> <span class=n>dist</span><span class=o>.</span><span class=n>Normal</span><span class=p>(</span><span class=n>theta</span><span class=p>,</span> <span class=n>sigma</span><span class=p>))</span>
</span></span></code></pre></td></tr></table></div></div><p>Note that the code and the mathematical model are almost identical,
except that they go in different directions. In the mathematical model,
we start with the observed data, and reason backward to determine how
they might be generated. In the code we start with the hyperparameters,
and move forward to generate the observed data.</p><p><code>J</code> and <code>sigma</code> are data we used to build our model, but they are not
part of the model, in the sense that they are not assigned any
probability distribution. <code>y</code>, on the other hand, is the central
variable of the model, and is named <code>obs</code> in the model.</p><p><code>numpyro.plate</code> is used to denote that the variables inside the plate
are conditionally independent. Probability distributions are the
building blocks of Bayesian models, and NumPyro has a lot of them. In
NunPyro, probability distributions are wrappers of JAX random number
generators, and they are translated into sampling statements using the
<code>numpyro.sample</code> primitive.</p><p>However <code>numpyro.sample</code> is used to define the model, not to draw
samples from the distribution. To actually draw samples from a
distribution, we use <code>numpyro.infer.Predictive</code>. In numpyro each model
defines a joint distribution, and since it&rsquo;s a probability distribution,
we can draw samples from it. And since we haven&rsquo;t conditioned on any
data, the samples we draw are from the prior distribution.</p><p>Sampling directly from the prior distribution, and inspect the samples,
is a good way to check if the model is correctly defined.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>es_prior_predictive_0</span> <span class=o>=</span> <span class=n>Predictive</span><span class=p>(</span><span class=n>es_0</span><span class=p>,</span> <span class=n>num_samples</span><span class=o>=</span><span class=mi>1000</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>es_prior_samples_0</span> <span class=o>=</span> <span class=n>es_prior_predictive_0</span><span class=p>(</span><span class=n>rng_key</span><span class=p>,</span> <span class=n>J</span><span class=p>,</span> <span class=n>sigma</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>print_stats</span><span class=p>(</span><span class=n>name</span><span class=p>,</span> <span class=n>samples</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Variable: </span><span class=si>{</span><span class=n>name</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  Shape: </span><span class=si>{</span><span class=n>samples</span><span class=o>.</span><span class=n>shape</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  Mean: </span><span class=si>{</span><span class=n>jnp</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>samples</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  Variance: </span><span class=si>{</span><span class=n>jnp</span><span class=o>.</span><span class=n>var</span><span class=p>(</span><span class=n>samples</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span><span class=si>}</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Print statistics for each variable</span>
</span></span><span class=line><span class=cl><span class=n>print_stats</span><span class=p>(</span><span class=s1>&#39;mu&#39;</span><span class=p>,</span> <span class=n>es_prior_samples_0</span><span class=p>[</span><span class=s1>&#39;mu&#39;</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>print_stats</span><span class=p>(</span><span class=s1>&#39;tau&#39;</span><span class=p>,</span> <span class=n>es_prior_samples_0</span><span class=p>[</span><span class=s1>&#39;tau&#39;</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>print_stats</span><span class=p>(</span><span class=s1>&#39;theta&#39;</span><span class=p>,</span> <span class=n>es_prior_samples_0</span><span class=p>[</span><span class=s1>&#39;theta&#39;</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>print_stats</span><span class=p>(</span><span class=s1>&#39;obs&#39;</span><span class=p>,</span> <span class=n>es_prior_samples_0</span><span class=p>[</span><span class=s1>&#39;obs&#39;</span><span class=p>])</span>
</span></span></code></pre></td></tr></table></div></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-text data-lang=text><span class=line><span class=cl>Variable: mu
</span></span><span class=line><span class=cl>  Shape: (1000,)
</span></span><span class=line><span class=cl>  Mean: 0.07919252663850784
</span></span><span class=line><span class=cl>  Variance: 25.190040588378906
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Variable: tau
</span></span><span class=line><span class=cl>  Shape: (1000,)
</span></span><span class=line><span class=cl>  Mean: 18.472793579101562
</span></span><span class=line><span class=cl>  Variance: 4237.69921875
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Variable: theta
</span></span><span class=line><span class=cl>  Shape: (1000, 8)
</span></span><span class=line><span class=cl>  Mean: [ 0.9881359   0.5466191  -3.216042    2.8044345  -1.2074522   2.4413567
</span></span><span class=line><span class=cl>  4.238887    0.55094534]
</span></span><span class=line><span class=cl>  Variance: [ 5404.2183  2688.3088  4958.9756  2792.266   2839.972   6843.413
</span></span><span class=line><span class=cl> 23103.627   1778.8481]
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Variable: obs
</span></span><span class=line><span class=cl>  Shape: (1000, 8)
</span></span><span class=line><span class=cl>  Mean: [ 0.3910051  0.649485  -3.0550027  3.0970583 -1.112252   2.3252409
</span></span><span class=line><span class=cl>  4.4084034  1.3089797]
</span></span><span class=line><span class=cl>  Variance: [ 5565.5737  2790.3027  5192.405   2918.936   2898.9036  6917.7285
</span></span><span class=line><span class=cl> 23114.23    2121.8855]
</span></span></code></pre></td></tr></table></div></div><p>When the samples of some variables are observed, we can condition on
these observations, and infer the conditional distributions of the other
variables. This process is called inference, and is commonly done using
MCMC methods. The conditioning is done using the
<code>numpyro.handlers.condition</code> primitive, by feeding it a data dict and
the model.</p><p>Since we have a GPU available, we will also configure the MCMC sampler
to use vectorized chains.</p><p><a id=code-snippet--baseline-inf></a></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>numpyro.handlers</span> <span class=kn>import</span> <span class=n>condition</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>mcmc_args</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;num_warmup&#39;</span><span class=p>:</span> <span class=mi>1000</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;num_samples&#39;</span><span class=p>:</span> <span class=mi>5000</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;num_chains&#39;</span><span class=p>:</span> <span class=mi>8</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;progress_bar&#39;</span><span class=p>:</span> <span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;chain_method&#39;</span><span class=p>:</span> <span class=s1>&#39;vectorized&#39;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>es_conditioned_0</span> <span class=o>=</span> <span class=n>condition</span><span class=p>(</span><span class=n>es_0</span><span class=p>,</span> <span class=n>data</span><span class=o>=</span><span class=p>{</span><span class=s1>&#39;obs&#39;</span><span class=p>:</span> <span class=n>y</span><span class=p>})</span>
</span></span><span class=line><span class=cl><span class=n>es_nuts_0</span> <span class=o>=</span> <span class=n>NUTS</span><span class=p>(</span><span class=n>es_conditioned_0</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>es_mcmc_0</span> <span class=o>=</span> <span class=n>MCMC</span><span class=p>(</span><span class=n>es_nuts_0</span><span class=p>,</span> <span class=o>**</span><span class=n>mcmc_args</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>es_mcmc_0</span><span class=o>.</span><span class=n>run</span><span class=p>(</span><span class=n>rng_key</span><span class=p>,</span> <span class=n>J</span><span class=p>,</span> <span class=n>sigma</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-text data-lang=text><span class=line><span class=cl>sample: 100% 6000/6000 [00:18&lt;00:00, 328.12it/s]
</span></span></code></pre></td></tr></table></div></div><p>The NUTS sampler is a variant of the Hamiltonian Monte Carlo (HMC)
sampler, which is a powerful tool for sampling from complex probability
distributions. HMC also comes with its own set of diagnostics, which can
be used to check the convergence of the Markov Chain. The most important
ones are the effective sample size and the Gelman-Rubin statistic, which
is a measure of the convergence of the Markov Chain.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>es_mcmc_0</span><span class=o>.</span><span class=n>print_summary</span><span class=p>()</span>
</span></span></code></pre></td></tr></table></div></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-text data-lang=text><span class=line><span class=cl>
</span></span><span class=line><span class=cl>                mean       std    median      5.0%     95.0%     n_eff     r_hat
</span></span><span class=line><span class=cl>        mu      4.46      3.30      4.55     -0.72     10.12   2801.92      1.00
</span></span><span class=line><span class=cl>       tau      4.11      3.21      3.26      0.49      8.25   1534.54      1.01
</span></span><span class=line><span class=cl>  theta[0]      6.61      5.83      6.05     -2.52     15.59   6268.80      1.00
</span></span><span class=line><span class=cl>  theta[1]      5.04      4.86      5.00     -2.70     13.15   6283.30      1.00
</span></span><span class=line><span class=cl>  theta[2]      3.90      5.57      4.24     -4.87     12.66   6632.62      1.00
</span></span><span class=line><span class=cl>  theta[3]      4.90      4.98      4.91     -3.12     13.02   6666.61      1.00
</span></span><span class=line><span class=cl>  theta[4]      3.53      4.85      3.89     -4.25     11.55   4699.78      1.00
</span></span><span class=line><span class=cl>  theta[5]      4.04      5.04      4.38     -3.89     12.33   5403.89      1.00
</span></span><span class=line><span class=cl>  theta[6]      6.63      5.25      6.09     -1.70     15.17   5692.19      1.00
</span></span><span class=line><span class=cl>  theta[7]      4.98      5.59      4.99     -4.08     13.58   7813.93      1.00
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Number of divergences: 1169
</span></span></code></pre></td></tr></table></div></div><p>The effective sample size for \(\tau\) is low, and judging from the
<code>r_hat</code> value, the Markov Chain might not have converged. Also, the
large number of divergences is a sign that the model might not be well
specified. Here we are looking at a prominent problem in hierarchical
modeling, known as Radford&rsquo;s funnel, where the posterior distribution
has a very sharp peak at the center, and a long tail, and the curvature
of the distribution is very high. This seriously hinders the performance
of HMC, and the divergences are a sign that the sampler is having
trouble exploring the space.</p><p>However, the issue can be readily rectified using a non-centered
parameterization.</p><h2 id=manual-reparameterisation>Manual reparameterisation<a hidden class=anchor aria-hidden=true href=#manual-reparameterisation>#</a></h2><p>The remedy we are proposing is quite simple: replacing</p>$$
\theta_n \sim \text{N} (\mu, \tau)
$$<p>with</p>\[
\begin{align*}
\theta_n &= \mu + \tau \theta_0 \\
\theta_0 &\sim \text{N} (0, 1).
\end{align*}
\]<p>In essence, instead of drawing from a Normal distribution whose
parameters are themselves variables in the model, we draw from the unit
Normal distribution, and transform it to get the variable we want. By
doing so we untangled the sampling process of \(\theta\) from that of
\(\mu\) and \(\tau\).</p><p><a id=code-snippet--manual></a></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>es_1</span><span class=p>(</span><span class=n>J</span><span class=p>,</span> <span class=n>sigma</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>mu</span> <span class=o>=</span> <span class=n>numpyro</span><span class=o>.</span><span class=n>sample</span><span class=p>(</span><span class=s1>&#39;mu&#39;</span><span class=p>,</span> <span class=n>dist</span><span class=o>.</span><span class=n>Normal</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>5</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>tau</span> <span class=o>=</span> <span class=n>numpyro</span><span class=o>.</span><span class=n>sample</span><span class=p>(</span><span class=s1>&#39;tau&#39;</span><span class=p>,</span> <span class=n>dist</span><span class=o>.</span><span class=n>HalfCauchy</span><span class=p>(</span><span class=mi>5</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>with</span> <span class=n>numpyro</span><span class=o>.</span><span class=n>plate</span><span class=p>(</span><span class=s1>&#39;J&#39;</span><span class=p>,</span> <span class=n>J</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>theta_0</span> <span class=o>=</span> <span class=n>numpyro</span><span class=o>.</span><span class=n>sample</span><span class=p>(</span><span class=s1>&#39;theta_0&#39;</span><span class=p>,</span> <span class=n>dist</span><span class=o>.</span><span class=n>Normal</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>theta</span> <span class=o>=</span> <span class=n>numpyro</span><span class=o>.</span><span class=n>deterministic</span><span class=p>(</span><span class=s1>&#39;theta&#39;</span><span class=p>,</span> <span class=n>mu</span> <span class=o>+</span> <span class=n>theta_0</span> <span class=o>*</span> <span class=n>tau</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>numpyro</span><span class=o>.</span><span class=n>sample</span><span class=p>(</span><span class=s1>&#39;obs&#39;</span><span class=p>,</span> <span class=n>dist</span><span class=o>.</span><span class=n>Normal</span><span class=p>(</span><span class=n>theta</span><span class=p>,</span> <span class=n>sigma</span><span class=p>))</span>
</span></span></code></pre></td></tr></table></div></div><p>Here we use another primitive, <code>numpyro.deterministic</code>, to register the
transformed variable, so that its values can be stored and used later.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>es_prior_predictive_1</span> <span class=o>=</span> <span class=n>Predictive</span><span class=p>(</span><span class=n>es_1</span><span class=p>,</span> <span class=n>num_samples</span><span class=o>=</span><span class=mi>1000</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>es_prior_samples_1</span> <span class=o>=</span> <span class=n>es_prior_predictive_1</span><span class=p>(</span><span class=n>rng_key</span><span class=p>,</span> <span class=n>J</span><span class=p>,</span> <span class=n>sigma</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>print_stats</span><span class=p>(</span><span class=s1>&#39;theta_0&#39;</span><span class=p>,</span> <span class=n>es_prior_samples_1</span><span class=p>[</span><span class=s1>&#39;theta_0&#39;</span><span class=p>])</span>
</span></span></code></pre></td></tr></table></div></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-text data-lang=text><span class=line><span class=cl>Variable: theta_0
</span></span><span class=line><span class=cl>  Shape: (1000, 8)
</span></span><span class=line><span class=cl>  Mean: [ 0.03691418  0.02472821  0.01554041  0.03054582 -0.01188057  0.00954548
</span></span><span class=line><span class=cl> -0.01233995  0.03313057]
</span></span><span class=line><span class=cl>  Variance: [1.0163321 1.0030503 0.9524028 0.9750142 1.0098913 0.9325964 1.0606602
</span></span><span class=line><span class=cl> 1.0366122]
</span></span></code></pre></td></tr></table></div></div><p>we can see the mean and variance are indeed quite close to that of the unit normal.</p><p>Condition on the observed data and do inference.</p><p><a id=code-snippet--manual-inf></a></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>es_conditioned_1</span> <span class=o>=</span> <span class=n>condition</span><span class=p>(</span><span class=n>es_1</span><span class=p>,</span> <span class=n>data</span><span class=o>=</span><span class=p>{</span><span class=s1>&#39;obs&#39;</span><span class=p>:</span> <span class=n>y</span><span class=p>})</span>
</span></span><span class=line><span class=cl><span class=n>es_nuts_1</span> <span class=o>=</span> <span class=n>NUTS</span><span class=p>(</span><span class=n>es_conditioned_1</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>es_mcmc_1</span> <span class=o>=</span> <span class=n>MCMC</span><span class=p>(</span><span class=n>es_nuts_1</span><span class=p>,</span> <span class=o>**</span><span class=n>mcmc_args</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>es_mcmc_1</span><span class=o>.</span><span class=n>run</span><span class=p>(</span><span class=n>rng_key</span><span class=p>,</span> <span class=n>J</span><span class=p>,</span> <span class=n>sigma</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>es_mcmc_1</span><span class=o>.</span><span class=n>print_summary</span><span class=p>(</span><span class=n>exclude_deterministic</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-text data-lang=text><span class=line><span class=cl>sample: 100% 6000/6000 [00:10&lt;00:00, 546.18it/s]
</span></span><span class=line><span class=cl>                mean       std    median      5.0%     95.0%     n_eff     r_hat
</span></span><span class=line><span class=cl>        mu      4.37      3.32      4.39     -1.12      9.79  36259.41      1.00
</span></span><span class=line><span class=cl>       tau      3.61      3.20      2.78      0.00      7.86  29127.09      1.00
</span></span><span class=line><span class=cl>  theta[0]      6.21      5.57      5.66     -2.33     14.76  37206.26      1.00
</span></span><span class=line><span class=cl>  theta[1]      4.93      4.69      4.85     -2.45     12.63  45631.36      1.00
</span></span><span class=line><span class=cl>  theta[2]      3.89      5.30      4.13     -4.26     12.39  37622.30      1.00
</span></span><span class=line><span class=cl>  theta[3]      4.77      4.79      4.72     -2.97     12.43  41713.54      1.00
</span></span><span class=line><span class=cl>  theta[4]      3.60      4.68      3.85     -3.90     11.11  43218.27      1.00
</span></span><span class=line><span class=cl>  theta[5]      4.01      4.85      4.20     -3.50     11.99  42052.70      1.00
</span></span><span class=line><span class=cl>  theta[6]      6.29      5.07      5.78     -1.80     14.32  41045.35      1.00
</span></span><span class=line><span class=cl>  theta[7]      4.87      5.34      4.76     -3.54     13.29  38723.13      1.00
</span></span><span class=line><span class=cl>theta_0[0]      0.32      0.99      0.34     -1.33      1.92  42168.81      1.00
</span></span><span class=line><span class=cl>theta_0[1]      0.10      0.93      0.11     -1.45      1.60  47772.91      1.00
</span></span><span class=line><span class=cl>theta_0[2]     -0.08      0.97     -0.09     -1.67      1.50  46935.15      1.00
</span></span><span class=line><span class=cl>theta_0[3]      0.07      0.94      0.07     -1.49      1.61  46038.02      1.00
</span></span><span class=line><span class=cl>theta_0[4]     -0.16      0.93     -0.16     -1.69      1.39  46670.00      1.00
</span></span><span class=line><span class=cl>theta_0[5]     -0.07      0.94     -0.07     -1.56      1.55  44819.96      1.00
</span></span><span class=line><span class=cl>theta_0[6]      0.36      0.96      0.38     -1.30      1.88  41323.50      1.00
</span></span><span class=line><span class=cl>theta_0[7]      0.08      0.99      0.08     -1.59      1.64  44932.38      1.00
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Number of divergences: 4
</span></span></code></pre></td></tr></table></div></div><p>This looks much better, both the effective sample size and the <code>r_hat</code>
have massively improved for the hyperparameter <code>tau</code>, and the number of
divergences is also much lower. However, the fact that there are still
divergences tells us that reparameterisation might improve the topology
of the posterior parameter space, but there is no guarantee that it will
completely eliminate the problem. When doing Bayesian inference,
especially with models of complex dependency relationships as in
hierarchical models, good techniques are never a sufficient replacement
for good thinking.</p><h2 id=using-numpyro-s-reparameterisation-handler>Using numpyro&rsquo;s reparameterisation handler<a hidden class=anchor aria-hidden=true href=#using-numpyro-s-reparameterisation-handler>#</a></h2><p>Since this reparameterisation is so widely used, it has already been
implemented in NumPyro. And since reparameterisation in general is so
important in probabilistic modelling, NumPyro has implemented a wide
suite of them.</p><p>In probabilistic modeling, although it&rsquo;s always a good practice to
separate modeling from inference, it&rsquo;s not always easy to do so. As we
have seen, how we formulate the model can have a significant impact on
the inference performance. When building the model, not only do we need
to configure the variable transformations, but we also need to inform
the inference engine how to handle these transformed variables. This is
where the <code>numpyro.handlers.reparam</code> handler comes in.</p><p><a id=code-snippet--handler></a></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>numpyro.handlers</span> <span class=kn>import</span> <span class=n>reparam</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>numpyro.infer.reparam</span> <span class=kn>import</span> <span class=n>TransformReparam</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>numpyro.distributions.transforms</span> <span class=kn>import</span> <span class=n>AffineTransform</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>numpyro.distributions</span> <span class=kn>import</span> <span class=n>TransformedDistribution</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>es_2</span><span class=p>(</span><span class=n>J</span><span class=p>,</span> <span class=n>sigma</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>mu</span> <span class=o>=</span> <span class=n>numpyro</span><span class=o>.</span><span class=n>sample</span><span class=p>(</span><span class=s1>&#39;mu&#39;</span><span class=p>,</span> <span class=n>dist</span><span class=o>.</span><span class=n>Normal</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>5</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>tau</span> <span class=o>=</span> <span class=n>numpyro</span><span class=o>.</span><span class=n>sample</span><span class=p>(</span><span class=s1>&#39;tau&#39;</span><span class=p>,</span> <span class=n>dist</span><span class=o>.</span><span class=n>HalfCauchy</span><span class=p>(</span><span class=mi>5</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>with</span> <span class=n>numpyro</span><span class=o>.</span><span class=n>plate</span><span class=p>(</span><span class=s1>&#39;J&#39;</span><span class=p>,</span> <span class=n>J</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>with</span> <span class=n>reparam</span><span class=p>(</span><span class=n>config</span><span class=o>=</span><span class=p>{</span><span class=s1>&#39;theta&#39;</span><span class=p>:</span> <span class=n>TransformReparam</span><span class=p>()}):</span>
</span></span><span class=line><span class=cl>            <span class=n>theta</span> <span class=o>=</span> <span class=n>numpyro</span><span class=o>.</span><span class=n>sample</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=s1>&#39;theta&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>TransformedDistribution</span><span class=p>(</span><span class=n>dist</span><span class=o>.</span><span class=n>Normal</span><span class=p>(</span><span class=mf>0.</span><span class=p>,</span> <span class=mf>1.</span><span class=p>),</span> <span class=n>AffineTransform</span><span class=p>(</span><span class=n>mu</span><span class=p>,</span> <span class=n>tau</span><span class=p>)))</span>
</span></span><span class=line><span class=cl>        <span class=n>numpyro</span><span class=o>.</span><span class=n>sample</span><span class=p>(</span><span class=s1>&#39;obs&#39;</span><span class=p>,</span> <span class=n>dist</span><span class=o>.</span><span class=n>Normal</span><span class=p>(</span><span class=n>theta</span><span class=p>,</span> <span class=n>sigma</span><span class=p>))</span>
</span></span></code></pre></td></tr></table></div></div><p>The process of reparameterisation goes as follows:</p><ol><li>Start with a standard Normal distribution <code>dist.Normal(0., 1.)</code>,</li><li>Transform it using the affine transformation <code>AffineTransform(mu, tau)</code>,</li><li>Denote the result as a <code>TransformedDistribution</code>,</li><li>Register the transformed variable <code>theta</code> using <code>numpyro.sample</code>,</li><li>Inform the inference engine of the reparameterisation using <code>reparam</code>.</li></ol><p>Proceed with prior predictive sampling.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>es_prior_predictive_2</span> <span class=o>=</span> <span class=n>Predictive</span><span class=p>(</span><span class=n>es_2</span><span class=p>,</span> <span class=n>num_samples</span><span class=o>=</span><span class=mi>1000</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>es_prior_samples_2</span> <span class=o>=</span> <span class=n>es_prior_predictive_2</span><span class=p>(</span><span class=n>rng_key</span><span class=p>,</span> <span class=n>J</span><span class=p>,</span> <span class=n>sigma</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>print_stats</span><span class=p>(</span><span class=s1>&#39;theta&#39;</span><span class=p>,</span> <span class=n>es_prior_samples_2</span><span class=p>[</span><span class=s1>&#39;theta&#39;</span><span class=p>])</span>
</span></span></code></pre></td></tr></table></div></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-text data-lang=text><span class=line><span class=cl>Variable: theta
</span></span><span class=line><span class=cl>  Shape: (1000, 8)
</span></span><span class=line><span class=cl>  Mean: [ 0.9881359   0.5466191  -3.216042    2.8044345  -1.2074522   2.4413567
</span></span><span class=line><span class=cl>  4.238887    0.55094534]
</span></span><span class=line><span class=cl>  Variance: [ 5404.2183  2688.3088  4958.9756  2792.266   2839.972   6843.413
</span></span><span class=line><span class=cl> 23103.627   1778.8481]
</span></span></code></pre></td></tr></table></div></div><p>Condition on the observed data and do inference.</p><p><a id=code-snippet--handler-inf></a></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>es_conditioned_2</span> <span class=o>=</span> <span class=n>condition</span><span class=p>(</span><span class=n>es_2</span><span class=p>,</span> <span class=n>data</span><span class=o>=</span><span class=p>{</span><span class=s1>&#39;obs&#39;</span><span class=p>:</span> <span class=n>y</span><span class=p>})</span>
</span></span><span class=line><span class=cl><span class=n>es_nuts_2</span> <span class=o>=</span> <span class=n>NUTS</span><span class=p>(</span><span class=n>es_conditioned_2</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>es_mcmc_2</span> <span class=o>=</span> <span class=n>MCMC</span><span class=p>(</span><span class=n>es_nuts_2</span><span class=p>,</span> <span class=o>**</span><span class=n>mcmc_args</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>es_mcmc_2</span><span class=o>.</span><span class=n>run</span><span class=p>(</span><span class=n>rng_key</span><span class=p>,</span> <span class=n>J</span><span class=p>,</span> <span class=n>sigma</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>es_mcmc_2</span><span class=o>.</span><span class=n>print_summary</span><span class=p>()</span>
</span></span></code></pre></td></tr></table></div></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-text data-lang=text><span class=line><span class=cl>sample: 100% 6000/6000 [00:10&lt;00:00, 550.07it/s]
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>                   mean       std    median      5.0%     95.0%     n_eff     r_hat
</span></span><span class=line><span class=cl>           mu      4.37      3.32      4.39     -1.12      9.79  36259.41      1.00
</span></span><span class=line><span class=cl>          tau      3.61      3.20      2.78      0.00      7.86  29127.09      1.00
</span></span><span class=line><span class=cl>theta_base[0]      0.32      0.99      0.34     -1.33      1.92  42168.81      1.00
</span></span><span class=line><span class=cl>theta_base[1]      0.10      0.93      0.11     -1.45      1.60  47772.91      1.00
</span></span><span class=line><span class=cl>theta_base[2]     -0.08      0.97     -0.09     -1.67      1.50  46935.15      1.00
</span></span><span class=line><span class=cl>theta_base[3]      0.07      0.94      0.07     -1.49      1.61  46038.02      1.00
</span></span><span class=line><span class=cl>theta_base[4]     -0.16      0.93     -0.16     -1.69      1.39  46670.00      1.00
</span></span><span class=line><span class=cl>theta_base[5]     -0.07      0.94     -0.07     -1.56      1.55  44819.96      1.00
</span></span><span class=line><span class=cl>theta_base[6]      0.36      0.96      0.38     -1.30      1.88  41323.50      1.00
</span></span><span class=line><span class=cl>theta_base[7]      0.08      0.99      0.08     -1.59      1.64  44932.38      1.00
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Number of divergences: 4
</span></span></code></pre></td></tr></table></div></div><p>The model reparameterised using handlers, as we can see, performs just like the manually reparameterised one, with the same mean and variance estimation, same effective number of samples, and the same number of divergences.</p><p>The results are consistent with the manual reparameterisation. It might
seem uncessarily complicated to use the reparameterisation handler in
this simple example, but in more complex models, especially those with
many layers of dependencies, the reparameterisation handler can greatly
facilitate the model building process.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://riversdark.github.io/tags/reparam/>Reparam</a></li></ul><nav class=paginav><a class=prev href=https://riversdark.github.io/posts/2026/01/solving-advent-of-code-2025/><span class=title>« Prev</span><br><span>Solving Advent of Code 2025</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://riversdark.github.io/>Lottery of Babylonian Variations</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>